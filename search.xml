<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[对软件开发中需求的思考]]></title>
    <url>%2Fposts%2F57%2F</url>
    <content type="text"><![CDATA[终极原则 一切都是为沟通，做软件最重要的是沟通 沟通是为了让交付团队充分理解业务 需求以用户故事方式编写格式： 为了[业务价值]，作为 [角色]，我需要[功能] 示例： 为了体验系统功能，作为用户，我需要登录系统 好处： 每一条需求都表明了业务价值，不做无用功能 每一条需求都指定了用户角色，说明功能是给谁用的，不臆想没人会使用的功能 同一个功能，不同角色，可能实现不同 突出业务价值，站在用户的角度考虑问题，功能的业务价值明确 重要的不是格式而是沟通，用户故事只是一个标题，一个索引，是为让团队沟通讨论并一致的理解需求 用户提的需求可能只是一种解决方案拿到需求先问为什么? 示例：用户提出的需求：能不能把密码保存到文件？ 5W 方式找到真实需求，使用更好的解决方案来实现真实的需求： 为什么要把密码保存到文件？因为用户不想输入密码。 为什么不想输入密码？因为输入用户名密码太频繁，半个小时就要输入一次。 为什么输入用户名密码太频繁？因为为了安全，我们设置了30分钟的登录过期策略。 几次为什么之后，我们已经找到最终的问题，我们可以通过浏览器记住密码，或者延长登录期策略或者其他方案来解决用户的需求。 反馈循环 传统方式功能有没有完成，需要等待测试或者产品等其他利益相关人验收，反馈循环长，手动验证，不可重复验证。 更好的方式提前讨论沟通需求，使用各方对需求理解一致，并输出成业务相关的验收测试，在开发过程中即可执行验收测试，验证需求是否完成，而且可以反复执行。 为什么需求文档不是最好的选择同步需求文档不易与真实的需求保持一致，需求会经常变更，而需求文档可能会忘记更新同步 理解不同同一份需求文档，不同人阅读，会得到不同的理解，无法保证理解一致，可能只有在验收时，才发现理解不一致的问题，然后返工，成本大，反馈循环长 可验证性需求文档不能验证，功能是否已经准确无误的完成了，仍然需要手动验证 更好的选择可视化需求分析文字版本效果，理解困难，不直观 可视化图表，清晰易懂 实例化需求抽象的需求和规则一般难于理解，只有业务规则和实际的例子一起，才能更好的理解需求 规则： 密码至少包含数字、小写字母、大写字母 密码长度最短为6个字符 实例化： a1234546B 合法 a123456c 不合法 123456 不合法 A123456 不合法 实例化得到的即是测试用例 实践 Example Mapping BDD 活文档 从业务语言到代码实现 开发循环 需求规划迭代计划，小版本发布每一次迭代发布都要有明确的目标，把对目标有贡献的需求加入到当次迭代不能只见树木不见森林，要有整个产品的用户故事地图，用户旅程 用户故事地图 示例： 用户旅程 示例： 参考文档 用户故事地图 用户故事与敏捷方法 实例化需求 BDD in Action The BDD Books - Discover 可视化软件需求分析]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>agile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Istio 上线可能会遇到的坑总结]]></title>
    <url>%2Fposts%2F56%2F</url>
    <content type="text"><![CDATA[我们从迁移服务到 Istio 到现在稳定运行已经有半年时间。虽然在使用 Istio 的过程中，整体过程还算顺利，但是我们还是碰到了一些大大小小的问题，我们把这些问题以及解决方法做一个总结，希望能帮助到有意愿使用 Istio 的团队和个人。 我们碰到的 Istio 使用问题： k8s 中 service 的命名端口问题 gRPC 服务端使用 TLS 证书问题 服务没有监听 127.0.0.1问题 HTTP 1.0 协议问题 由 keepalive 导致的偶发 503 Proxy 与应用的启动顺序不固定 路由域名冲突问题 Istio 导致 PV 挂载问题 Headless 服务问题 k8s 中 service 的命名端口问题service 使用命名端口123456789101112apiVersion: v1kind: Servicemetadata: name: my-servicespec: selector: app: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 9376 Istio 实现了 HTTP 和 HTTP2 流量的自动探测，当服务没有使用命名端口时候，并且开启了流量自动探测功时（默认开启），Istio 可以进行自动的流量探测来识别协议。但是如果你使用了命名端口，但是指定错了协议，那就会导致服务无法正常访问，通常会得到 connection reset 的错误。 具体支持协议的类型，以及自动探测相关文档可以参考Istio 协议选择 gRPC 服务端使用 TLS 问题当你有服务使用 gRPC 协议并且使用 TLS 证书时，你需要把服务的协议类型命名为 https 或者 tls ，而不能直接使用 grpc ，具体问题可参考Istio issue 123456789101112apiVersion: v1kind: Servicemetadata: name: my-grpc-service-with-tlsspec: selector: app: MyApp ports: - name: https protocol: TCP port: 80 targetPort: 9376 服务没有监听 127.0.0.1 问题 如上图所示，当服务 A 访问服务 B 时，实际上流量是先达到服务 A 的 Proxy 上的，然后再到服务 B 的 Proxy 上的，然后服务 B 上的 Proxy 会访问本地 127.0.0.1 地址对应的端口。如果服务 B 没有监听 127.0.0.1 地址上的对应端口，就会导致服务 B 无法自动被访问，出现 connection refused 的情况。 迁移 zookeeper 或者 redis 到 Istio 中时，就会碰到类似问题，官方有专门的FAQ 说明 HTTP 1.0 协议问题由于 Istio 不支持 HTTP 1.0 版本的协议，所以当客户端在 Istio 集群中使用 HTTP 1.0 协议时，就会出现无法正常访问的情况。比如当使用 Nginx 作了代理时，默认情况下会使用 HTTP 1.0 协议，访问 Nginx 时，就会出现 503 无法正常访问的情况。具体问题可见官方帮助文档 由 keepalive 导致的偶发 503默认情况下 Istio 的 Proxy 会跟后端服务保持长连接，TCP 默认情况下保持跟系统的 tcp keepalive 一样，linux 系统一般默认为 7200s ，HTTP 协议默认设置为 1h，除非后端主动断开，否则连接将会一直被重复使用。 由于后端服务设置的 keepalive 时间过短，在高并发情况下，可能会出现当后端服务发送断开请求数据包 fin 时，数据包还没有达到 Proxy 时， Proxy 认为此连接依然正常可用，正好有请求到达 Proxy， 于是 Proxy 仍然会发请求给这个连接，由于后端服务已经发送了断开请求数据包 fin 包，当这条连接再有数据过来时，会直接返回 tcp reset ，导致 Proxy 返回 503 给调用方。 解法方法： 可以通过给服务的 destinationrule 设置 keepalive 时间，让 Proxy 的 keepalive 时间小于后端服务的 keepalive 时间，让 Proxy 主动断开连接。 HTTP 层可以设置idleTimeout参数 TCP 层可以设置tcpKeepalive参数 Proxy 与应用的启动顺序不固定由于 k8s 的 pod 中的容器并不保证启动顺序，所以可能导致应用启动早于 Proxy，如果此时应用对外发起请求，就会出现请求失败的情况，应用可能会出现反复重启的情况，比如应用启动时会去先拿配置中心的配置，然后再启动应用。 路由域名冲突问题某些版本的 Istio 当出现域名冲突时，会停止更新路由，导致路由不正常，集群无法正常提供服务。比如当配置了相同域名的 service 和 serviceentry 时 Istio 导致 PV 挂载问题在 Istio 1.5 版本之后，加入了third party service account tokens 来增强 Proxy 到控制平面的认证。但是由于k8s 的 bug ，导致 Istio 使用了另一个方式绕过这个 bug ，但是这就导致了另外一个问题。由于使用 fsGroup ，会导致每次挂载 PV 时，都要修改整个 PV 中的文件权限，当文件比较多时，就可能会导致挂载超时，无法成功挂载 PV，问题可见Istio issue。不管使用的是third-party-jwt还是first-party-jwt，都会有该问题。 我之前的解决方案是使用first-party-jwt，并且注释了 Istio 代码中强制插入 fsGroup 的代码，重新编译了 pilot 镜像，替换了官方的镜像。 Headless 服务问题Istio 默认情况下会开启服务间的兼容模式的 mTLS，即服务可以支持 mTLS 的方式访问，也可以使用 plain 流量访问。当两个服务服务都部署在 Istio 集群中时，会默认使用 mTLS 方式访问，当 Istio 集群中服务访问没有部署在 Istio 集群中的服务时，默认使用 plain 流量访问。但是由于 Istio 1.6 之前的版本对于 headless 服务支持不够完善，导致部署在 Istio 集群中的服务无法访问没有部署在 Istio 集群中的 headless 服务，访问时会出现 connection reset 的错误。排查后发现是由于 Istio 把集群中访问 headless 服务的路由规则设置成了只使用 mTLS 方式访问，所以导致了访问 Istio 集群外的 headless 服务出现问题。 具体分析可见Istio 运维实战系列（2）：让人头大的『无头服务』-上 总结以上这些问题都是我们在生产环境碰到的问题，有一些是我们自己的使用方式问题，还有一些是 Istio 自身的问题，甚至还有一些是 k8s 的问题，不过我们都找到了还不错的解决方法，希望以后 Istio 能越来越好用稳定。]]></content>
      <categories>
        <category>服务网格</category>
      </categories>
      <tags>
        <tag>service mesh</tag>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[clean agile 敏捷读书笔记]]></title>
    <url>%2Fposts%2F55%2F</url>
    <content type="text"><![CDATA[前言这本书既适合程序员也适合非程序员。它不是技术性的。没有代码。它旨在提供敏捷软件开发的原始意图的概述，而不涉及任何编程、测试和管理的深入技术细节。 这是一本小书。那是因为这个话题不大。敏捷是一个关于小型编程团队做小事情的小问题的小想法。敏捷不是关于大型编程团队做大事情的大问题的大想法。大事情不会由大团队来完成，大事情是由许多小团队协作完成的。 在过去的几年中，敏捷所传达的简单而微小的信息已经变得模糊不清。它混合了精益、看板、LeSS、SAFe、Modern、Skilled 等概念。这些想法还不错，但它们不是最初敏捷表达的信息。是时候记住敏捷到底是什么了。 在这本书中，你不会发现什么特别的新东西，没有什么令人吃惊的东西，没有什么革命性的东西打破常规。你会发现它是敏捷的一个重述，就像它在 2000 年被告知的那样。在过去的 20 年里，我们学到了一些东西，我将把它们包括在内。但总的来说，这本书传达的信息是 2001 年和 1950 年的信息。 这是一个古老的信息。这是一个真实的信息。它给了我们一个小的解决方案，解决小的软件团队做一些小事情碰到的小问题。 什么是敏捷2001 年 2 月，在犹他州的 Snowbird 这个地方，由 17 位软件专家组成的小组谈论软件的糟糕状况发展。当时，大多数软件是使用低效的，繁重的，高礼节性的流程所构建的，例如 Waterfall 和 Rational Unified Process（RUP）。这 17 位专家的目标是创建一份宣言，介绍一种更有效、更轻量级的方法。 这绝非易事。这 17 个人有着不同的经历和强烈的不同观点。期望这样一个团体达成共识是不可能的。然而，尽管困难重重，大家还是达成了共识，并撰写了敏捷宣言，并且诞生了软件领域最具影响力和最持久的运动之一。 这样的运动在软件行业遵循可预测的路径。一开始有一小部分热情的支持者，另一部分热情的反对者，还有绝大多数的人并不关心。许多运动在那个阶段结束，或者至少永远不会结束。想想面向切面编程、逻辑编程或 CRC 卡。然而，有些跨越了鸿沟，变得格外受欢迎，也引起了争议。有些人甚至设法将争议抛诸脑后，而只是成为主流思想的一部分。面向对象就是后者的一个例子。敏捷也是如此。 不幸的是，一旦一个运动变得流行起来，它的名字就会因为误解和篡夺而变得模糊不清。与实际运动无关的产品和方法会借用名称，利用名称的知名度和重要性来赚钱。敏捷也是如此。 这本书是在 Snowbird 会议发生近 20 年后写的，其目的是澄清事实。这本书用尽可能务实的方式描述敏捷，没有废话，没有不确定的表述，没有不确定的术语。 这本书介绍了敏捷的基础。许多人美化和扩展了这些想法，这并没有错。然而，这些扩展和修饰都不是敏捷的。它们是敏捷加上其他的东西。你将在这里读到敏捷是什么，敏捷过去是什么，以及敏捷将不可避免地永远是什么。 敏捷历史Winston Royce 在 1970 年写了一篇论文，在其中描述了自己对如何管理大规模软件开发项目的想法。论文中包含了如下的一张图。 由于这个图和瀑布很像，因此这种技术被人们以“瀑布”命名所熟知。 瀑布软件开发模式源自于科学管理，它通过分析、制定详细的计划、执行计划来完成软件开发。Winston Royce 并没有推荐瀑布软件开发模式，人们从他的论文中拿走了瀑布软件开发模式的概念，然后瀑布软件开发模式统治了接下来的 30 年软件开发模式。 由于软件开发的特殊性，我们几乎不可能在开始就做好软件需求分析，软件架构设计，因为需求一直在发生变化。 在 1994 年，我第一次见到了 Kent Beck ，在 1999 年我再次见到了 Kent Beck ，在这之后我了解到了 Kent Beck 的极限编程，然后被极限编程所吸引。极限编程中的想法很有颠覆性。我在自己的公司引入了极限编程的训练营。在 2000 年的夏天 Kent Beck 邀请极限编程社区和设计模式社区一起组织会议，称之为极限编程领导会议（XP Leadership），讨论接下来极限编程应该如何发展，其中有一个想法是围绕极限编程创建一个非盈利性的组织。我同意这个想法，但是其他人不太同意这个想法。我有点受挫的离开了那个会议，Martin Fowler 也跟了出来，建议我们可以之后在芝加哥再开会讨论，我同意了。 在 2000 年的秋天，我和 Martin 见面讨论决定组织一个轻量级工作流程会议（Light Weight Process Summit），我们决定邀请了很多人。在我们的受邀人中，Alistair Cockburn 告诉我说他也在准备组织类似的会议，他把他的邀请人和我们的邀请人进行了合并。如果我们同意在 Snowbird 滑雪场组织会议，他愿意为组织会议做跑腿活。所以会议就定在了 Snowbird 举行。 Snowbird2001 年 17 位软件开发人员聚集在一起探讨寻找更好的软件开发方法，他们讨论得出了敏捷软件开发方法，并提出了如下的敏捷宣言： 个体和互动高于流程和工具。（Individuals and interactions over processes and tools） 工作的软件高于详尽的文档。（Working software over comprehensive documentation） 客户合作高于合同谈判。（Customer collaboration over contract negotiation） 响应变化高于遵循计划。（Responding to change over following a plan） 敏捷软件开发概览项目管理铁十字原则任何项目都只能从好、快、便宜、完成四个方面中选择三个。 现实中，好的项目管理者知道这四个方面可以有不同的系数，好的管理者带领着项目朝着足够好，足够快，足够便宜并且只完成必须功能的方向前进，他们不会把这四方面的系数都设置为 100%，这也是敏捷软件开发想要实现的管理方法。 敏捷软件开发就是这样一个帮助开发者和管理者执行这种实用项目管理的框架。 敏捷软件开发提供数据，管理者在做决策时需要的数据团队速率图 团队速率图表示团队每个迭代完成的用户故事点数 燃尽图 燃尽图表示项目用户故事点数的变化情况 传统软件开发老板确定了项目的截止日期。开会决定分析阶段所需时间、设计阶段所需时间、实现阶段所需时间。 分析阶段是一个很轻松欢乐的阶段，我们上上网，与客户聊聊天，当计划的时间结束时，我们结束了分析，“神奇”的完成了分析阶段。 设计阶段我们把项目分成多个模块，并设计它们之间的接口。当然，不可预知的事情也会发生，新的需求被添加进来，老的需求被移除或者修改，我们很想重新分析这些改变，但是由于时间紧迫，我们只能把这些改变 hack 进设计，当计划的时间结束，我们结束了设计，设计阶段也神奇的完成了。 实现阶段有明确的标准，我们没有办法来假装我们已经把实现阶段的工作做完了。在实现阶段，需求也在持续改变，新的需求会加入，旧的需求会被移除或修改。我们很想回去重新进行分析、重新设计这些改变，但是由于所剩时间不多，我们只能把这些改变一个接一个的 hack 进代码里。当我们回头把这些代码与设计对比时，发现我们的代码与设计与之前的设想已经相差甚远，但是我们已经没有时间去担心这些东西了。在与计划的交付日期还有两个星期时，我们告诉利益相关人（可能是经理、产品负责人、客户等），我们不能如期交付软件。你可以想像下利益相关人会作何反应。 最后我们压力倍增，继续完成未完成工作，这个阶段被称为死亡行军阶段（The Death March Phase）。我们告诉自己之后再也不会像这样一样做项目，下一次我们会做更多的分析，更多的设计。结果还是一样，因为我们的方法错了。 瀑布开发模式并不会摧毁每一个项目，但是它仍然是一种灾难性的软件项目开发方式。 敏捷软件开发项目开始于分析，但是分析从来都不会停止，我们把时间分隔正常的增量的小段，我们称它为迭代（iterations）或者冲刺（sprints），右侧是截止时间。迭代通常为一周或者二周。 第一个迭代通常被称为迭代 0，用来产生需求列表，被称作用户故事。迭代 0 也用来建立开发环境，评估用户故事，制定初步计划，该计划只是将故事分配给最初的几个迭代。最后，迭代 0 被开发人员和架构师用来根据暂定的故事清单来构想系统的初始暂定设计，编写用户故事，评估用户故事，计划用户故事和架构设计永远不会停止。每一个迭代的任何时间，都会有一些分析、设计与实现，在敏捷软件开发中，我们一直在分析和设计。迭代并不是一个小瀑布。 迭代 1 从评估本次迭代计划完成多少用户故事开始，然后团队开始工作，完成用户故事。在迭代结束时，我们完成了部分用户故事，这是我们对一次迭代中可以完成数量的首次测量，这是真实的数据，如果我们假定我们每一个迭代都相似，然后我们就可以调整我们的原始项目计划，重新计算一个新的项目完成日期。这可能会严重超过我们之前计划的截止时间，但这至少是真实数据，但是也不用太过认真，因为这仅仅是第一次迭代数据，随着迭代的进行，团队完成的点数，可能会变化，我们的调整可能会持续进行，直到它非常的稳定。 让他们失去希望是敏捷软件开发的主要目标，我们采用敏捷的目的就是为了在希望杀死项目之前摧毁希望，因为希望会导致团队误导管理者看不到项目的真实进度。敏捷软件开发引导项目走向最好的可能结果，可能这并不是最想要的结果，但这就是最好的可能结果。 管理铁十字原则 项目管理者需要决定项目应该多好，多快，多便宜和完成多少功能。通常管理者可以调整范围、时间、人员和质量。 改变时间，但有时因为商业原因，时间并不能更改。 增加人员，有数据表明增加人员的前几周并不能提高生产力，反而会降低生产力，后面生产力会逐渐增加。你只能寄希望于后面会补上前面丢失的生产力，并且增加人员，通常会增加预算。 降低质量，我们都知道通过停止写测试、停止做代码评审、停止做重构，仅仅写生产代码，可以加快速度。但是事实并不是这样的，不做这些看似没用的事情，不仅不会加快速度，反而会降低速度。如果你想走的更快，你应该先走好。如果你想减少项目时间，唯一的选项就是提高质量。 改变范围，有些需求可能并不需要在截止时间内完成。 业务价值优先级 我们会问利益相关者下一个我们应该实现的需求，我们只做利益相关者要求我们做的需求 上面描述的只是敏捷软件开发的大概，但这是敏捷的要点。每一个迭代的输出都是可以衡量的，用于持续评估时间表，需求按业务价值的顺序来实现，质量保持尽量的高，时间表主要靠改变需求范围来调整，这就是敏捷。 Circle of LifeXP（Extreme Programming）中文被译为极限编程。它最符合敏捷软件开发的要求。Ron Jeffries 总结了 XP 的实践图，这个图被亲切的称为 “Circle of Life” 。 外圈的环是面向业务的实践，本质上相当于 Scrum 。它提供了软件开发人员与业务人员的沟通框架，并且还提供了软件开发人员和业务人员管理项目的原则。 中间的环是面向团队的实践，这些实践提供了开发团队内部沟通和自我管理的原则和框架。 内圈的环代表了技术实践，指导和限制程序员保证尽可能高的技术质量。 总结敏捷就是用小的纪律帮助小团队管理小项目。 为什么要采用敏捷在我们深入讨论敏捷开发的细节之前，我想先解释一下其中的利害关系。敏捷开发不仅对软件开发很重要，而且对我们的行业、社会和最终的文明都很重要。 开发人员和管理人员常常因为一些短暂的原因而被敏捷开发所吸引。他们可能会尝试，因为他们只是觉得这样做是对的，或者他们可能会相信速度和质量的承诺。这些原因是无形的，模糊的，很容易挫败。许多人放弃敏捷开发仅仅是因为他们没有立即体验到他们认为的敏捷承诺的结果。 这些稍纵即逝的理由并不是敏捷开发重要的原因。敏捷开发对于更深层次的哲学和伦理原因非常重要。这些原因与我们的专业和客户的合理期望有关。 专业主义软件已经遍布我们生活的方方面面，我们统治了世界，我们的软件错误可能会给其他人带来灾难，我们应该更专业。 合理的期望 我们不制作质量不合格的软件。 软件一直处于技术就绪状态，软件可以随时发布。 稳定的软件生产效率，我们开发软件的速率稳定。 廉价的适应性，软件应该易修改。 持续改进，软件应该是持续改进的。 无所畏惧的能力，当我们修改代码时，我们需要有机制给我们足够的信心。 QA 应该找不到任何 BUG，当 QA 检查软件时，开发团队应该让 QA 找不出任何 BUG。 测试应该自动化。 开发人员之间的工作应该可以随时交换，保证每个人的工作都可以交由其他人来完成，防止突发情况。 诚实的用户故事时间评估。 在必要时说不，我们是专业的，我们应该在不合理的地方说不。 持续积极的学习。 指导别人，与别人一起相互指导学习。 权力法案客户权利法案 你有权利制定一个全面的计划，知道什么时候可以完成什么事情，要付出什么代价。 你有权从每个迭代中获得最大可能的价值。 你有权查看正在运行的系统的进展，并通过指定的可重复测试来验证其有效性。 你有权利改变你的想法，替换功能，改变优先级而不付出过高的成本。 你有权获知日程安排和评估变更，及时选择如何缩小范围以满足所需日期。你可以随时取消项目，并留下一个有用的可工作的系统，反映投资到目前为止的软件。 开发者权利法案 你有权利知道什么需求是需要的，并带有明确描述与优先级。 你有权在任何时候都做出高质量的作品，业务不能强迫你降低质量。 你有权向同事、经理和客户寻求帮助并接受他们的帮助。 你有权作出和更新自己的评估，你可以在发现新因素时更改估算值，评估并不是承诺。 你有权利接受你的责任，而不是被分配给你，你有权力拒绝。 总结敏捷是一个支持专业软件开发的纪律框架。敏捷不是一个流程，也不是时尚，也不仅仅是一些规则的集合。敏捷是一组权力、期望和纪律，它们构成了职业道德的基础。 业务实践为了取得成功，开发必须遵循大量面向业务的实践。这些包括计划、小版本发布、验收测试和团队协作。 计划项目时间评估如果你想要一个准确而又精确的项目时间评估，你就需要把项目逐步分解，分解到越小越好，你甚至可以分解到单独的实现代码行数。你做这些的时间就是项目的精确预估时间，因为你所做的这些就是在开发实现一个软件。 评估就是猜测，我们想在不实际开发这个软件的情况下，猜测项目会花费的时间，所以这肯定是不精确的，想更精确，你花费在评估上的时间就要更多，这需要我们自己选择。 关于评估我们可以了解三变量评估和 PERT 评估方法。 用户故事与点数用户故事是从用户的角度描述系统特性的简短描述，例如：作为一名汽车司机，为了提高我的速度，我会更用力地踩油门踏板。（As the driver of a car, in order to increase my velocity, I will press my foot harder on the accelerator pedal） 通常，我们把故事写在索引卡上，不一定非要使用软件工具。 ATM 用户故事示例用户故事 取钱（Withdrawal） 存钱（Deposit） 转账（Transfer） 登录（Login） 登出（Logout） 在迭代 0，我们团队写出了以上的 5 个用户故事， 我们也讨论了这些用户故事的细节，比如用户使用密码登录等，但是我们不相信这些细节，我们没有把他们写在故事卡上，我们在故事卡上只写了上面简短的单词。 用户故事评估 现在有了用户故事，开发、测试、项目管理或其他利益相关者一起开会来进行用户故事的评估，这样的会议会很多，每当有新的用户故事或者对之前的用户故事有了新的理解时，我们就会开用户故事评估相关的会议，这个会议是非正式的，每个迭代都会进行。 由于这是第一次用户故事评估，我们选择一个我们认为复杂度平均的用户故事来开始进行评估，我们选择登录用户故事，我们要求利益相关者回顾之前讨论的细节，这样我们就都能了解上下文，之后我们为这个用户故事选择一个数字 3，为什么是 3 呢？登录用户故事是一个平均故事，所以我们给它一个平均的数字，如果我们选择 1 到 6 来表用户故事评估，那 3 就是那个平均数字。 登录用户故事现在是我们的 Golden Story，之后所有的用户故事都会与之比较来进行评估，因此我们得出如下评估，登出 1、取钱 6、存钱 5、转账 3，我们把用户故事评估写在用户故事卡上。 用户故事的评估的数字并不表示周、天、时等其他时间单位，它只是个相对数字，只是表示需要付出努力的单位，和实际时间没有关系，可能有的人需要一天，有的人需要两天。 计划迭代 1 迭代以迭代计划会议（Iteration Planning Meeting (IPM)）开始，这个会议应该花费大约整个迭代 1/20 的时间，所有的团队成员都需要参加这个 IPM 会议，包括利益相关者、程序员、测试、项目经理。利益相关者查看用户故事，并按业务价值给它们排序。 利益相关者的主要工作是选出程序员和测试人员在这个迭代将要完成的用户故事，因此，他们需要知道程序员认为他们能完成多少，这个数字就是速率，由于这是第一个迭代，我们并不知道速率，所以我们随便猜一个数字，比如：30。 必须要说明的是，速率并不是承诺，他们甚至不是试着去完成 30 点，它只是个猜测。 投资回报 ROI （return on investment）和用户故事优先级评估。 中点检查 在迭代的时间中点，我们发现我们只完成了 10 点，那利益相关者就需要从迭代中去除 10 点的用户故事。 到迭代结束可能只完成了 18 点，但这并不表示这个迭代失败了，一个迭代的目的是为了给管理者产生数据。 昨天的天气 现在我们知道了我们一个迭代可以完成 18 点，在下一个迭代我们应该计划 18 点。 今天天气的最佳预报是昨天的天气，迭代进展的最佳预测器是前一个迭代。 在 IPM 会议上，利益相关者选择 18 点的用户故事，这一次可能奇怪的事情发生了，在这个迭代，中点检查时发现已经完成了 12 点，因此利益相关者又增加了 6 点用户故事，总计划 24 点，可能结果我们完成了 22 点，因此下一个迭代就设置为 22 点。 项目结束 随着迭代持续进行，速率被持续增加到速率图中，每个人都知道我们有多快。 可能到了某个阶段，项目并没有实现所有的用户故事，但是项目却结束了，因为根据 ROI 原则，已经没有更多的用户故事值得去实现了，最早被写出来的用户故事的重要性可能早已消失不见了。 用户故事用户故事应该遵循 INVEST 原则： I: Independent 独立，用户故事之间应该相互独立，这表示他们不需要以特定的顺序实现，登出不能要求登录先实现。 N: Negotiable 可协商，开发者可以和业务协商具体细节。 V: Valuable 有价值，用户故事必须对业务具有清晰和可量化的价值。 E: Estimable 可评估，用户故事必须具体到开发者可以评估。 S: Small 足够小，用户故事不应该大到需要一到两个开者一个单独迭代还不能实现。 T: Testable 可测试，业务应该能够清晰地写测试，以证明故事已经完成。 用户故事评估方法 1：Flying Fingers 开发者坐围着桌子坐，阅读用户故事并与利益相关者讨论（如果需要的话），然后开发者在背后伸出手指数量，然后所有开发者同时亮出手指，某个人统计所有开发者表示的点数，如果分歧很小并且有一个明显的平均值，记录下用户故事的评估，如果不统一，再次讨论，再次进行打分。 大拇指向下表达 0 大拇指向上表达 ∞ 打开手掌表达 ? 方法 2：Planning Poker 通常使用斐波那契数列，?, 0, ½, 1, 2, 3, 5, 8, 13, 20, 40, 100, ∞。 0 表示太过琐碎无法评估，可以把几个用户故事合并。 ∞ 表示太大无法评估，用户故事应该被拆分。 ? 表示你不知道，你需要一个 spike。 Spike 一个 spike 是一个元故事，或者更确切地说，是一个用来估计一个故事的故事。你需要先去了解这个技术，然后才能对当前的故事做出评估，例如：你可以写一个用户故事为评估打印 PDF。 管理迭代每一个迭代的目的是通过完成用户故事来产生数据，团队应该着力于用户故事，而不是用户故事里的任务，完成了 80% 的用户故事，远好于把每个用户故事都完成了 80%。 管理者不要分配用户故事，而应该让他们自己来协商选择用户故事。 验收测试应该尽早写，尽量在迭代中点之前完成所有验收测试代码，QA 应该和程序员紧密配合。 真正的完成就是验收测试通过。 小版本发布小步快跑，才能跑的更快。把发布和部署分开，发布表示软件已经准备就绪，可以部署，部署只是业务方面的决定。 验收测试验收测试是最少被理解、最少被使用、最混乱的敏捷实践。其基本思想非常简单：需求应该由业务来指定。 验收测试是一种规范，它也是一个测试，例如：当用户输入有效的用户名和密码，然后单击登录，系统将显示欢迎页面。 工具和方法 FitNesse, JBehave, SpecFlow, Cucumber BDD(Behavior-Driven Development) 实践 验收测试由业务分析人员和 QA 在迭代中点之前编写。开发人员把这些测试集成进持续构建。这些测试就是用户故事完成的定义，只有通过这些测试才表示用户故事完成。 业务分析人员指定正常业务路径，QA 编写异常业务路径，开发者与业务分析人员和 QA 一起确保从技术角度来看这些测试是有意义的。 QA 不在是在最后阶段才进入保证质量，他们在每一个迭代的开始就介入开发团队来阻止错误和遗漏，最后他们来决定软件是否可以部署。 运行测试的工作应该由程序员来做，只有运行测试通过才表示他们的用户故事完成了，当然程序员可以通过持续构建来自动化这一测试过程。 团队协作 团队所有成员应该坐在一起工作。 保证团队可以随时面对面交流。 远程办公也是可以的，只要能实时面对面沟通即可。 总结敏捷想打破业务和开发团队之间的鸿沟，让业务和开发能更好的合作，面向业务的实践在满足这个目标方面扮演了重要的角色。通过遵循这些实践，业务和开发有了一种简单而明确的沟通方式。这种交流产生了信任。 团队实践Ron Jeffries 的 Circle of Life 中间部分由敏捷团队实践组成。这些实践控制着团队成员之间以及他们所创建的产品之间的关系。我们将讨论的实践包括隐喻、可持续的速度、集体所有制和持续集成。 然后，我们将简要讨论所谓的站立会议（Standup Meetings）。 隐喻寻找可以形象比喻项目或者项目中模块组件的词语，然后团队使用这个词语交流。领域驱动设计（Domain-Driven Design）使用统一语言（Ubiquitous Language）方便团队交流。 可持续的速度 开发团队可持续的开发速度很重要。 通过加班来提高开发速度不可持续，可能会在加班期间做错误的决定，写错误的代码，最后反而会起反效果。 软件开发是一个马拉松过程，我们不能在早期就把体力用的过快，我们要维持可持续的速度。 开发工作不是体力劳动，加班并不能说明你工作努力且专业。 要保证有足够的睡眠。 偶尔加班是可以的，但这不能是常态。 集体所有制代码集体所有，任何人随时都可以查看获取修改代码。 持续集成 尽早的进行代码集成，尽早的发现错误，修改错误。 持续集成的极致是每次的代码提交都进行集成。 当持续集成失败时，所有人都不能提交代码，直到持续集成被修复。 当持续集成失败时，可以发报警邮件，甚至可以设置报警灯和报警声音。 每日站会指导原则 会议是可选的。 可以不用每天都开，有意义时才需要开。 会议应该小 10 分钟，即使大团队。 发言规则 上次会议到现在我做了什么。 到下次会议之前我会做什么。 我碰到了什么困难。 会议上不允许讨论，不允许深入解释。每个人都可以发言，包括经理等，只要他们遵守规则。你也可以在会议是感谢给你帮助的其他人。 总结敏捷就是一组原则、实践和纪律，帮助小的团队构建小的软件项目，本章节的实践帮助小团队表现的像一个真正的团队。帮助团队构建他们的交流语言，以及团队成员如何对待彼此和他们正在构建的项目的期望。 技术实践本章中描述的实践与过去 70 年中大多数程序员的行为方式大不相同。它们强制执行一组深刻的、以分钟为单位的、以秒为单位的行为，大多数程序员最初都会认为这是荒谬的仪式性行为。因此，许多程序员试图在没有这些实践的情况下实现敏捷。然而，他们失败了，因为这些实践是敏捷的核心。没有 TDD，没有重构，没有简单设计，甚至没有结对编程，敏捷就变成了一个没有效率的松散外壳。 测试驱动开发（Test-Driven Development）先写测试代码，然后编写生产代码让测试通过，然后重构改善代码。 当使用 TDD 时，每一个需要的行为都被输入了两次，一次作为测试，一次作为使测试通过的生产代码，它们是互补的，当一起执行的时候，产生 0 结果，0 个测试失败。 TDD 三原则 在编写失败的测试代码之前，不要写任何生产代码。 在有测试失败的情况下不要再写测试代码，编译失败也是失败的测试。 只编写足够的生产代码来让测试通过。 没有经过数月练习 TDD 的程序员可能觉得这些原则有些怪异，甚至无法接受。 使用 TDD 的程序员很少 Debugging，因为新的代码都是几分前引入的，那么错误也就是这段时间引入的，很容易找到出错的代码。 如果你遵循了 TDD 原则，你写的测试可能就是软件最好的文档，测试里有软件或者库的多种使用方式。 每一个新测试都是一个挑战，每一次你写代码让测试通过，你就完成了一个挑战，这样一直下去你会感觉这不像忙碌的工作，感觉像让东西工作起来。 遵守三原则会给你一个完善的测试套件，但是这并不是 100% 完整的，并不是只有达到 100% 代码覆盖率才能进行软件部署，90% 的覆盖率已经很不错了，不要把代码覆盖率当作目标和管理指标。 由于先编写测试，你需要让你们代码更容易测试，因为松耦合的代码更容易测试，所以你需要解耦代码，这样你的代码设计也会更好。 由于测试比较完整，当你看到需要改善的代码时，你可以放心的修改它，因为有测试代码帮你验证，你的改动有没有影响到之前的功能。 重构（Refactoring）在不改变软件外在行为的情况下，改善代码的内部设计。 重构与 TDD 密切相关，为了不害怕重构代码，我们需要完整的测试来给我们非常高的信心，以保证我们的修改不会破坏之前的功能。 红绿重构 首先，我们创建一个失败的测试。 然后，我们编写代码让测试通过。 然后， 我们重构让代码变的整洁。 回到开始的步骤。 我们不预留时间来进行大的重构，我们以一次一小步的方式的迁移代码，同时继续添加新功能，这个改变可能会持续数天、数周甚至数月，在这期间，系统可以一直通过测试，并且可以部署到生产环境。 简单设计（Simple Design）Kent Beck 的简单设计原则 通过所有测试：完成所有功能。 表明意图：之后要考虑重构让代码能表达程序员的意图，代码要易于阅读，自描述。 去除重复：之后要考虑重构去除重复代码，可能会使用到设计模式等。 减少元素：最后考虑减少代码元素，比如类、函数、变量等。 设计的重量 设计越复杂，程序员的认知负担就越大，程序员就越难理解和操作系统，这就是设计的重量。程序员持续重构系统来保持需求与简单设计的平衡，保持最大的生产力。 结对编程（Pair Programming）两个人在同一个编程问题上工作，他们可能分享屏幕、键盘和鼠标，只要他们看和操作同一块的代码即可。结对编程有时会用不同的角色。 一个是司机，一个是导航员，司机有键盘和鼠标，导航员会有更长远的视角并给出建议。 一个程序员写测试代码，别一个程序员写生产代码让测试通过，然后交换角色继续。 最常见的情况是根本没有角色。程序员只是以协作的方式共享鼠标和键盘的作者。 结对编程不是按期进行的，程序员根据自己的偏好来决定。结对编程是短期的，一次结对编程可能持续一天，但是通常情况下不会多于一到两个小时。 用户故事不是分配给结对编程的，用户故事是分配给独立的开发者的。 在一周的时间内，每个程序员将花费大约一半的结对时间来完成自己的任务，并寻求其他几个人的帮助。另一半的配对时间将用于帮助其他人完成任务。 高级程序员应该注意与低级程序员结对编程，而不是与其他高级程序员结对编程。低级程序员应该更多地请求高级程序员的帮助。 具有专业技能的程序员应该花费大量的结对时间与他们专业以外的程序员一起工作。目的是传播和交换知识，防止形成知识简仓、知识孤岛。 结对编程是在团队成员之间共享知识和防止知识孤岛形成的最佳方式。 结对编程能减少了错误并提高设计质量。 结对编程是另一种形式的代码评审。 结对编程可能会多花 15% 的编码时间，一个简单的计算表明，一个团队 50% 的时间是结对的，那么它的生产力就会降低 8% 以下。另一方面，如果结对的实践代替了代码评审，那么很可能根本不会降低生产率。 结对编程并不只能有两个人。 管理者不要干涉结对编程，相信程序员。程序员也永远不要向管理者请求结对、测试和重构的时间，你是专家，你应该自己决定。 总结敏捷的技术实践是任何敏捷工作中最重要的组成部分。任何没有技术实践的敏捷实践尝试都注定要失败。原因很简单，敏捷是一种高效的机制，可以在很短的时间内把事情搞得一团糟。如果没有技术实践来保持高技术质量，团队的生产力将很快衰退并开始一个不可避免的死亡螺旋。 实施敏捷当我第一次学习 XP 时，我想，还有什么比这更容易的呢？只需遵循一些简单的原则和实践。仅此而已。 然而，基于尝试敏捷却失败的组织的数量，成为敏捷肯定是非常非常困难的。也许所有这些失败的原因是许多组织都错误的认识了敏捷，他们认为的敏捷和敏捷原本的思想不同。 敏捷价值勇气 部署最小的特性集需要勇气，维护高代码质量和高质量纪律也需要勇气。认为质量和纪律可以提高速度的信念是一种勇敢的信念，因为它将不断受到有权势但天真的人的挑战。 沟通 团队各成员可以更方便的面对面的、非正式的、人际间的对话。一个坐在一起并且交流频繁的团队可以创造奇迹。 反馈 敏捷原则实际上都是为那些需要做出重要决策的人提供快速的反馈。它们使我们能够尽早判断出什么时候出了问题，以便及时纠正。敏捷团队在反馈中茁壮成长。反馈是使团队高效工作的因素，也是推动项目取得有益成果的因素。 简单 简单性是指代码的直接性，以及沟通和行为的直接性。在代码中，一定数量的间接是必要的。间接是我们减少相互依赖复杂性的机制。在团队中，更少的间接是必要的。大多数时候，你想要尽可能的直接。保持代码简单，让团队更简单。 怪物博物馆众多的敏捷方法可能让你眼花缭乱，无论选择哪种方法，最终都将调整它以满足自己团队的需要。我所能给你的最有力的建议是充分采纳 Circle of Life，尤其是技术实践。选择一个方法，或者不选。确保你充分采纳了 Circle of Life。让团队同意。然后开始，记住勇气、沟通、反馈和简单，并定期调整规则和行为。不要请求许可。不要强调要把事情做好。只要在问题出现时解决它们，并继续将项目推向最佳结果。 转型敏捷的价值观与中层管理的职责完全相反，高管也常常被敏捷的冒险、直接、交流的价值观所驱动。这是他们试图转型的原因之一。障碍是中间的管理层。这些人被雇佣来不承担风险，避免直接，以最少的沟通来遵循和执行命令链。这就是组织的困境。组织的顶层和底层重视敏捷思维，但是中间层反对它。 敏捷团队能够存在于一个拥有强大的反对敏捷的中层管理层的组织中吗？我曾见过这种情况。一些软件开发团队悄悄地使用敏捷价值来驱动他们的开发，同时也遵从中层管理强加给他们的严格要求。只要中层管理人员对他们遵循的过程和标准感到满意，他们就可以让开发团队自行处理。团队在幕后执行敏捷，同时提供满足中级管理层的一切。这些团队并没有与中层管理者进行一场徒劳的战斗，而是在敏捷之上增加了一层，使得敏捷看起来更安全，更符合中层管理者的需求。 敏捷转型在小的组织中更容易成功。 教练敏捷培训师教导团队如何以敏捷的方式管理自己，他们经常是从企业外部人员或者团队外部人员。他们的任期应该很短。每个由十几个开发人员组成的团队应该只需要一到两周的培训。其他的一切他们需要学习的都要自学，不管敏捷培训师说什么或做什么。 敏捷教练不是培训师。他们是团队的成员，他们的角色是在团队中维护流程。在开发的高峰期，开发人员可能会不遵守敏捷流程。也许他们无意中停止了结对，停止了重构，或者忽略了连续构建中的失败。教练的工作就是看到这一点并把它指出来。教练作为团队的良心，总是提醒团队他们对自己的承诺和他们同意持有的价值观。 在一个团队转变的早期，培训师可能会临时填补教练的角色，但这是一个临时的情况。这个角色应该尽快在团队中选择，该角色通常根据需要按照非正式的时间表从一个团队成员轮换到下一个成员。一个成熟的团队稳步前进不需要教练。另一方面，一个团队在某种压力下，无论是日程安排、业务，还是人际关系，可能会决定让某个人暂时填补这个角色。 教练不是经理。教练不负责预算或时间表。教练不指导团队，也不向管理层代表团队的利益。教练不是客户和开发者之间的联络人。教练的角色是团队内部的。经理和客户都不知道教练是谁，甚至不知道现在是否有教练。 认证现有的敏捷认证完全是一个笑话，完全是荒谬的。不要把认证当回事。伴随认证项目的培训通常是值得的。然而，培训不应该专注于一个特定的角色，它应该适合团队中的每个人。 一个真正的敏捷认证项目应该是什么样的?这将是一个学期的课程，包括敏捷培训和一个小型敏捷项目的监督开发。这门课将被评分，学生们将被严格要求。认证人员将确保学生理解敏捷的价值，并在执行敏捷规程方面表现出熟练。 大型敏捷敏捷团队只是众多需要在大型项目中协调的团队之一。不同团队的整合是一个已经解决的问题。我没有看到任何迹象表明软件团队的唯一性会过度地影响到他们对大型团队的集成。 在大范围内不存在敏捷这样的事情。敏捷是组织小型软件团队的必要创新。但是一旦组织起来，这些团队就适应了大型组织使用了数千年的结构。 敏捷工具充分了解工具，才能用好工具，使用不当的工具甚至会对项目及其操作者造成伤害。 好的工具应该有如下特性： 帮助人们实现他们的目标。 能很快学好。 对用户透明。 允许适配和扩展。 可负担得起。 物理敏捷工具。敏捷使用者以使用白板、胶带、索引卡、记号笔和各种大小的便利贴(小的和翻页的)来对工作进行可视化管理而闻名。这些简单的手工工具具备所有伟大工具的品质： 它们有助于使正在进行的工作可见并易于管理。 它们是符合直觉的，不需要训练。 它们只需要微不足道的认知开销。你可以在专注于其他任务时轻松地使用它们。 它们都不是专享的。这些工具都不是专门为管理软件开发而设计的。 它们适应性好。你可以用胶带或油灰粘在上面，把图片或图标夹在上面，把其他的指示符粘在上面，通过新颖的自定义颜色和图标来增加意义上的细微差别。 它们都很便宜，很容易买到。 使用自动化工具的压力： 软件工具提供了一种帮助确保以一致的形式捕获数据的好方法。 使用一致捕获的数据，您可以轻松地获得看起来很专业的报告、图表和图形。 提供历史记录和安全存储很容易。 你可以立即与每个人共享信息，无论他们居住在哪里。 使用在线电子表格之类的工具，你甚至可以让一个完全分布式的团队实时协作。 考虑到大多数 ALM（Agile Lifecycle Management） 工具的当前状态，从物理工具开始可能更安全、更明智。之后，你可以考虑使用 ALM 工具。确保学习速度快，日常使用透明，容易适应，并在你的能力范围内获得和运行。最重要的是，确保它支持您的团队的工作方式，并为您的投资提供积极的回报。 教练（另一种观点）敏捷教练带领团队走向敏捷。 总结在很多方面，这一章更多的是关于不做什么，而不是做什么。也许这是因为我见过太多不去敏捷的例子。但是，我仍然认为，就像我 20 年前想的那样，还有什么比这更容易的呢?只需遵循一些简单的原则和实践。仅此而已。 匠艺兴奋。这就是许多开发人员第一次听说敏捷时的感受。对于我们大多数来自软件工厂和瀑布思想的开发人员来说，敏捷是解放的希望。我们希望在一个协作的环境中工作，我们的意见能够得到倾听和尊重。我们将有更好的工作流程和实践。我们将在小的迭代和短的反馈循环中工作。我们将定期将应用程序发布到生产环境中。我们会与用户互动并得到他们的反馈。我们会不断地检查和调整。 一开始，我们觉得敏捷好得让人难以置信。我们认为我们的公司永远不会接受敏捷思维，更不用说敏捷实践了。但他们大多数人都这么做了，我们对此感到非常惊讶。突然，一切都变了。我们有产品 backlog 和用户故事，而不是需求文档。我们有物理看板和燃尽图，而不是甘特图。我们有便利贴，我们每天早上根据进度来移动它们。这些便利贴有一种强大的力量，它能引发一种深深的心理瘾。他们是我们敏捷性的代表。我们贴在墙上的便签越多，我们就越觉得自己敏捷。我们变成了一个 Scrum 团队，而不是一个构建团队。我们再也没有项目经理了。我们被告知我们不需要管理，我们的经理将成为产品所有者，我们将自我管理。我们被告知，产品所有者和开发人员将作为一个单独的团队密切协作。从现在开始，作为 Scrum 团队，我们不仅被授权做出技术决策，还被授权做出与项目相关的决策。我们是这么想的。 敏捷席卷了软件行业。但是，就像在中国的耳语游戏中一样，最初的敏捷思想被扭曲和简化了，在公司看来，这是一个更快交付软件的过程的承诺。对于使用瀑布或 RUP 的公司和管理人员来说，这就是他们喜欢的音乐。经理和利益相关者都很兴奋。说到底，谁不想变得敏捷呢?谁不想更快地交付软件呢？即使是持怀疑态度的人，也不能拒绝敏捷。如果你的竞争对手在宣传他们是敏捷的，而你不是，那么这又会给你带来什么呢？你的潜在客户会怎么看你？公司不能承担不敏捷的后果。在敏捷峰会之后的几年里，全世界的公司都开始了他们的敏捷转型。敏捷转变的时代已经开始了。 敏捷的宿醉敏捷的转变过程并不容易，公司需要借助外部的帮助，敏捷教练这一职位的数量大量需要，出现了许多敏捷相关的认证，这其中的大多数证书都很容易获得。 向中层经理推销敏捷过程很容易，他们都希望软件能够更快地交付。经理们被告知，工程是容易的部分，如果我们修正了流程，工程就会被修正。这一直是人的问题。然后经理们相信了。 希望推动开发人员更快工作的管理人员正在使用过程的完全透明性来对他们进行微管理。既没有业务经验也没有技术经验的敏捷教练是在指导经理并告诉开发团队该做什么。路线图和里程碑是由经理定义的，并强制开发团队开发人员可以评估工作，但是他们很难将自己的评估纳入强加的里程碑中。在接下来的 6 到 12 个月中，经常可以看到项目的所有迭代和各自的用户场景已经被管理层定义。未能在 sprint 中交付所有的故事点意味着开发人员必须在下一个 sprint 中更加努力地工作以弥补延迟。日常的站立会议变成了开发人员必须向产品负责人和敏捷教练报告进展的会议，详细说明他们正在做什么，什么时候完成。如果产品负责人认为开发人员在自动化测试、重构或结对之类的事情上花费了太多时间，他们只会告诉团队停止这样做。战略技术工作在他们的敏捷过程中没有地位。不需要架构或设计。顺序是简单地将重点放在待办事项列表中优先级最高的项上，然后尽快完成一个又一个优先级最高的项。这种方法导致了一长串迭代的战术工作和技术债务的积累。脆弱的软件，著名的单体(或尝试微服务的团队的分布式单体)成为规范。bug 和操作问题是日常站立会议和回顾会议中的热门讨论主题。发布到产品中的频率不像业务预期的那么频繁。手工测试周期仍然需要几天(如果不是几周的话)才能完成。采用敏捷可以避免所有这些问题的希望已经破灭了。经理们指责开发人员行动不够迅速。开发人员指责管理人员不允许他们完成所需的技术和战略工作。产品负责人不认为自己是团队的一部分，当事情出错时也不承担责任。“我们对他们”的文化占据了主导地位。这就是我们所谓的敏捷宿醉。 经过多年的敏捷转型投资，公司意识到他们仍然遇到许多以前的问题。 当然，敏捷也因此而受到指责。 期望不匹配纯粹关注过程的敏捷转换是部分转换。虽然敏捷教练试图通过敏捷过程来指导经理和交付团队，但是没有人帮助开发人员学习敏捷技术实践和工程。修正人们之间的协作将改进工程的假设是大错特错的。 敏捷的采用带来了一个很大的期望：开发团队应该在完成一个特性时，或者至少在每次迭代结束时，交付准备好生产的软件。对于大多数开发团队来说，这是一个重要的变化。如果不改变他们的工作方式，他们是不可能做到这一点的，这意味着学习和掌握新的实践。但也有一些问题。在敏捷转换期间，很少有用于提高开发人员技能的预算。业务部门并不期望开发人员在采用敏捷时放慢速度。大多数人甚至不知道开发人员必须学习新的实践。他们被告知，如果他们以一种更协作的方式工作，开发人员的工作速度会更快。 认为团队仅仅通过创建一个更具协作性的环境来开发这些技能是不现实的。团队在获取这些技术技能时需要支持。这种支持可以通过指导、培训、实验和自学的结合来实现。业务敏捷性与公司发展软件的速度直接相关，这意味着他们的工程技能和技术实践的发展。 远离对于一些采用敏捷管理的公司，尽管公司确实比以前更好了，但是敏捷过程和工程之间的分歧仍然在伤害着他们。大多数现代敏捷教练没有足够的(如果有的话)技术技能来指导开发人员进行技术实践，而且他们很少谈论工程。多年来，开发人员开始将敏捷教练视为另一层管理：人们告诉他们做什么，而不是帮助他们更好地完成工作。 随着对技术技能的关注越来越少，敏捷是否能够显著地改进软件项目？敏捷是否仍然像敏捷宣言中所写的那样，专注于通过开发和帮助其他人来发现更好的软件开发方法?我不太确定。 软件匠艺为了提高专业软件开发的标准并重新确立一些最初的敏捷目标，一组开发人员于 2008 年 11 月在芝加哥开会，创建了一个新的运动:软件工艺。在那次会议上，与 2001 年敏捷峰会期间的情况类似，他们就一套核心价值观达成了一致，并提出了一份新的宣言。 As aspiring Software Craftsmen we are raising the bar of professional software development by practicing it and helping others learn the craft. Through this work we have come to value: Not only working software, but also well-crafted software. Not only responding to change, but also steadily adding value. Not only individuals and interactions, but also a community of professionals. Not only customer collaboration, but also productive partnerships. That is, in pursuit of the items on the left we have found the items on the right to be indispensable. 作为有理想的软件工匠，我们一直身体力行，提升专业软件开发的标准，并帮助他人学习此工艺。通过这些工作，我们建立了如下价值观： 不仅要让软件工作，更要精益求精。 不仅要响应变化，更要稳步增加价值。 不仅要有个体与交互，更要形成专业人员的社区。 不仅要与客户合作，更要建立卓有成效的伙伴关系。 也就是说，左项固然值得追求，右项同样不可或缺。 精心编写的软件意味着经过良好设计和测试的代码。我们并不害怕更改代码，正是这些代码使业务能够快速做出反应。它是既灵活又健壮的代码。 稳步增值意味着无论我们做什么，我们都应该致力于不断为客户和雇主提供增值服务。 一个由专业人士组成的社区意味着我们需要互相分享和学习，从而提高我们行业的水平。我们负责培养下一代的开发人员。 富有成效的伙伴关系意味着我们将与客户和雇主建立专业关系。我们将始终保持职业道德和尊重的态度，以最好的方式为客户和雇主提供建议和工作。我们期待相互尊重和专业的关系，即使我们需要采取主动，以身作则。 思想与方法意识形态是一种思想和理想的体系。方法学是方法和实践的系统。意识形态定义了目标的理想。一种或多种方法可以用来达到这些理想，它们是达到目的的手段。当我们看到敏捷宣言和 12 条原则时，我们可以清楚地看到它们背后的意识形态。 敏捷的主要目标是提供业务敏捷性和客户满意度，这是通过紧密协作，迭代开发，较短的反馈循环和卓越的技术来实现的。 诸如 Scrum，极限编程（XP），动态系统开发方法（DSDM），自适应软件开发（ASD），Crystal 方法，功能驱动开发（FDD）和其他敏捷方法之类的方法都是为了达到同一目的。 方法和实践就像训练轮，他们很容易带动人们。 与学习骑自行车的孩子一样，训练轮使他们能够以安全且受控的方式上手。 一旦他们更加自信，我们就会稍微抬高训练轮，以便他们练习平衡。 然后，我们将其中一个训练轮取下。 然后另一个。 此时，孩子已准备好独自行走。 但是，如果我们过多地关注训练轮的重要性，并且将其保持太长时间，则孩子会过于依赖训练轮，不希望将其卸下。 对方法论或一组实践的过分关注使团队和组织偏离了他们的实际目标。 目的是教孩子骑脚踏车，而不要使用辅助轮。 软件匠艺有实践吗软件匠艺社区认为 XP 是当前可用的最佳敏捷开发实践集。 TDD，重构，简单设计，持续集成和结对编程在软件匠艺社区中得到了大力倡导-但它们是 XP 的实践，而不是匠艺的实践。 它们不是唯一的做法。 匠艺还倡导清洁规范和 SOLID 原则。 它促进小提交，小发布和持续交付。 它促进了软件设计和任何自动化类型的模块化，从而消除了手动和重复的工作。 而且，它倡导任何可提高生产率，降低风险并有助于生产有价值，强大而灵活的软件的实践。 匠艺不仅仅涉及技术实践，工程和自我完善。 这也与专业精神有关，并使客户能够实现其业务目标。 这是敏捷，精益和手工艺完美融合的领域。 这三个目标都有相似的目标，但从不同但同样重要和互补的角度解决问题。 关注价值，而不是实践敏捷和软件匠艺社区中的一个常见错误是推广实践而不是其提供的价值。 让我们以 TDD 为例。 在软件匠艺社区中最常见的问题之一是“我如何说服我的经理/同事/团队进行 TDD？”这是一个错误的问题。 这里的问题是我们在达成一致意见之前就提供了解决方案。 如果人们看不到价值，人们将不会改变他们的工作方式。 在讨论实践时，首先要商定要实现的目标至关重要。 唯一不应该接受的事情是拒绝实践，而不提供更好的选择。 讨论实践有关实践的讨论应在适当的级别和适当的人员进行。如果我们想采用改善业务与技术之间协作的实践，则应将业务和技术领域的人员参与其中。如果开发人员正在讨论使他们能够以更好的方式构建系统的实践，则没有理由让业务人员参与其中。仅在项目成本或项目持续时间有重大影响时，才应参与业务人员。 开发人员不应要求编写测试的授权。他们不应为单元测试或重构承担单独的任务。这些技术活动应纳入任何功能的开发之中。它们不是可选的。经理和开发人员应该只讨论将交付什么以及何时交付，而不是如何交付。每次开发人员自愿提供有关工作方式的详细信息时，他们都会邀请经理对其进行微观管理。 我们是说开发人员应该隐藏他们的工作方式吗？一点都不。开发人员应该能够向感兴趣的人清楚地描述他们的工作方式以及以这种方式工作的优势。开发人员不应该做的是让其他人决定他们的工作方式。开发人员与企业之间的对话应该是关于为什么，什么以及何时进行的，而不是如何进行的。 匠艺对个人的影响人们通常将自己的生活与职业生活区分开来。诸如“离开办公室后我不想谈论工作”或“我对生活有不同的兴趣”等短语的表达方式使工作看起来像家务琐事，一件坏事或者你必须要做的事情，而不是你想要做的事情。将我们的生活分成多种生活的问题是，他们一直在发生冲突。总是有一种感觉，无论我们选择哪种生活，我们都必须牺牲另一种生活。 匠艺可以促进软件开发作为一种职业。工作和职业是有区别的。工作是我们要做的事情，但不是我们自己的一部分。专业是我们的一部分。专业是我们投资的东西。我们想要变得更好。我们希望获得更多技能，并拥有长期而充实的职业。 这并不意味着我们不会与家人在一起，也不会在生活中拥有其他利益。相反，这意味着我们将找到一种平衡所有承诺和利益的方法。有时候，我们想更加关注我们的家庭，我们的职业或我们可能有的爱好。那完全可以。我们在不同的时期有不同的需求。但是，当我们有专业时，上班不应该是一件繁琐的事。它应该是给我们带来快乐的事情，并使我们成为个人。专业赋予我们生活以意义。 匠艺对行业的影响自 2008 年以来，在世界范围内组织了越来越多的软件匠艺社区和会议，吸引了成千上万的开发人员。敏捷社区侧重于软件项目的人员和流程方面，而匠艺社区则更侧重于技术方面。它们是向全球许多开发人员和公司推广 XP 和其他许多技术实践的关键。通过软件匠艺社区，许多开发人员正在学习 TDD，持续集成，结对编程，简单设计，SOLID 原理，简洁代码和重构。他们还学习如何使用微服务构建系统，如何自动化其部署管道以及如何将其系统迁移到云。他们正在学习不同的编程语言和范例。他们正在学习新技术以及测试和维护其应用程序的不同方法。匠艺社区的开发人员正在创建安全和友好的空间，在那里他们可以结识志趣相投的人并谈论他们的职业。 软件匠艺社区极为包容。从一开始，软件匠艺的主要目标之一就是将来自各个背景的软件开发人员召集在一起，以便他们可以互相学习并提高专业软件开发的水准。匠艺社区与技术无关，所有开发人员，无论其经验水平如何，均欢迎参加会议。该社区致力于培养下一代专业人员，举办各种活动，使加入我们行业的人们可以学习构建实用软件的基本实践。 匠艺对公司的影响软件匠艺的采用正在增长。许多采用敏捷的公司现在都在寻求匠艺来提高其工程能力。但是，软件匠艺具有与敏捷不同的业务吸引力。 XP 仍然是许多经理不了解或不感到兴奋的东西。经理了解 Scrum，迭代，演示，回顾，协作和快速反馈循环。但是他们对与编程相关的技术并不那么感兴趣。对于大多数人而言，XP 与编程有关，而不与敏捷软件开发有关。 软件匠艺思想是许多开发人员的灵感。它给他们一种目的感，一种自豪感以及一种天生善于做事的意愿。一般而言，大多数开发人员都热衷于学习和做好事情，他们只需要支持和可以蓬勃发展的环境。拥护软件匠艺的公司通常会看到内部实践社区蓬勃发展。开发人员组织内部会议，他们在一起编码，练习 TDD 并提高他们的软件设计技能。他们对学习新技术和使他们工作的系统现代化感兴趣。他们讨论了改进代码库和消除技术借方的更好方法。软件匠艺促进了一种学习文化，使公司更具创新性和响应能力。 匠艺与敏捷创建软件匠艺运动的一些触发因素与许多开发人员对敏捷开发的挫败感有关。因此，有些人认为软件匠艺和敏捷相互矛盾。参加过敏捷运动的软件匠艺运动人士批评敏捷过分关注过程，而缺乏对工程的关注。敏捷运动中的人们批评软件匠艺的关注点太窄或缺乏对实际业务和人员问题的关注。 尽管双方都有一些合理的担忧，但大多数分歧更多是与部落主义有关，而不是实际的根本分歧。本质上，两个运动都希望实现非常相似的目标。他们俩都希望客户满意，他们都希望紧密合作，并且都重视短暂的反馈循环。两者都希望提供高质量，有价值的工作，并且都希望专业。为了实现业务敏捷性，公司不仅需要协作和迭代的过程，还需要良好的工程技能。将敏捷与软件匠艺相结合是实现这一目标的完美方法。 总结在 2001 年的 Snowbird 会议上，Kent Beck 说，敏捷是关于治愈开发与业务之间的鸿沟。 不幸的是，当项目经理涌入敏捷社区时，最初创建敏捷社区的开发人员被剥夺了价值，并低估了他们。 因此，他们离开去参加软件匠艺运动。 因此，古老的不信任仍在继续。但是，敏捷的价值和软件匠艺的价值是高度一致的。 这两个动作不应分开。 希望有一天，他们能再聚在一起。 总结敏捷可能是我们所见过的所有关于软件过程和方法的革命中最重要、最持久的。这种重要性和坚持不懈的精神证明，2001 年 2 月，那 17 个人去犹他州的 Snowbird，开始了一场从一个很长的山上滚下来的雪球运动。骑着雪球，看着雪球越滚越大、越滚越快，看着雪球打在石头和树上，对我来说真是一件乐事。 我写这本书是因为我认为是时候有人站出来大声疾呼敏捷是什么，敏捷应该是什么。我想是时候记住这些基础知识了。 这些基本的东西过去是，现在是，将来也会是 Ron Jeffries 的 Circle of Life。这些基础就是 Kent Beck 所阐述的极限编程的价值观、原则和纪律。这些基础是 Martin Fowler 重构的动机、技术和纪律。这些基础是 Booch、DeMarco、Yourdon、Constantine、Page-Jones 和 Lister 提出的。 这些基本原则是古老的、经过考验的、正确的。不管在边缘添加了多少新的绒毛，这些基础仍然存在，仍然相关，仍然是敏捷软件开发的核心。]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>agile</tag>
        <tag>TDD</tag>
        <tag>XP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个非前端开发者的CSS布局学习笔记]]></title>
    <url>%2Fposts%2F54%2F</url>
    <content type="text"><![CDATA[在 Web 前端开发中，CSS 布局是非常重要的知识技能。本篇文章主要是记录我在学习 CSS 布局知识时的学习笔记，阅读之前需要你了解部分的 CSS 和 HTML 知识。 几个重要的 CSS 布局相关属性display 属性display是 CSS 中非常重要的用来控制布局的属性， HTML 中每个元素都有一个默认的display属性，大多数元素该属性的默认值为block或inline，block元素被称为块级元素，inline元素通常被称为行内元素。display属性值通常有如下几种取值： block：块级元素。它会新开始一行，HTML 中大多数元素都默认为该值，如：div、p、form等元素。 inline：行内元素。它不会新开始一行，行内元素可以在段落中而不打乱段落布局。span、a等元素默认为该值。 inline-block：它是 block 和 inline 的结合体，主要用来把块级元素变换为行内元素，方便布局。 flex：一种新的布局方式，可以让布局更简单。 none：设置为该值的元素会被隐藏，布局时此类型元素可以不用考虑。 margin 属性margin属性用于指定本元素距离周围元素的距离，在布局时，会经常用到。 max-width 属性max-width用于指定当前元素最大宽度，当父容器宽度变小时，元素宽度会减小，但是当父容器宽度变大时，该元素最大宽度为指定宽度。 盒子模型CSS 元素由内容、内边距、边框、外边距组成，我们平时设置元素的 width 和 height 等相关属性时，默认情况下，设置的都只是内容的大小，内边距，边框、外边距都可能会增大元素的实际大小。具体内容可能参考这篇文章：CSS 基础框盒模型介绍。 如下所示，两个div元素都设置了同样的宽度，但是设置了外边距和边框的div元素明显实际宽度更大一些。 代码示例： 1234567891011121314151617181920&lt;style&gt; #box-model .simple &#123; width: 500px; margin: 20px auto; border: 2px solid green; &#125; #box-model .fancy &#123; width: 500px; margin: 20px auto; padding: 50px; border: 10px solid green; &#125;&lt;/style&gt;&lt;div class="simple"&gt; I'm use box-sizing, My width is 500px.&lt;/div&gt;&lt;div class="fancy"&gt; I'm use box-sizing, My width is 500px.&lt;/div&gt; box-sizing 属性当把一个元素的 box-sizing 属性设置为 border-box 时，内边距和边框不会再增加元素的实际大小。 如下所示，两个div元素都设置了同样的宽度，但是设置了内边距和边框的div元素实际宽度没有变大。 代码示例如下： 12345678910111213141516171819202122&lt;style&gt; #box-sizing .simple &#123; box-sizing: border-box; width: 500px; margin: 20px auto; border: 2px solid green; &#125; #box-sizing .fancy &#123; box-sizing: border-box; width: 500px; margin: 20px auto; padding: 50px; border: 10px solid green; &#125;&lt;/style&gt;&lt;div class="simple"&gt;I'm use box-sizing, My width is 500px.&lt;/div&gt;&lt;div class="fancy"&gt;I'm use box-sizing, My width is 500px.&lt;/div&gt; 使用 position 布局position 属性position 属性是 CSS 布局中常用的属性，可以有如下的取值： static：默认值，当一个元素被设置为 static 时，表示元素不能被 positioned（主要为absolute服务）。 relative：与 static 基本一致，区别在于元素可以被 positioned 。 fixed：固定定位，元素会相对于整个视窗（可以理解为浏览器窗口）定位，即使页面发生滚动，它还是会停留在相同的位置。 absolute：绝对定位，与 fixed 相似，但它的定位是相对于最近的可以被 positioned 的祖先元素。 position 布局示例使用 position 完成如下所示的布局： 代码示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;style&gt; #position-layout &#123; width: 90%; margin: 0 auto; position: relative; border: 2px solid green; &#125; #position-layout .nav &#123; position: absolute; top: 0px; left: 0px; width: 200px; padding-left: 20px; border: 2px solid red; box-sizing: border-box; &#125; #position-layout section &#123; margin-left: 200px; border: 2px solid orange; box-sizing: border-box; &#125;&lt;/style&gt;&lt;div id="position-layout"&gt; &lt;div class="nav"&gt; &lt;li&gt;&lt;a href="#"&gt;menuItem1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;menuItem2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;menuItem3&lt;/a&gt;&lt;/li&gt; &lt;/div&gt; &lt;section&gt; This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section. &lt;/section&gt; &lt;section&gt; This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too. &lt;/section&gt;&lt;/div&gt; 使用 float 布局float 属性为了防止影响后续元素，在使用 float 时注意使用 clear 清除浮动。 float 可以用于实现如下所示的文字环绕图片效果： 代码示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;style&gt; #float &#123; overflow: auto; border: 2px solid orange; &#125; #float img &#123; width: 200px; float: left; margin: 0 1em 1em 0; &#125; #float:after &#123; content: '.'; display: block; height: 0; clear: both; visibility: hidden; &#125;&lt;/style&gt;&lt;div id="float"&gt; &lt;img src="https://cn.bing.com/th?id=OHR.RhinosOxpecker_ZH-CN6392794613_1920x1080.jpg" /&gt; &lt;section&gt; 犀牛（学名：Dicerorhinus）是哺乳类犀科的总称，有4属5种。是世界上最大的奇蹄目动物， 犀类动物腿短、体肥笨拙，体长2.2-4.5米，肩高1.2-2米，体重2000-5000千克。前后肢均三趾； 皮厚粗糙，并于肩腰等处成褶皱排列，毛被稀少而硬，甚或大部无毛；耳呈卵圆形，头大而长，颈短粗，长唇延长伸出； 头部有实心的独角或双角（有的雌性无角），起源于真皮，角脱落仍能复生；无犬齿；尾细短，身体呈黄褐、褐、黑或灰色。 栖息于低地或海拔2000多米的高地。夜间活动，独居或结成小群。生活区域从不脱离水源。食性因种类而异， 以草类为主，或以树叶、嫩枝、野果、地衣等为食物。母兽妊娠期18-19个月。寿命30-50年。 因犀牛角的装饰和药用价值而被大量捕捉，除白犀外均为濒危物种。分布于亚洲南部、东南亚和非洲撒哈拉以南地区 9月22日是“世界犀牛日”（World Rhino Day）。2010年，“世界犀牛日”由南非世界自然基金会创办（WWF-South Africa），现已为全世界广泛接受。该活动旨在关注全球稀有动物。 在渐新世出现了有史以来最大陆生哺乳动物——巨犀，它体格健壮和高大，体长约8米，身高5米。不过虽然巨犀和犀牛同属奇蹄目，但并不属于犀牛科。 中新世的后期，出现了独角犀牛的祖先。独角犀牛仅存爪哇犀牛和印度犀牛，均分布在亚洲。在中新世以后出现的犀牛体型与现代犀牛相接近。 其中有下唇比上唇略大些的大唇犀，下颌有两颗大牙向前伸出，生活在沼泽地带，以水中的植物为食。 上新世后期（约300万年前），双角犀牛出现。双角犀牛有苏门达腊犀、白犀牛和黑犀牛。第四纪时期人类已经出现，早期的犀牛以板齿犀、披毛犀为代表。 板齿犀个体巨大，5米长，身披厚甲，在额部生有大角，约2米长，牙齿的齿冠高，呈方柱状，草地上生活， 更新世时期在中国华北的及欧洲等地曾有板齿犀生活；披毛犀和猛犸象外形相似，巨大的身体及长着粗毛的厚皮可以抵御寒冷，长鼻上有一对巨角， 前面一支最长可达1米，生活在寒冷地带。这两种犀类先后在不同的时期都已经灭绝了。 在犀类的后代中，现仅残存有犀牛科的4属5种，主要分布在亚洲和非洲，其中分布在亚洲的犀牛已经濒临绝种。 主要是因为犀牛角作为药材，其实犀牛角跟指甲是一样的构造，随数量减少现在也不容易买到真正的犀牛角， 市场还得以购买是因为现在商贩懂得以牛角替代来获利，甚至用相似成分的猫狗爪磨成假货变换充数。 &lt;/section&gt;&lt;/div&gt; float 布局示例使用 float 完成如下所示的布局： 代码示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;style&gt; #float-layout &#123; width: 90%; margin: 0 auto; border: 2px solid green; &#125; #float-layout .nav &#123; float: left; width: 200px; padding-left: 20px; border: 2px solid red; box-sizing: border-box; &#125; #float-layout section &#123; margin-left: 200px; border: 2px solid orange; box-sizing: border-box; &#125; #float-layout:after &#123; content: '.'; display: block; height: 0; clear: both; visibility: hidden; &#125;&lt;/style&gt;&lt;div id="float-layout"&gt; &lt;div class="nav"&gt; &lt;li&gt;&lt;a href="#"&gt;menuItem1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;menuItem2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;menuItem3&lt;/a&gt;&lt;/li&gt; &lt;/div&gt; &lt;section&gt; This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section. &lt;/section&gt; &lt;section&gt; This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too. &lt;/section&gt;&lt;/div&gt; 使用 inline-block 布局使用 inline-block 的注意事项 vertical-align 属性会影响到 inline-block 元素，可能需要把它的值设置为 top 。 需要设置每一列的宽度 如果源代码中 inline-block 元素之间有空格或者换行，那么列之间会产生空隙。如果同行的元素使用了百分比宽度且加起来和是100%宽度，但是由于代码的换行，导致列之间有空隙，会出现元素无法排列在同一行的现象。 inline-block 布局示例使用 inline-block 完成如下所示的布局： 代码示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;style&gt; #inline-block-layout &#123; width: 90%; margin: 0 auto; border: 2px solid green; font-size: 0; &#125; #inline-block-layout .nav &#123; display: inline-block; vertical-align: top; width: 25%; padding-left: 20px; border: 2px solid red; box-sizing: border-box; font-size: 16px; &#125; #inline-block-layout .column &#123; display: inline-block; vertical-align: top; width: 75%; border: 2px solid red; box-sizing: border-box; font-size: 16px; &#125; #inline-block-layout .column section &#123; border: 2px solid orange; box-sizing: border-box; &#125;&lt;/style&gt;&lt;div id="inline-block-layout"&gt; &lt;div class="nav"&gt; &lt;li&gt;&lt;a href="#"&gt;menuItem1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;menuItem2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;menuItem3&lt;/a&gt;&lt;/li&gt; &lt;/div&gt; &lt;div class="column"&gt; &lt;section&gt; This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section. &lt;/section&gt; &lt;section&gt; This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too. &lt;/section&gt; &lt;/div&gt;&lt;/div&gt; 使用 flexbox 布局flextbox 是一种新的布局方式，可能会有一些旧的浏览器无法支持 flexbox 布局。使用flexbox布局可以轻松实现复杂的布局，可以非常容易的实现垂直居中和水平居中，使用起来极为方便。 flexbox 简单布局示例使用 flexbox 完成如下所示的布局： 代码示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;style&gt; #flexbox-layout &#123; display: flex; width: 90%; margin: 0 auto; border: 2px solid green; &#125; #flexbox-layout .nav &#123; width: 200px; padding-left: 20px; border: 2px solid red; box-sizing: border-box; &#125; #flexbox-layout .column &#123; flex: 1; border: 2px solid red; box-sizing: border-box; &#125; #flexbox-layout .column section &#123; border: 2px solid orange; box-sizing: border-box; &#125;&lt;/style&gt;&lt;div id="flexbox-layout"&gt; &lt;div class="nav"&gt; &lt;li&gt;&lt;a href="#"&gt;menuItem1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;menuItem2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;menuItem3&lt;/a&gt;&lt;/li&gt; &lt;/div&gt; &lt;div class="column"&gt; &lt;section&gt; This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section. &lt;/section&gt; &lt;section&gt; This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too. &lt;/section&gt; &lt;/div&gt;&lt;/div&gt; flexbox 复杂布局示例实现如下所示四列布局方式，左边两列固定宽度，右边两列分别占剩下的空间1/3和2/3。 示例代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;style&gt; #flexbox-complex-layout &#123; display: flex; width: 90%; margin: 0 auto; border: 2px solid green; &#125; #flexbox-complex-layout .nav &#123; width: 200px; min-width: 100px; border: 2px solid red; box-sizing: border-box; &#125; #flexbox-complex-layout .carousel &#123; flex: none; width: 200px; border: 2px solid red; box-sizing: border-box; &#125; #flexbox-complex-layout .column1 &#123; flex: 1; border: 2px solid red; box-sizing: border-box; &#125; #flexbox-complex-layout .column2 &#123; flex: 2; border: 2px solid red; box-sizing: border-box; &#125;&lt;/style&gt;&lt;div id="flexbox-complex-layout"&gt; &lt;div class="nav"&gt; I have 200px width when the space is enough, otherwise my width is 100px. &lt;/div&gt; &lt;div class="carousel"&gt; I have 200px width whenever the space is enough or not. &lt;/div&gt; &lt;div class="column1"&gt; I have one third width of the left space. &lt;/div&gt; &lt;div class="column2"&gt; I have two third width of the left space. &lt;/div&gt;&lt;/div&gt; flexbox 水平与垂直居中示例实现如下所示的水平与垂直方向上的居中。 示例代码： 12345678910111213141516171819202122&lt;style&gt; #flexbox-easy-center &#123; display: flex; width: 90%; height: 100px; margin: 0 auto; align-items: center; justify-content: center; border: 2px solid green; box-sizing: border-box; &#125; #flexbox-easy-center section &#123; border: 2px solid green; box-sizing: border-box; &#125;&lt;/style&gt;&lt;div id="flexbox-easy-center"&gt; &lt;section&gt; I'm in the middle of the container. &lt;/section&gt;&lt;/div&gt; 媒体查询与响应式布局随着不同大小的视窗动态调整布局，这就被称为响应式布局，响应式布局可以通过媒体查询来实现。 如下图表示，当视窗的宽度大于600px时，采用两栏式布局，当宽度小于600px时，采用单栏式布局，菜单也切换为横向菜单。 示例代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;style&gt; #media-query &#123; width: 90%; margin: 0 auto; position: relative; border: 2px solid green; &#125; #media-query .nav &#123; padding-left: 20px; border: 2px solid red; box-sizing: border-box; &#125; #media-query section &#123; border: 2px solid orange; box-sizing: border-box; &#125; @media screen and (min-width: 600px) &#123; #media-query .nav &#123; position: absolute; top: 0px; left: 0px; width: 25%; &#125; #media-query section &#123; margin-left: 25%; &#125; &#125; @media screen and (max-width: 599px) &#123; #media-query .nav li &#123; display: inline; &#125; &#125;&lt;/style&gt;&lt;div id="media-query"&gt; &lt;div class="nav"&gt; &lt;li&gt;&lt;a href="#"&gt;menuItem1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;menuItem2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;menuItem3&lt;/a&gt;&lt;/li&gt; &lt;/div&gt; &lt;section&gt; This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section.This is a long section. &lt;/section&gt; &lt;section&gt; This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too.This is a long section too. &lt;/section&gt;&lt;/div&gt; 元素居中元素居中是常用的布局技巧，包括水平居中和垂直居中。下文的居中示例代码使用的通用CSS代码如下所示： 1234567891011121314151617181920212223242526272829&lt;style&gt; h4 &#123; text-align: center; &#125; #align-center &gt; div &#123; margin-top: 50px; width: 100%; height: 100px; box-sizing: border-box; &#125; #align-center .child &#123; height: 30px; border: 2px solid black; &#125; #vertical-center &gt; div &#123; margin-top: 50px; width: 100%; height: 100px; box-sizing: border-box; &#125; #vertical-center .child &#123; height: 30px; border: 2px solid black; &#125;&lt;/style&gt; 水平居中的常用方法水平居中包括以下几种常见实现方式。水平居中效果如下图所示： 子元素为inline父元素设置text-align: center，代码示例： 123456789101112131415&lt;style&gt; #align-center &gt; .inline &#123; text-align: center; border: 2px solid green; &#125; #align-center &gt; .inline &gt; .child &#123; display: inline; &#125;&lt;/style&gt;&lt;div id="align-center"&gt; &lt;div class="inline"&gt; &lt;div class="child"&gt;I am inline&lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 子元素为block并且宽度固定子元素设置margin: 0 auto，代码示例： 123456789101112131415161718&lt;style&gt; #align-center &gt; .block-width &#123; border: 2px solid blue; &#125; #align-center &gt; .block-width &gt; .child &#123; display: block; width: 200px; margin: 0 auto; &#125;&lt;/style&gt;&lt;div id="align-center"&gt; &lt;div class="block-width"&gt; &lt;div class="child"&gt; I am block with width &lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 子元素为block并且宽度不固定设置子元素display: inline，设置父元素text-align: center，代码示例： 1234567891011121314151617&lt;style&gt; #align-center &gt; .block-no-width &#123; border: 2px solid red; text-align: center; &#125; #align-center &gt; .block-no-width &gt; .child &#123; display: inline; &#125;&lt;/style&gt;&lt;div id="align-center"&gt; &lt;div class="block-no-width"&gt; &lt;div class="child"&gt; I am block without width &lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 使用transform父元素设置position: relative，子元素使用绝对定位与transform配合实现，代码示例： 12345678910111213141516171819&lt;style&gt; #align-center &gt; .transform &#123; position: relative; border: 2px solid orange; &#125; #align-center &gt; .transform &gt; .child &#123; position: absolute; transform: translate(-50%, 0); left: 50%; &#125;&lt;/style&gt;&lt;div id="align-center"&gt; &lt;div class="transform"&gt; &lt;div class="child"&gt; I am transform &lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 使用flex父元素设置flex布局与justify-content: center，代码示例： 123456789101112&lt;style&gt; #align-center &gt; .flex &#123; display: flex; justify-content: center; border: 2px solid purple; &#125;&lt;/style&gt;&lt;div id="align-center"&gt; &lt;div class="flex"&gt; &lt;div class="child"&gt;I am flex&lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 垂直居中的常用方法垂直居中包括以下几种常见实现方式。水平居中效果如下图所示： 子元素为block父元素设置position: relative，子元素使用绝对定位与margin配合实现，代码示例： 12345678910111213141516171819&lt;style&gt; #vertical-center &gt; .block &#123; position: relative; border: 2px solid green; &#125; #vertical-center &gt; .block &gt; .child &#123; display: block; position: absolute; margin: auto; top: 0; bottom: 0; &#125;&lt;/style&gt;&lt;div id="vertical-center"&gt; &lt;div class="block"&gt; &lt;div class="child"&gt;I am block&lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 子元素为inline父元素设置line-height与height值相等，代码示例： 123456789101112131415&lt;style&gt; #vertical-center &gt; .inline &#123; border: 2px solid blue; line-height: 100px; &#125; #vertical-center &gt; .inline &gt; .child &#123; display: inline; &#125;&lt;/style&gt;&lt;div id="vertical-center"&gt; &lt;div class="inline"&gt; &lt;div class="child"&gt;I am inline&lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 子元素为inline-block父元素添加after伪元素，子元素设置vertical-align: middle，代码示例： 12345678910111213141516171819202122&lt;style&gt; #vertical-center &gt; .inline-block &#123; border: 2px solid red; &#125; #vertical-center &gt; .inline-block:after &#123; content: ''; height: 100%; display: inline-block; vertical-align: middle; &#125; #vertical-center &gt; .inline-block &gt; .child &#123; display: inline-block; vertical-align: middle; &#125;&lt;/style&gt;&lt;div id="vertical-center"&gt; &lt;div class="inline-block"&gt; &lt;div class="child"&gt;I am inline-block&lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 使用transform父元素设置position: relative，子元素使用绝对定义配合transform实现，代码示例： 1234567891011121314151617&lt;style&gt; #vertical-center &gt; .transform &#123; position: relative; border: 2px solid orange; &#125; #vertical-center &gt; .transform &gt; .child &#123; position: absolute; transform: translate(0, -50%); top: 50%; &#125;&lt;/style&gt;&lt;div id="vertical-center"&gt; &lt;div class="transform"&gt; &lt;div class="child"&gt;I am transform&lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 使用flex父元素设置flex布局与align-items: center，代码示例： 123456789101112&lt;style&gt; #vertical-center &gt; .flex &#123; display: flex; align-items: center; border: 2px solid purple; &#125;&lt;/style&gt;&lt;div id="vertical-center"&gt; &lt;div class="flex"&gt; &lt;div class="child"&gt;I am flex&lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 参考文档 https://zh.learnlayout.com/ https://louiszhai.github.io/2016/03/12/css-center/]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>css</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你听说过测试驱动开发吗？]]></title>
    <url>%2Fposts%2F53%2F</url>
    <content type="text"><![CDATA[TDD 是什么根据维基百科的定义，测试驱动开发（Test-driven development 简称为 TDD）是一种软件开发过程，这种软件开发过程依赖于对一个非常短的开发循环周期的重复执行。先把需求转换成非常具体的测试用例，然后对软件进行编码让测试用例通过，最后对软件进行改进重构，消除代码的重复，保持代码整洁。没有测试验证的功能，不为会为其编写代码。 TDD 是由多个非常短的开发循环周期组成，一个 TDD 开发循环包括如下的3个步骤： 编写测试，让测试运行失败，此时代码处于红色状态。 编写生产代码，让测试通过，此时代码处于绿色状态。 重构代码，消除重复，此时代码处于重构状态。 这3个步骤就是人们常说的红、绿、重构循环，这就是一个完整的 TDD 开发循环周期。 TDD 起源于极限编程中的测试先行编程原则，最早由 Kent Beck 提出。TDD 是一种编程技巧，TDD 的主要目标是让代码整洁简单无 Bug。世界著名的软件大师 Kent Beck、Martin Fowler、Robert C. Martin 均表示支持 TDD 开发模式，他们甚至和 David Heinemeier Hansson 就 TDD 本身以及 TDD 对软件设计开发的影响有过深入的讨论：Is TDD Dead? TDD 的优点TDD 的优点有很多，下面列出几点我认为比较重要的优点： 开发人员更了解业务，更懂得任务分解：由于测试用例需要从用户或者使用者的角度来进行描述，这就要求开发人员能更加充分的了解业务，只有更充分的了解业务，才能写好测试用例，而且由于测试应该尽量小，这也就会促使我们把开发任务分解的更小，只有把任务分解的更小，我们才能达到 TDD 理想的小步快跑的状态。 代码测试覆盖率高，bug 少：由于先写测试，然后才能写生产代码，只有所有测试通过开发人员才能提交代码，这就会使得代码的测试覆盖率非常高，代码测试覆盖率高能表明我们的代码是经过充分测试的，这样生产中会碰到的 bug 就会相对少许多。 更自信的重构：由于代码的测试覆盖率高，每个功能都有对应的测试代码，开发人员可以更大胆进行重构，因为有充分的测试代码，当我们重构时，如果破坏了原有的功能，测试就会马上失败，这可以让开发人员在开发阶段就能发现问题，问题越早发现，修复的成本就越低。开发人员不会因为修改代码导致其他功能的损坏却不能及时发现，引发生产 bug 而变得畏手畏脚，开发人员重构代码也会变得非常自信。 代码整洁易扩展：由于 TDD 开发循环中，我们在不断重构代码，消除代码的坏味道，这会让我们得到更加整洁的代码，为了让软件更加容易测试，这会让我们更深入地思考评估我们的软件架构，从而改善优化我们的软件架构，让软件更加的灵活易扩展。 不会出现生产无用的代码：由于我们先把需求转换成测试用例，并且我们只为通过测试来编写最少的代码，这样我们几乎不会编写出生产无用的代码，我们所有的代码都是为相应的需求来服务的。 TDD 开发循环一个完整的 TDD 开发循环如下图所示： 编写测试，测试应该尽量小。运行测试，测试会失败，编译不通过也是一种失败，如果测试没有失败，这表明这个测试没有任何意义，因为这个测试既没有帮助我们实现需求，也没有帮助我们修复 bug 完善代码。这可能是如下的原因导致的： 我们在上一次 TDD 循环中，生产代码编写的太多，已经把这次的测试需要测试的功能实现了。 我们在之前的测试中忽略了这一次测试中应该测试的部分。 编写最少的代码让测试通过。为了尽量脱离测试无法通过的状态中，此步骤中可以使用特殊的方法，比如使用伪实现直接返回常量结果值，然后在重构阶段逐渐替换常量为真正的实现。 重构代码，减少代码中的重复代码，清除代码中的坏味道。清除生产代码与测试间的重复设计。这一步骤非常的重要，没有这一步骤的 TDD 开发是没有灵魂的 TDD 开发模式，并且可能导致你得到一个比不使用 TDD 开发模式开发出来的还要糟糕的软件。 重复上述步骤。 TDD 开发原则TDD 三定律 在编写不能通过的单元测试前，不可编写生产代码。这是 TDD 开发最重要的原则，是 TDD 得以实行的重要指导原则，这条原则包含两层含义： 测试先行，在编写生产代码之前要先编写测试代码。 只有在编写的测试失败的情况下，才能进行生产代码的编写。 只可编写刚好无法通过的单元测试，不能编译也算是不通过。这条原则指导我们在编写测试时，也应该把测试尽量的拆分的小一些，不要指望一个测试就能完整的测试一整个功能。 只可编写刚好足以通过当前失败测试的生产代码。这条原则告诉我们要编写尽量少的生产代码，尽快脱离测试失败的状态，这里的尽量少的代码并不是表示让你使用语法糖来达到使用少的代码行数，处理更多的事情的目标，这里尽量少的代码的意思是，只需要编写能通过测试的代码即可，不需要处理所有情况，比如异常情况等。这可以通过后面的测试来驱动我们来写这些处理异常情况的代码。 TDD 开发策略 伪实现，直接返回常量，并在重构阶段使用变量逐渐替换常量。 明显实现，由于代码逻辑简单，可以直接写出代码实现。 三角法，通过添加测试使用其失败，逐渐驱动我们朝目标前进。 根据错误的情况，伪实现和明显实现可以交替进行，当开发进行顺畅时，可以使用明显实现，当开发过程中经常碰到错误时，可以使用伪实现，慢慢找回自信，然后再使用明显实现进行开发。当完全没有实现思路或者实现思路不清晰时， 可以使用三角法来驱动我们开发，逐渐理清思路。 TDD 的难点 任务分解到底需要多细？我们需要把功能分解成多小的任务才合适呢？然后把测试分解多小才合适呢？这是一个比较难的问题，没有人能确切给出答案，一切都需要你自己去体会，去练习，去不断的尝试，去学习，去积累经验。 到底要测试什么？如果我们测试写的不好，很容易造成测试代码需要跟着生产代码被频繁的修改，这样测试不仅没有给我们的代码带来好处，反而给我们的重构带来很多的额外的负担。关于要测试什么，有一句正确但却无法给你具体建议名言：“测试行为，不要测试实现”，这也是需要长时间的去学习，去练习，去体会的。简单来说你应该测试所有公开给别人使用的接口，类，函数等，而内部私有的你可以选择性的测试，具体的关于应该如何写测试，可以观看如下的关于如何测试的公开演讲视频： https://dave.cheney.net/2019/04/03/absolute-unit-test https://www.youtube.com/watch?v=EZ05e7EMOLM TDD 开发示例我们使用 Go 语言来开发一个简单的 http 服务来演示 TDD 开发模式。服务支持如下的两种功能： GET /users/{name} 会返回用户使用 POST 方法 调用 API 的次数。 POST /users/{name} 会记录用户的一次 API 调用，把之前的 API 调用次数加1。 TDD 示例代码仓库地址 https://github.com/mgxian/tdd-example 任务分解 实现 GET 请求 验证响应码 验证返回 API 调用次数 验证不存在的用户 实现 POST 请求 验证响应码 验证是否调用了记录函数 验证调用记录是否正确 集成测试 完善主程序 实现 GET 请求先写测试测试获取 will 的 API 调用次数，并验证响应码 123456789101112func TestGetUsers(t *testing.T) &#123; t.Run("return will's api call count", func(t *testing.T) &#123; request, _ := http.NewRequest(http.MethodGet, "/users/will", nil) response := httptest.NewRecorder() UserServer(response, request) got := response.Code want := http.StatusOK if got != want &#123; t.Errorf("got %d, want %d", got, want) &#125; &#125;)&#125; 运行测试你会得到如下所示的错误 1.\user_test.go:13:3: undefined: UserServer 编写最少的代码让测试能运行并检查失败的测试输出现在让我们添加对UserServer函数的定义 1func UserServer() &#123;&#125; 再次运行测试你会得到如下的错误 123.\user_test.go:13:13: too many arguments in call to UserServer have (*httptest.ResponseRecorder, *http.Request) want () 现在让我们给函数添加相应的参数 1func UserServer(w http.ResponseWriter, r *http.Request) &#123;&#125; 再次运行测试，测试通过了。 先写测试测试响应数据 123456789101112131415161718func TestGetUsers(t *testing.T) &#123; t.Run("return will's api call count", func(t *testing.T) &#123; request, _ := http.NewRequest(http.MethodGet, "/users/will", nil) response := httptest.NewRecorder() UserServer(response, request) got := response.Code want := http.StatusOK if got != want &#123; t.Errorf("got %d, want %d", got, want) &#125; gotCount := response.Body.String() wantCount := "6" if gotCount != wantCount &#123; t.Errorf("got % q, want % q", gotCount, wantCount) &#125; &#125;)&#125; 运行测试，你会得到如下的错误 1user_test.go:23: got &quot;&quot;, want &quot;6&quot; 编写足够的代码让测试通过123func UserServer(w http.ResponseWriter, r *http.Request) &#123; fmt.Fprint(w, "6")&#125; 现在测试通过，但是你肯定会想骂人了，你这是写的啥，直接给写死了返回值？说好的不要写死呢？先别着急，由于我们没有存储数据的地方，现在返回一个固定值让测试通过，也不能说不是一个好办法，后面我们会来解决这个问题的。 完成主程序的结构我们尽量早的把经过验证的生产代码，放到主程序中，这样我们可以尽快的得到一个可运行的软件，而且后续的程序结构的改动，可以及时发现。 12345678910111213package mainimport ( "log" "net/http")func main() &#123; handler := http.HandlerFunc(UserServer) if err := http.ListenAndServe(":5000", handler); err != nil &#123; log.Fatalf("could not listen on port 5000 %v", err) &#125;&#125; 先写测试现在让我们再尝试获取 mgxian 的 API 调用数据 12345678910111213141516t.Run("return mgxian's api call count", func(t *testing.T) &#123; request, _ := http.NewRequest(http.MethodGet, "/users/mgxian", nil) response := httptest.NewRecorder() UserServer(response, request) got := response.Code want := http.StatusOK if got != want &#123; t.Errorf("got %d, want %d", got, want) &#125; gotCount := response.Body.String() wantCount := "8" if gotCount != wantCount &#123; t.Errorf("got % q, want % q", gotCount, wantCount) &#125;&#125;) 现在运行测试，你会得到如下的错误 1user_test.go:40: got &quot;6&quot;, want &quot;8&quot; 编写足够的代码让测试通过现在让我们来修复这个错误，为了能让我们能根据 user 的不同来响应不同的内容，我们需要从 URL 中获取到 user ，测试驱动着我们完成接下来的工作。 123456789101112func UserServer(w http.ResponseWriter, r *http.Request) &#123; user := r.URL.Path[len("/users/"):] if user == "will" &#123; fmt.Fprint(w, "6") return &#125; if user == "mgxian" &#123; fmt.Fprint(w, "8") return &#125;&#125; 运行测试通过。 重构根据 user 来响应不同内容的逻辑我们可以放在一个单独的函数中去。 1234567891011121314151617func UserServer(w http.ResponseWriter, r *http.Request) &#123; user := r.URL.Path[len("/users/"):] apiCallCount := GetUserAPICallCount(user) fmt.Fprint(w, apiCallCount)&#125;func GetUserAPICallCount(user string) string &#123; if user == "will" &#123; return "6" &#125; if user == "mgxian" &#123; return "8" &#125; return ""&#125; 重构之后，运行测试，测试通过，我们观察到我们的测试程序有部分代码是重复的，我们也可以进行重构，不仅生产代码需要重构，测试代码也需要重构。 12345678910111213141516171819202122232425262728293031323334353637383940func TestGetUsers(t *testing.T) &#123; t.Run("return will's api call count", func(t *testing.T) &#123; user := "will" request := newGetUserAPICallCountRequest(user) response := httptest.NewRecorder() UserServer(response, request) assertStatus(t, response.Code, http.StatusOK) assertCount(t, response.Body.String(), "6") &#125;) t.Run("return mgxian's api call count", func(t *testing.T) &#123; user := "mgxian" request := newGetUserAPICallCountRequest(user) response := httptest.NewRecorder() UserServer(response, request) assertStatus(t, response.Code, http.StatusOK) assertCount(t, response.Body.String(), "8") &#125;)&#125;func newGetUserAPICallCountRequest(user string) *http.Request &#123; request, _ := http.NewRequest(http.MethodGet, fmt.Sprintf("/users/%s", user), nil) return request&#125;func assertStatus(t *testing.T, got, want int) &#123; t.Helper() if got != want &#123; t.Errorf("wrong status code got %d, want %d", got, want) &#125;&#125;func assertCount(t *testing.T, got, want string) &#123; t.Helper() if got != want &#123; t.Errorf("got % q, want % q", got, want) &#125;&#125; 运行测试，测试通过，测试代码重构完成。现在让我们进一步的思考，我们的 UserServer 相当于 MVC 模式中的 Controller ，GetUserAPICallCount 相当于 Model ，我们应该让它们之间通过 Interface UserStore 来交流，隔离关注点。为了能让 UserServer 使用 UserStore 我们应该把 UserServer 定义为 struct 类型。 12345678910111213type UserStore interface &#123; GetUserAPICallCount(user string) int&#125;type UserServer struct &#123; store UserStore&#125;func (u *UserServer) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; user := r.URL.Path[len("/users/"):] apiCallCount := u.store.GetUserAPICallCount(user) fmt.Fprint(w, apiCallCount)&#125; 运行测试你会得到如下的错误 1main.go:9:30: type UserServer is not an expression 修改 main 函数新创建的 UserServer 123456func main() &#123; server := &amp;UserServer&#123;&#125; if err := http.ListenAndServe(":5000", server); err != nil &#123; log.Fatalf("could not listen on port 5000 %v", err) &#125;&#125; 修改测试使用新创建的 UserServer 12345678910111213141516171819202122func TestGetUsers(t *testing.T) &#123; server := &amp;UserServer&#123;&#125; t.Run("return will's api call count", func(t *testing.T) &#123; user := "will" request := newGetUserAPICallCountRequest(user) response := httptest.NewRecorder() server.ServeHTTP(response, request) assertStatus(t, response.Code, http.StatusOK) assertCount(t, response.Body.String(), "6") &#125;) t.Run("return mgxian's api call count", func(t *testing.T) &#123; user := "mgxian" request := newGetUserAPICallCountRequest(user) response := httptest.NewRecorder() server.ServeHTTP(response, request) assertStatus(t, response.Code, http.StatusOK) assertCount(t, response.Body.String(), "8") &#125;)&#125; 再次运行测试你会得到如下的错误，这是由于我们并没有传递 UserStore 给 UserServer 。 123panic: runtime error: invalid memory address or nil pointer dereference [recovered] panic: runtime error: invalid memory address or nil pointer dereference[signal 0xc0000005 code=0x0 addr=0x18 pc=0x66575f] 编写一个 stub 类型的 mock 来模拟测试 1234567type StubUserStore struct &#123; apiCallCounts map[string]int&#125;func (s *StubUserStore) GetUserAPICallCount(user string) int &#123; return s.apiCallCounts[user]&#125; 修改测试使用我们 mock 出来的 StubUserStore 12345678910111213141516171819202122232425262728func TestGetUsers(t *testing.T) &#123; store := StubUserStore&#123; apiCallCounts: map[string]int&#123; "will": 6, "mgxian": 8, &#125;, &#125; server := &amp;UserServer&#123;&amp;store&#125; t.Run("return will's api call count", func(t *testing.T) &#123; user := "will" request := newGetUserAPICallCountRequest(user) response := httptest.NewRecorder() server.ServeHTTP(response, request) assertStatus(t, response.Code, http.StatusOK) assertCount(t, response.Body.String(), "6") &#125;) t.Run("return mgxian's api call count", func(t *testing.T) &#123; user := "mgxian" request := newGetUserAPICallCountRequest(user) response := httptest.NewRecorder() server.ServeHTTP(response, request) assertStatus(t, response.Code, http.StatusOK) assertCount(t, response.Body.String(), "8") &#125;)&#125; 再次运行测试，测试全部通过。 为了使我们的主程序能正常运行，我们需要实现一个假的 UserStore 123456789101112131415161718192021package mainimport ( "log" "net/http")type InMemoryUserStore struct&#123;&#125;func (i *InMemoryUserStore) GetUserAPICallCount(user string) int &#123; return 666&#125;func main() &#123; store := InMemoryUserStore&#123;&#125; server := &amp;UserServer&#123;&amp;store&#125; if err := http.ListenAndServe(":5000", server); err != nil &#123; log.Fatalf("could not listen on port 5000 %v", err) &#125;&#125; 先写测试测试一个不存在的用户 12345678t.Run("return 404 on unknown user", func(t *testing.T) &#123; user := "unknown" request := newGetUserAPICallCountRequest(user) response := httptest.NewRecorder() server.ServeHTTP(response, request) assertStatus(t, response.Code, http.StatusNotFound)&#125;) 运行测试得到如下的错误 1user_test.go:52: wrong status code got 200, want 404 编写足够的代码让测试通过12345678func (u *UserServer) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; user := r.URL.Path[len("/users/"):] apiCallCount := u.store.GetUserAPICallCount(user) if apiCallCount == 0 &#123; w.WriteHeader(http.StatusNotFound) &#125; fmt.Fprint(w, apiCallCount)&#125; 运行测试，测试通过。 实现 POST 请求先写测试测试记录 API 调用次数，验证响应码 1234567891011121314func TestStoreAPICalls(t *testing.T) &#123; store := StubUserStore&#123; map[string]int&#123;&#125;, &#125; server := &amp;UserServer&#123;&amp;store&#125; t.Run("return accepted on POST", func(t *testing.T) &#123; request, _ := http.NewRequest(http.MethodPost, "/users/will", nil) response := httptest.NewRecorder() server.ServeHTTP(response, request) assertStatus(t, response.Code, http.StatusAccepted) &#125;)&#125; 运行测试，你会得到如下的错误 1user_test.go:67: wrong status code got 404, want 202 编写足够的代码让测试通过12345678910111213func (u *UserServer) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; method := r.Method if method == http.MethodPost &#123; w.WriteHeader(http.StatusAccepted) return &#125; user := r.URL.Path[len("/users/"):] apiCallCount := u.store.GetUserAPICallCount(user) if apiCallCount == 0 &#123; w.WriteHeader(http.StatusNotFound) &#125; fmt.Fprint(w, apiCallCount)&#125; 运行测试通过。 重构把处理 post 和 get 请求的业务逻辑封装到单独的函数。 123456789101112131415161718192021func (u *UserServer) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; switch r.Method &#123; case http.MethodGet: u.showAPICallCount(w, r) case http.MethodPost: u.processAPICall(w, r) &#125;&#125;func (u *UserServer) showAPICallCount(w http.ResponseWriter, r *http.Request) &#123; user := r.URL.Path[len("/users/"):] apiCallCount := u.store.GetUserAPICallCount(user) if apiCallCount == 0 &#123; w.WriteHeader(http.StatusNotFound) &#125; fmt.Fprint(w, apiCallCount)&#125;func (u *UserServer) processAPICall(w http.ResponseWriter, r *http.Request) &#123; w.WriteHeader(http.StatusAccepted)&#125; 运行测试通过。 先写测试验证当使用 POST 方法时，UserStore 是否被调用记录 API 请求 给我们之前实现的 StubUserStore 添加 RecordAPICall 函数，记录并验证函数的调用。 123456789101112type StubUserStore struct &#123; apiCallCounts map[string]int apiCalls []string&#125;func (s *StubUserStore) GetUserAPICallCount(user string) int &#123; return s.apiCallCounts[user]&#125;func (s *StubUserStore) RecordAPICall(user string) &#123; s.apiCalls = append(s.apiCalls, user)&#125; 添加测试验证调用 123456789101112131415161718func TestStoreAPICalls(t *testing.T) &#123; store := StubUserStore&#123; map[string]int&#123;&#125;, &#125; server := &amp;UserServer&#123;&amp;store&#125; t.Run("record api call when POST", func(t *testing.T) &#123; request, _ := http.NewRequest(http.MethodPost, "/users/will", nil) response := httptest.NewRecorder() server.ServeHTTP(response, request) assertStatus(t, response.Code, http.StatusAccepted) if len(store.apiCalls) != 1 &#123; t.Errorf("got %d calls to RecordAPICall want %d", len(store.apiCalls), 1) &#125; &#125;)&#125; 运行测试，你会得到如下的错误 1user_test.go:63:17: too few values in StubUserStore literal 编写最少的代码让测试能运行并检查失败的测试输出12345678910111213141516171819func TestStoreAPICalls(t *testing.T) &#123; store := StubUserStore&#123; map[string]int&#123;&#125;, nil, &#125; server := &amp;UserServer&#123;&amp;store&#125; t.Run("record api call when POST", func(t *testing.T) &#123; request, _ := http.NewRequest(http.MethodPost, "/users/will", nil) response := httptest.NewRecorder() server.ServeHTTP(response, request) assertStatus(t, response.Code, http.StatusAccepted) if len(store.apiCalls) != 1 &#123; t.Errorf("got %d calls to RecordAPICall want %d", len(store.apiCalls), 1) &#125; &#125;)&#125; 运行测试，你会得到如下的错误 1user_test.go:76: got 0 calls to RecordAPICall want 1 编写足够的代码让测试通过给 UserStore 添加相应的函数 1234type UserStore interface &#123; GetUserAPICallCount(user string) int RecordAPICall(user string)&#125; 由于编译器报错，我需要 InMemoryUserStore 实现相应的函数 1func (i *InMemoryUserStore) RecordAPICall(user string) &#123;&#125; 编写代码调用 RecordAPICall 1234func (u *UserServer) processAPICall(w http.ResponseWriter, r *http.Request) &#123; u.store.RecordAPICall("bob") w.WriteHeader(http.StatusAccepted)&#125; 运行测试，测试通过。 先写测试验证 API 调用的用户记录 123456789101112131415161718192021222324func TestStoreAPICalls(t *testing.T) &#123; store := StubUserStore&#123; map[string]int&#123;&#125;, nil, &#125; server := &amp;UserServer&#123;&amp;store&#125; t.Run("record api call when POST", func(t *testing.T) &#123; user := "will" request, _ := http.NewRequest(http.MethodPost, fmt.Sprintf("/users/%s", user), nil) response := httptest.NewRecorder() server.ServeHTTP(response, request) assertStatus(t, response.Code, http.StatusAccepted) if len(store.apiCalls) != 1 &#123; t.Errorf("got %d calls to RecordAPICall want %d", len(store.apiCalls), 1) &#125; if store.apiCalls[0] != user &#123; t.Errorf("did not record correct api call user got %q want %q", store.apiCalls[0], user) &#125; &#125;)&#125; 运行测试，你会得到如下的错误 1user_test.go:81: did not record correct api call user got &quot;bob&quot; want &quot;will&quot; 编写足够的代码让测试通过12345func (u *UserServer) processAPICall(w http.ResponseWriter, r *http.Request) &#123; user := r.URL.Path[len("/users/"):] u.store.RecordAPICall(user) w.WriteHeader(http.StatusAccepted)&#125; 运行测试通过。 重构从请求中获取 user 的代码重复，提取到调用方，以参数形式传递。 12345678910111213141516171819202122func (u *UserServer) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; user := r.URL.Path[len("/users/"):] switch r.Method &#123; case http.MethodGet: u.showAPICallCount(w, user) case http.MethodPost: u.processAPICall(w, user) &#125;&#125;func (u *UserServer) showAPICallCount(w http.ResponseWriter, user string) &#123; apiCallCount := u.store.GetUserAPICallCount(user) if apiCallCount == 0 &#123; w.WriteHeader(http.StatusNotFound) &#125; fmt.Fprint(w, apiCallCount)&#125;func (u *UserServer) processAPICall(w http.ResponseWriter, user string) &#123; u.store.RecordAPICall(user) w.WriteHeader(http.StatusAccepted)&#125; 运行测试，测试通过，重构完成。 集成测试两个功能已经分别开发完成，我们现在进行集成测试，由于集成测试不容易写，出错后不易查找，并且由于可能会使用真实的组件如数据库，所以可能会运行缓慢。因此集成测试应该尽量少写。 先写测试1234567891011121314151617func TestRecordAPICallsAndGetThem(t *testing.T) &#123; store := InMemoryUserStore&#123;&#125; server := UserServer&#123;&amp;store&#125; user := "will" request, _ := http.NewRequest(http.MethodPost, fmt.Sprintf("/users/%s", user), nil) server.ServeHTTP(httptest.NewRecorder(), request) server.ServeHTTP(httptest.NewRecorder(), request) server.ServeHTTP(httptest.NewRecorder(), request) response := httptest.NewRecorder() request = newGetUserAPICallCountRequest(user) server.ServeHTTP(response, request) assertStatus(t, response.Code, http.StatusOK) assertCount(t, response.Body.String(), "3")&#125; 运行测试，你会得到如下 的错误 1server_integration_test.go:25: got &quot;666&quot;, want &quot;3&quot; 编写足够的代码让测试通过为 InMemoryUserStore 编写具体实现 1234567891011121314151617type InMemoryUserStore struct &#123; store map[string]int&#125;func (i *InMemoryUserStore) GetUserAPICallCount(user string) int &#123; return i.store[user]&#125;func (i *InMemoryUserStore) RecordAPICall(user string) &#123; i.store[user]++&#125;func NewInMemoryUserStore() *InMemoryUserStore &#123; return &amp;InMemoryUserStore&#123; store: make(map[string]int), &#125;&#125; 集成测试使用 InMemoryUserStore 1234567891011121314151617func TestRecordAPICallsAndGetThem(t *testing.T) &#123; store := NewInMemoryUserStore() server := UserServer&#123;store&#125; user := "will" request, _ := http.NewRequest(http.MethodPost, fmt.Sprintf("/users/%s", user), nil) server.ServeHTTP(httptest.NewRecorder(), request) server.ServeHTTP(httptest.NewRecorder(), request) server.ServeHTTP(httptest.NewRecorder(), request) response := httptest.NewRecorder() request = newGetUserAPICallCountRequest(user) server.ServeHTTP(response, request) assertStatus(t, response.Code, http.StatusOK) assertCount(t, response.Body.String(), "3")&#125; 再次运行测试，测试通过。 完善主程序修改主程序使用 NewInMemoryUserStore 函数。 12345678func main() &#123; store := NewInMemoryUserStore() server := &amp;UserServer&#123;store&#125; if err := http.ListenAndServe(":5000", server); err != nil &#123; log.Fatalf("could not listen on port 5000 %v", err) &#125;&#125; 到此一个使用内存来记录查询用户 API 调用次数的程序已经完成，后续步骤你可选择其他数据存储来替换内存存储进行数据的持久化。只需要实现 UserStore 接口即可。 TDD 总结当你学习了 TDD 之后，你就学会了这种小步快跑的开发方法，你可以把它应用在你没有太大自信的关键核心组件的开发中，TDD 能帮助你以小步快跑的方式向目标前进，TDD 只是给了你一种小步快跑的能力，你可以只在关键的时候才使用这种能力。学习 TDD 并不是为了让你在所有涉及到编码的地方全部使用 TDD 开发模式。 TDD 的关键在于驱动（driven），要让测试驱动我们来进行功能开发，每写一个测试，都驱动我们写更多的生产代码，都在向实现我们的功能的方向前进。 重构是 TDD 中重要的环节，如果没有重构，你得到的可能只是由一堆零乱代码组合的勉强凑合工作的软件。只有注重重构才能让我们的代码更整洁，更利于后续 TDD 开发模式的正常执行。 TDD 开发模式减轻人开发人员的心智负担，通过红、绿、重构循环，开发人员每一个阶段都只有一个特定的目标，这使得开发人员每个阶段的关注点只有一个，注意力集中。 TDD 开发模式能让开发人员更自信，由于我们的任务分解的小，开发循环比较短，我们可以在很短时间内获得测试的反馈，我们几乎随时都有可运行的软件，这给我们开发人员带来很强的安全感，这给了我们自信心。 TDD 不是银弹，不是所有项目开发都可以使用 TDD 开发模式来进行开发，在测试成本比较高的情况下就不太适合使用 TDD 开发模式，比如在前端（Web、iOS、Android）的项目开发中，检查页面中的元素的位置及大小等操作比较麻烦，就不太适合使用 TDD 开发模式，但是我们可以尽量减少 UI 部分的业务逻辑，UI 只根据其他模块处理后的数据来做简单直接的展示，把 TDD 应用在其他为 UI 提供数据的模块开发中。 TDD 并非要求我们非常严格的遵循 TDD 三定律，我们可以根据特殊情况，做适当的小调整，但是整体流程与节奏不能有偏离，TDD 三定律并不是为了给你加上了无法挣脱的枷锁，它只是给了我们一个整体指导原则。 要想流畅的使用 TDD 需要不断的练习，掌握 TDD 的节奏是流畅使用 TDD 关键。想要真正学会使用 TDD ，只能练习、练习、再练习。 后续学习 测试驱动开发 重构 架构整洁之道 参考文档 https://book.douban.com/subject/1230036/ https://quii.gitbook.io/learn-go-with-tests/build-an-application/http-server Https://mgxian.dev/posts/52/ https://juejin.im/post/5c3e73876fb9a049d37f5db1 https://www.guru99.com/test-driven-development.html https://en.wikipedia.org/wiki/Test-driven_development https://blog.testlodge.com/what-is-tdd/]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>test</tag>
        <tag>tdd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在测试中更好地使用mock]]></title>
    <url>%2Fposts%2F52%2F</url>
    <content type="text"><![CDATA[注意：本文大部分内容为翻译 Bob 大叔的文章，原文链接可以在文章底部的参考文档处找到。 什么是 mockmock 作为名词时表示 mock 对象，在维基百科的解释中如下： 在面向对象程序设计中，模拟对象（英语：mock object，也译作模仿对象）是以可控的方式模拟真实对象行为的假的对象。程序员通常创造模拟对象来测试其他对象的行为。 mock 作为动词时表示编写使用 mock 对象。 mock 多用于测试代码中，对于不容易构造或者不容易获取的对象，使用一个虚拟的对象来方便测试。 mock 的分类 为了使用示例说明各个mock 种类的区别与联系，文章使用 go 语言作为示例，如下为示例的基础代码： 12345678910111213141516171819202122232425type Authorizer interface &#123; authorize(username, password string) bool&#125;type System struct &#123; authorizer Authorizer&#125;func NewSystem(authorizer Authorizer) *System &#123; system = new(System) system.authorizer = authorizer return system&#125;func (s *System) loginCount() int &#123; // skip return 0&#125;func (s *System) login(username, password string) error &#123; if s.authorizer.authorize(username, password) &#123; return nil &#125; return errors.New("username or password is not right")&#125; dummy当你不关心传入的参数被如何使用时，你就应该使用 dummy 类型的 mock，一般用于作为其他对象的初始化参数。示例如下： 123456789101112131415type DummyAuthorizer struct &#123;&#125;func (d *DummyAuthorizer) authorize(username, password string) bool &#123; // return nil return false&#125;// Testfunc TestSystem(t *testing.T) &#123; system := NewSystem(new(DummyAuthorizer)) got := system.loginCount() want := 0 if got != want &#123; t.Errorf("got %d, want %d", got, want) &#125;&#125; 在上面的测试示例代码中，DummyAuthorizer 的作为只是为了初始化 System 对象的需要，后续测试中并没有使用该 DummyAuthorizer 对象。 注意：此处的 authorize 方法原文返回了 null ，由于 go 语言不允许为 bool 返回 nil ，因此此处返回了 false stub当你只关心方法的返回结果，并且需要特定返回值的时候，这时候你就可以使用 stub 类型的 mock 。比如我们需要测试系统中某些功能是否能正确处理用户登录和不登录的情况，而登录功能我们已经在其他地方经过测试，而且使用真实的登录功能调用又比较的麻烦，我们就可以直接返回已登录或者未登录状态来进行其他功能的验证。 1234567891011type AcceptingAuthorizerStub struct &#123;&#125;func (aas *AcceptingAuthorizerStub) authorize(username, password string) bool &#123; return true&#125;type RefusingAuthorizerStub struct &#123;&#125;func (ras *RefusingAuthorizerStub) authorize(username, password string) bool &#123; return false&#125; spy当你不只是只关心方法的返回结果，还需要检查方法是否真正的被调用了，方法的调用次数等，或者需要记录方法调用过程中的信息。这个时候你就应该使用 spy 类型的 mock ，调用结束后你需要自己检查方法是否被调用，检查调用过程中记录的其他信息。但是请注意，这将会使你的测试代码和被测试方法相耦合，测试需要知道被测试方法的内部实现细节。使用时需要谨慎一些，不要过渡使用，过渡使用可能导致测试过于脆弱。 12345678910111213141516171819202122type AcceptingAuthorizerSpy struct &#123; authorizeWasCalled bool&#125;func (aas *AcceptingAuthorizerSpy) authorize(username, password string) bool &#123; aas.authorizeWasCalled = true return true&#125;// Testfunc TestSystem(t *testing.T) &#123; authorizer := new(AcceptingAuthorizerSpy) system := NewSystem(authorizer) got := system.login("will", "will") if got != nil &#123; t.Errorf("login failed with error %v", got) &#125; if authorizer.authorizeWasCalled != true &#123; t.Errorf("authorize was not called") &#125;&#125; mockmock 类型的 mock 可以算作是真正的 ”mock“ 。把 spy 类型的 mock 在测试代码中的断言语句移动到 mock 对象中，这使它更关注于测试行为。这种类型的 mock 对方法的返回值并不是那么的感兴趣，它更关心的是哪个方法被使用了什么参数在什么时间被调用了，调用的频率等。这种类型的 mock 使得编写 mock 相关的工具更加的简单，mock 工具可以帮助你在运行时创建 mock 对象。 123456789101112type AcceptingAuthorizerVerificationMock struct &#123; authorizeWasCalled bool&#125;func (aavm *AcceptingAuthorizerVerificationMock) authorize(username, password string) bool &#123; aavm.authorizeWasCalled = true return true&#125;func (aavm *AcceptingAuthorizerVerificationMock) verify() bool &#123; return aavm.authorizeWasCalled&#125; fakefake 类型的 mock 与其他类型的 mock 最大的区别是它包含了真实的业务逻辑。当以不同的数据调用时，你会得到不同的结果。随着业务逻辑的改变，它可能也会越来越复杂，最终你也需要为这种类型的 mock 编写单元测试，甚至最后它可能成为了一个真实的业务系统。如果不是必须，请不要使用 fake 类型的 mock 。 12345678type AcceptingAuthorizerFake struct &#123;&#125;func (aas *AcceptingAuthorizerFake) authorize(username, password string) bool &#123; if username == "will" &#123; return true &#125; return false&#125; 总结mock 是 spy 的一种类型，spy 又是 stub 的一种类型，而 stub 又是 dummy 的一种类型，但是 fake 与其他所有 mock 类型不同，fake 包含了真实的业务逻辑，而其他类型的 mock 都不包含真实的业务逻辑。 根据 Bob 大叔的实践来看，他使用最多的是 spy 和 stub 类型的 mock ，并且他不会经常使用 mock 工具，很少使用 dummy 类型的 mock ，只有在使用 mock 工具时才会使用 mock 类型的 mock 。现在的编程 IDE 中，只需要你定义好接口，IDE 就可以帮你轻松的实现他们，你只需要简单的修改就可以实现 spy 和 stub 类型的 mock ，因此 Bob 大叔很少使用 mock 工具。 mock 的使用时机mock 对象是一个强大的工具，但是 mock 对象也有两面性，如果使用不正确也可能会带来强大的破坏力。 完全不使用 mock如果我们完全不使用 mock ，直接使用真实的对象进行测试，这会带来什么问题呢？ 测试将会运行缓慢。我们使用真实的数据库，真实的上游服务，由于这些都需要通过网络来进行通信，这会将比程序内部的函数调用慢上几个数量级。当我们修改一行简单的代码，进行测试时，可能需要等待数分钟，数小时，甚至可能要几天才能把测试运行结束。 代码的测试覆盖率可能会降低很多。一些错误和异常在没有使用 mock 的情况下可能根本无法进行测试，例如网络协议的异常。一些危险的测试用例，比如删除文件、删除数据库表很难进行安全的测试。 测试变得异常的脆弱。与测试无关的其他问题可能会导致测试失败，例如由于机器负载导致的网络时延问题，数据库表的结构不正确，配置文件被错误修改等问题。 在完全不使用 mock 对象的情况下，我们的测试会变得缓慢、不完整、脆弱。 过度使用 mock如果过度使用 mock 对象，所有的测试都使用 mock 对象，这会带来什么问题呢？ 测试将会运行缓慢。一些 mock 工具强依赖反射机制，因此会使得测试变慢。 mock 所有类之间的交互，会导致你必须创建返回其他 mock 类的 mock 类，你可能需要 mock 整个交互链路上所有的类，这将会导致你的测试异常的复杂，并且所有交互链路上的 mock 类可能都耦合在了一起，当其中一个修改时，可能会导致整个测试失败。 暴露本不需要暴露的接口。由于需要 mock 每一个类之间的交互，就需要为每一个类之间的交互创建接口，这将会导致你需要创建出许多只用于 mock 对象的接口，这是一种过度抽象和可怕的设计损坏。 过度使用 mock 对象，将会使用测试变得缓慢、脆弱、复杂，并且有可能损坏你的软件设计。 mock 的使用建议在架构的重要边界使用 mock ，不要在边界内部使用 mock 例如可以在数据库、web服务器等所有第三方服务的边界处使用 mock 。可以参考如下的整洁架构图： 可以在最外环的边界处使用 mock 隔离外部依赖，方便测试，这样做可以得到如下的好处： 测试运行速度快。 测试不会因为外部依赖的错误而失败。 更容易的模拟测试外部依赖的所有异常情况。 横跨边界的有限状态机的每条路径都可以被测试。 mock 不在需要相互耦合依赖，代码会更整洁。 另一个比较大的好处是它强迫你思考找出软件的重要边界，并且为它们定义接口，这使得你的软件不会强耦合依赖于边界外的组件。因此你可以独立开发部署边界两边的组件。像这样去分离架构关注点是一个很好的软件设计原则。 使用你自己的 mock mock 工具有它们自己的领域语言，在使用它们之前你必须先学习它。通过前面的 mock 类型介绍，我们已经知道用的最多的 mock 是 stub 和 spy 类型，而由于现在的 IDE 可以很方便的生成这些 mock 代码，我们只需要稍作修改就可以直接使用，所以综合来看，我们一般情况下是不需要使用 mock 工具的。 由于你自己写 mock 时不会使用反射，这将会让你的测试代码运行速度更快。如果你决定使用 mock 工具，请尽量少的使用它。 总结mock 对象既不能完全不使用，也不能过度使用。我们应该在软件的重要边界处使用 mock ，要尽量少的使用 mock 工具，使用 mock 工具时不要过度依赖它，我们应该尽量使用轻量级的 stub 和 spy 的 mock 类型，并且我们应该自己手写这些简单的 mock 类型。如果你这样做了，你会发现你的测试运行速度更快，更稳定，并且还会有更高的测试覆盖率，你的软件架构设计也会越来越好。 参考文档 https://blog.cleancoder.com/uncle-bob/2014/05/14/TheLittleMocker.html https://blog.cleancoder.com/uncle-bob/2014/05/10/WhenToMock.html]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>mock</tag>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何简单的给静态博客网站用上HTTPS]]></title>
    <url>%2Fposts%2F51%2F</url>
    <content type="text"><![CDATA[简介谷歌浏览器已经把没有使用HTTPS协议的网站全部标记为不安全，为了让浏览器不把我们的网站标记为不安全，为了让访问者能更安心的浏览我们的网站，我们只能为我们的网站开启HTTPS协议支持，由于 Let’s Encrypt 可以为我们提供免费证书使用，我们就可以免费的为我们的网站申请证书，但是由于证书有效期只有3个月，当证书快过期时，我们需要重新续签。为此有不少工具能帮助我们完成自助的申请证书以及续签。但是这仍然需要不少的配置。这时一个名为 Caddy 的跨开台开源软件出现了，Caddy 能自动帮我们申请证书，当证书快过期时自动续签，只需一次配置，后续基本不需要再做其他配置，非常的方便，完全可以代替 Nginx 与 Apache 。 安装去 Caddy 的 Github 发布页下载对应平台的二进制压缩包 https://github.com/caddyserver/caddy/releases 解压安装 123tar xf caddy_v1.0.3_linux_amd64.tar.gzmv caddy /usr/local/bincaddy -version 配置 Caddy 把如下的配置文件写入名为 Caddyfile 的文件中。 12345678910mgxian.dev &#123; root /data/blog/mgxian gzip log ./access.log&#125;www.mgxian.dev &#123; redir https://mgxian.dev&#123;uri&#125; log ./access.log&#125; 1-5 为 mgxian.dev 域名的相关配置，指定根目录为 /data/blog/mgxian ，开启 gzip 压缩，并把访问日志记录在当前目录的 access.log 文件中。 7-10 为 www.mgxian.dev 域名的相关配置，表示当访问 www.mgxian.dev 域名的相关资源时，将会自动跳转到 mgxian.dev 域名。 启动 Caddy 启动 Caddy 之后会自动申请 HTTPS 的证书，并自动开启对 HTTP2 协议的支持，启动过程中可能会提示你输入邮箱接受证书相关的提醒。 1caddy -conf Caddyfile 注意事项 在配置使用 Caddy 之前请确保 DNS 解析配置正常，请把你需要配置的域名解析到你安装配置 Caddy 机器的外网 IP 上。 如果需要申请支持通配符的证书，如为 *.mgxian.dev 申请证书，需要使用 DNS Challenge 的方式来申请证书，具体详细文档可参考 DNS Challenge 。 Caddy 不仅可以作为一个像 Apache 与 Nginx 一样的 Web Server ，也可以配置为反向代理，代理后端 Apache 与 Nginx 等应用。 访问测试 启动完成后，使用 cURL 访问你的网站域名进行测试。 123456789curl -I https://mgxian.dev/HTTP/1.1 200 OKAccept-Ranges: bytesContent-Length: 64846Content-Type: text/html; charset=utf-8Etag: "pwsec41e1a"Last-Modified: Sun, 25 Aug 2019 09:57:40 GMTServer: CaddyDate: Sun, 25 Aug 2019 09:59:27 GMT 参考文档 https://github.com/caddyserver/caddy https://caddyserver.com/docs]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>proxy</tag>
        <tag>caddy</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个可供参考的 git commit message 规范]]></title>
    <url>%2Fposts%2F50%2F</url>
    <content type="text"><![CDATA[一个可供参考的 git commit message 规范]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7升级内核]]></title>
    <url>%2Fposts%2F49%2F</url>
    <content type="text"><![CDATA[配置yum源1234567# 安装yum源rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm# 查看列表yum --disablerepo=* --enablerepo=elrepo-kernel repolistyum --disablerepo=* --enablerepo=elrepo-kernel list kernel* 安装最新版本的kernel12# 安装yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y 设置为默认内核123# 设置生成新的grubgrub2-set-default 0grub2-mkconfig -o /etc/grub2.cfg 安装新版本工具包（可省略）12345# 移除旧版本yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64# 安装新版本yum --disablerepo=* --enablerepo=elrepo-kernel install -y kernel-ml-tools.x86_64 重启查看内核版本12345# 重启reboot# 查看内核版本uname -sr 参考文档 https://www.tecmint.com/install-upgrade-kernel-version-in-centos-7/ https://www.centos.bz/2017/08/upgrade-centos-7-6-kernel-to-4-12-4/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>kernel</tag>
        <tag>centos</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s使用ceph实现动态持久化存储]]></title>
    <url>%2Fposts%2F48%2F</url>
    <content type="text"><![CDATA[简介本文章介绍如何使用ceph为k8s提供动态申请pv的功能。ceph提供底层存储功能，cephfs方式支持k8s的pv的3种访问模式ReadWriteOnce，ReadOnlyMany ，ReadWriteMany ，RBD支持ReadWriteOnce，ReadOnlyMany两种模式访问模式只是能力描述，并不是强制执行的，对于没有按pvc声明的方式使用pv，存储提供者应该负责访问时的运行错误。例如如果设置pvc的访问模式为ReadOnlyMany ，pod挂载后依然可写，如果需要真正的不可写，申请pvc是需要指定 readOnly: true 参数 部署部署k8scentos7使用kubeadm安装k8s-1.11版本 部署cephcentos7安装ceph分布式存储集群 在k8s集群中配置使用ceph使用Ceph RBD使用kubeadm安装集群的额外配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103# 如果使用kubeadm部署的集群需要这些额外的步骤# 由于使用动态存储时 controller-manager 需要使用 rbd 命令创建 image# 所以 controller-manager 需要使用 rbd 命令# 由于官方controller-manager镜像里没有rbd命令# 如果没使用如下方式会报错无法成功创建pvc# 相关 issue https://github.com/kubernetes/kubernetes/issues/38923cat &gt;external-storage-rbd-provisioner.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: rbd-provisioner namespace: kube-system---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: rbd-provisionerrules: - apiGroups: [""] resources: ["persistentvolumes"] verbs: ["get", "list", "watch", "create", "delete"] - apiGroups: [""] resources: ["persistentvolumeclaims"] verbs: ["get", "list", "watch", "update"] - apiGroups: ["storage.k8s.io"] resources: ["storageclasses"] verbs: ["get", "list", "watch"] - apiGroups: [""] resources: ["events"] verbs: ["create", "update", "patch"] - apiGroups: [""] resources: ["endpoints"] verbs: ["get", "list", "watch", "create", "update", "patch"] - apiGroups: [""] resources: ["services"] resourceNames: ["kube-dns"] verbs: ["list", "get"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: rbd-provisionersubjects: - kind: ServiceAccount name: rbd-provisioner namespace: kube-systemroleRef: kind: ClusterRole name: rbd-provisioner apiGroup: rbac.authorization.k8s.io---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: rbd-provisioner namespace: kube-systemrules:- apiGroups: [""] resources: ["secrets"] verbs: ["get"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: rbd-provisioner namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: rbd-provisionersubjects:- kind: ServiceAccount name: rbd-provisioner namespace: kube-system---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: rbd-provisioner namespace: kube-systemspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: rbd-provisioner spec: containers: - name: rbd-provisioner image: "quay.io/external_storage/rbd-provisioner:v2.1.1-k8s1.11" env: - name: PROVISIONER_NAME value: ceph.com/rbd serviceAccount: rbd-provisionerEOFkubectl apply -f external-storage-rbd-provisioner.yaml# 查看状态 等待running之后 再进行后续的操作kubectl get pod -n kube-system 配置 storageclass12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 在k8s集群中所有节点安装 ceph-common# 需要使用kubelet使用rdb命令map附加rbd创建的imageyum install -y ceph-common# 创建 osd pool 在ceph的mon或者admin节点ceph osd pool create kube 4096ceph osd pool ls# 创建k8s访问ceph的用户 在ceph的mon或者admin节点ceph auth get-or-create client.kube mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=kube' -o ceph.client.kube.keyring# 查看key 在ceph的mon或者admin节点ceph auth get-key client.adminceph auth get-key client.kube# 创建 admin secret# CEPH_ADMIN_SECRET 替换为 client.admin 获取到的keyexport CEPH_ADMIN_SECRET='AQBBAnRbSiSOFxAAEZXNMzYV6hsceccYLhzdWw=='kubectl create secret generic ceph-secret --type="kubernetes.io/rbd" \--from-literal=key=$CEPH_ADMIN_SECRET \--namespace=kube-system# 在 default 命名空间创建pvc用于访问ceph的 secret# CEPH_KUBE_SECRET 替换为 client.kube 获取到的keyexport CEPH_KUBE_SECRET='AQBZK3VbTN/QOBAAIYi6CRLQcVevW5HM8lunOg=='kubectl create secret generic ceph-user-secret --type="kubernetes.io/rbd" \--from-literal=key=$CEPH_KUBE_SECRET \--namespace=default# 查看 secretkubectl get secret ceph-user-secret -o yamlkubectl get secret ceph-secret -n kube-system -o yaml# 配置 StorageClass# 如果使用kubeadm创建的集群 provisioner 使用如下方式# provisioner: ceph.com/rbdcat &gt;storageclass-ceph-rdb.yaml&lt;&lt;EOFkind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: dynamic-ceph-rdbprovisioner: ceph.com/rbd# provisioner: kubernetes.io/rbdparameters: monitors: 11.11.11.111:6789,11.11.11.112:6789,11.11.11.113:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-user-secret fsType: ext4 imageFormat: "2" imageFeatures: "layering"EOF# 创建kubectl apply -f storageclass-ceph-rdb.yaml# 查看kubectl get sc 测试使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 创建pvc测试cat &gt;ceph-rdb-pvc-test.yaml&lt;&lt;EOFkind: PersistentVolumeClaimapiVersion: v1metadata: name: ceph-rdb-claimspec: accessModes: - ReadWriteOnce storageClassName: dynamic-ceph-rdb resources: requests: storage: 2GiEOFkubectl apply -f ceph-rdb-pvc-test.yaml # 查看kubectl get pvckubectl get pv # 创建 nginx pod 挂载测试cat &gt;nginx-pod.yaml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-pod1 labels: name: nginx-pod1spec: containers: - name: nginx-pod1 image: nginx:alpine ports: - name: web containerPort: 80 volumeMounts: - name: ceph-rdb mountPath: /usr/share/nginx/html volumes: - name: ceph-rdb persistentVolumeClaim: claimName: ceph-rdb-claimEOFkubectl apply -f nginx-pod.yaml # 查看kubectl get pods -o wide # 修改文件内容kubectl exec -ti nginx-pod1 -- /bin/sh -c 'echo Hello World from Ceph RBD!!! &gt; /usr/share/nginx/html/index.html' # 访问测试POD_ID=$(kubectl get pods -o wide | grep nginx-pod1 | awk '&#123;print $(NF-1)&#125;')curl http://$POD_ID# 清理kubectl delete -f nginx-pod.yamlkubectl delete -f ceph-rdb-pvc-test.yaml 使用 CephFS linux内核需要4.10+，否则会出现无法正常使用的问题，详细issue信息 https://github.com/kubernetes-incubator/external-storage/issues/345 centos7升级内核 在ceph集群创建CephFS123456789101112# 如下操作在ceph的mon或者admin节点# CephFS需要使用两个Pool来分别存储数据和元数据ceph osd pool create fs_data 128ceph osd pool create fs_metadata 128ceph osd lspools# 创建一个CephFSceph fs new cephfs fs_metadata fs_data# 查看ceph fs ls 部署cephfs-provisioner123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102# 官方没有cephfs动态卷支持# 使用社区提供的cephfs-provisionercat &gt;external-storage-cephfs-provisioner.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: cephfs-provisioner namespace: kube-system---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: cephfs-provisionerrules: - apiGroups: [""] resources: ["persistentvolumes"] verbs: ["get", "list", "watch", "create", "delete"] - apiGroups: [""] resources: ["persistentvolumeclaims"] verbs: ["get", "list", "watch", "update"] - apiGroups: ["storage.k8s.io"] resources: ["storageclasses"] verbs: ["get", "list", "watch"] - apiGroups: [""] resources: ["events"] verbs: ["create", "update", "patch"] - apiGroups: [""] resources: ["endpoints"] verbs: ["get", "list", "watch", "create", "update", "patch"] - apiGroups: [""] resources: ["secrets"] verbs: ["create", "get", "delete"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: cephfs-provisionersubjects: - kind: ServiceAccount name: cephfs-provisioner namespace: kube-systemroleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: cephfs-provisioner namespace: kube-systemrules: - apiGroups: [""] resources: ["secrets"] verbs: ["create", "get", "delete"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: cephfs-provisioner namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisionersubjects:- kind: ServiceAccount name: cephfs-provisioner namespace: kube-system---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: cephfs-provisioner namespace: kube-systemspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: cephfs-provisioner spec: containers: - name: cephfs-provisioner image: "quay.io/external_storage/cephfs-provisioner:v2.0.0-k8s1.11" env: - name: PROVISIONER_NAME value: ceph.com/cephfs command: - "/usr/local/bin/cephfs-provisioner" args: - "-id=cephfs-provisioner-1" serviceAccount: cephfs-provisionerEOFkubectl apply -f external-storage-cephfs-provisioner.yaml# 查看状态 等待running之后 再进行后续的操作kubectl get pod -n kube-system #####配置 storageclass 12345678910111213141516171819202122232425262728293031323334# 查看key 在ceph的mon或者admin节点ceph auth get-key client.admin# 创建 admin secret# CEPH_ADMIN_SECRET 替换为 client.admin 获取到的key# 如果在测试 ceph rbd 方式已经添加 可以略过此步骤export CEPH_ADMIN_SECRET='AQBBAnRbSiSOFxAAEZXNMzYV6hsceccYLhzdWw=='kubectl create secret generic ceph-secret --type="kubernetes.io/rbd" \--from-literal=key=$CEPH_ADMIN_SECRET \--namespace=kube-system# 查看 secretkubectl get secret ceph-secret -n kube-system -o yaml# 配置 StorageClasscat &gt;storageclass-cephfs.yaml&lt;&lt;EOFkind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: dynamic-cephfsprovisioner: ceph.com/cephfsparameters: monitors: 11.11.11.111:6789,11.11.11.112:6789,11.11.11.113:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: "kube-system" claimRoot: /volumes/kubernetesEOF# 创建kubectl apply -f storageclass-cephfs.yaml# 查看kubectl get sc 测试使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 创建pvc测试cat &gt;cephfs-pvc-test.yaml&lt;&lt;EOFkind: PersistentVolumeClaimapiVersion: v1metadata: name: cephfs-claimspec: accessModes: - ReadWriteOnce storageClassName: dynamic-cephfs resources: requests: storage: 2GiEOFkubectl apply -f cephfs-pvc-test.yaml # 查看kubectl get pvckubectl get pv # 创建 nginx pod 挂载测试cat &gt;nginx-pod.yaml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-pod1 labels: name: nginx-pod1spec: containers: - name: nginx-pod1 image: nginx:alpine ports: - name: web containerPort: 80 volumeMounts: - name: cephfs mountPath: /usr/share/nginx/html volumes: - name: cephfs persistentVolumeClaim: claimName: cephfs-claimEOFkubectl apply -f nginx-pod.yaml # 查看kubectl get pods -o wide # 修改文件内容kubectl exec -ti nginx-pod1 -- /bin/sh -c 'echo Hello World from CephFS!!! &gt; /usr/share/nginx/html/index.html' # 访问测试POD_ID=$(kubectl get pods -o wide | grep nginx-pod1 | awk '&#123;print $(NF-1)&#125;')curl http://$POD_ID# 清理kubectl delete -f nginx-pod.yamlkubectl delete -f cephfs-pvc-test.yaml 参考文档 https://kubernetes.io/docs/concepts/storage/storage-classes/ https://docs.openshift.com/container-platform/3.5/install_config/storage_examples/ceph_rbd_dynamic_example.html https://ieevee.com/tech/2018/05/17/k8s-cephfs.html https://github.com/kubernetes-incubator/external-storage/tree/master/ceph/rbd https://github.com/kubernetes-incubator/external-storage/blob/master/ceph/rbd/deploy/README.md https://github.com/heketi/heketi/blob/master/docs/admin/install-kubernetes.md https://github.com/gluster/gluster-kubernetes/blob/master/docs/setup-guide.md https://github.com/gluster/gluster-kubernetes/blob/master/docs/examples/hello_world/README.md https://jimmysong.io/kubernetes-handbook/practice/using-heketi-gluster-for-persistent-storage.html https://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://docs.openshift.com/enterprise/3.1/architecture/additional_concepts/storage.html#pv-access-modes]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph安装配置]]></title>
    <url>%2Fposts%2F47%2F</url>
    <content type="text"><![CDATA[简介ceph是一个开源分布式存储系统，支持PB级别的存储，支持对象存储，块存储和文件存储，高性能，高可用，可扩展。 部署网络建议架构图 部署 部署架构图，本次实验部署jewel版本 实验环境的Vagrantfile lab1节点既作admin节点又作node节点，lab2，lab3只作为node节点，lab4作为作测试使用ceph的节点 123456789101112131415161718192021222324252627# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..4).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 3 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "3096"] file_to_disk = "lab#&#123;i&#125;_vdb.vdi" unless File.exist?(file_to_disk) # 50GB v.customize ['createhd', '--filename', file_to_disk, '--size', 50 * 1024] end v.customize ['storageattach', :id, '--storagectl', 'IDE', '--port', 1, '--device', 0, '--type', 'hdd', '--medium', file_to_disk] end end endend 配置阿里ceph源 在所有节点执行如下操作 1234567891011121314151617181920212223cat &gt;/etc/yum.repos.d/ceph.repo&lt;&lt;EOF[ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/gpgcheck=0priority=1[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/gpgcheck=0priority=1[ceph-source]name=Ceph source packagesbaseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/SRPMSenabled=0gpgcheck=1type=rpm-mdgpgkey=http://mirrors.163.com/ceph/keys/release.ascpriority=1EOFyum makecache 在admin节点安装ceph-deploy lab1 节点 12345678910111213141516171819# 官方源# 如果已经配置了上面的阿里源，不需要再配置如下的源# 推荐使用阿里源，因为官方源速度太慢cat &gt;/etc/yum.repos.d/ceph.repo&lt;&lt;EOF[ceph-noarch]name=Ceph noarch packagesbaseurl=https://download.ceph.com/rpm-jewel/el7/noarchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascEOF# 更新系统软件# 此操作可省略# yum update -y# 安装 ceph-deployyum install -y ceph-deploy 配置admin节点连接node节点 安装之后需要配置admin节点可以ssh无密码登录每个node节点和测试节点，用户需要有sudo权限 123456789101112131415161718192021222324252627282930313233343536# 在每一个node节点执行useradd cephecho 'ceph' | passwd --stdin cephecho "ceph ALL = (root) NOPASSWD:ALL" &gt; /etc/sudoers.d/cephchmod 0440 /etc/sudoers.d/ceph# 配置sshd可以使用password登录sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_configsystemctl reload sshd# 配置sudo不需要ttysed -i 's/Default requiretty/#Default requiretty/' /etc/sudoers# 在所有节点配置hosts# 包括要进行ceph测试的机器# 使用vagrant实验时注意# 由于vagrant会自动把主机名解析为 127.0.0.1# 所以在实验时如果在ceph集群内任意一台机器进行实验时# 注意把本机名解析为 127.0.0.1 的行注释，如下所示# 127.0.0.1 lab1 lab1cat &gt;&gt;/etc/hosts&lt;&lt;EOF11.11.11.111 lab111.11.11.112 lab211.11.11.113 lab311.11.11.113 lab4EOF# 在admin节点执行# 创建ceph用户，配置sshkey登录# 由于lab1节点作为node节点时已经创建过ceph用户# 第一条命令可能会出错，忽略即可useradd cephsu - cephssh-keygenssh-copy-id ceph@lab1ssh-copy-id ceph@lab2ssh-copy-id ceph@lab3ssh-copy-id ceph@lab4 在admin节点创建集群 在lab1节点执行如下操作，node的主机名一定要设置正确 lab1, lab2, lab3。否则可能会无法实验成功 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 不要使用sudo也不要使用root用户运行如下的命令su - cephmkdir my-clustercd my-cluster# 创建lab1为monitorceph-deploy new lab1# 查看配置文件ls -l# 配置ceph.conf[global]...# 如果有多个网卡，应该配置如下选项，# public network是公共网络，负责集群对外提供服务的流量# cluster network是集群网络，负载集群中数据复制传输通信等# 本次实验使用同一块网卡，生境环境建议分别使用一块网卡public network = 11.11.11.0/24cluster network = 11.11.11.0/24# 安装 ceph 包# 如果按照官方文档安装方法 会重新配置安装官方ceph源# 由于网络问题，安装可能会出错，需要多次执行# ceph-deploy install 其实只是会安装 ceph ceph-radosgw 两个包# ceph-deploy install lab1 lab2 lab3# 推荐使用阿里源安装，因为使用ceph-deploy安装会很慢# 使用如下命令手动安装包，替代官方的 ceph-deploy install 命令# 如下操作在所有node节点上执行yum install -y ceph ceph-radosgw# 部署monitor和生成keysceph-deploy mon create-initialls -l *.keyring# 复制文件到node节点ceph-deploy admin lab1 lab2 lab3# 部署manager （luminous+）12及以后的版本需要部署# 本次部署 jewel 版本 ，不需要执行如下命令# ceph-deploy mgr create lab1# 添加osd 以磁盘方式# 本次实验采用此种方法# sdb 为虚拟机添加的磁盘设置名ceph-deploy osd create lab1:sdb lab2:sdb lab3:sdb# 在node节点创建目录rm -rf /data/osd1mkdir -pv /data/osd1chmod 777 -R /data/osd1chown ceph.ceph -R /data/osd1# 添加osd 以文件目录方式ceph-deploy osd prepare lab1:/data/osd1 lab2:/data/osd1 lab3:/data/osd1ceph-deploy osd activate lab1:/data/osd1 lab2:/data/osd1 lab3:/data/osd1# 查看状态ssh lab1 sudo ceph healthssh lab1 sudo ceph -s 清理集群12345# 如果安装过程出错，使用如下命令清理之后重新开始ceph-deploy purge lab1 lab2 lab3ceph-deploy purgedata lab1 lab2 lab3ceph-deploy forgetkeysrm ceph.* 扩展集群提高可用性 在lab1上运行metadata server 为后续使用cephfs 在lab2,lab3运行monitor和manager提高集群可用性 1234567891011121314# 为了使用CephFS，必须启动 metadata serverceph-deploy mds create lab1# 添加monitorceph-deploy mon add lab2ceph-deploy mon add lab3ssh lab1 sudo ceph -s# 在monitor节点查看状态（需要以root用户或者sudo查看）ceph quorum_status --format json-pretty# 添加manager （luminous+）12及以后的版本需要部署# 本次部署 jewel 版本 ，不需要执行如下命令# ceph-deploy mgr create lab2 lab3 部署RGW使用Ceph Object Gateway 提供S3/Swift存储功能，实现S3和Swift兼容的接口，可以使用S3或Swift的命令行工具或SDK来使用ceph1234567891011121314# 启动 rgwceph-deploy rgw create lab1# 修改配置 /etc/ceph/ceph.conf# 使用 rgw 监听在 80 端口# lab1 为启动 rgw 的主机名[client.rgw.lab1]rgw_frontends = "civetweb port=80"# 重启 rgwsystemctl restart ceph-radosgw@rgw.lab1# 访问测试curl -I http://11.11.11.111/ 使用ceph存储 应用存储使用架构图 对象存储12345678910111213141516171819202122232425# 安装cephyum install -y ceph# 复制相关文件到要使用ceph-client的机器ceph-deploy admin lab4# 测试# 存储文件echo 'hello ceph oject storage' &gt; testfile.txtceph osd pool create mytest 8rados put test-object-1 testfile.txt --pool=mytest# 查看读取文件rados -p mytest lsrados get test-object-1 testfile.txt.1 --pool=mytestcat testfile.txt.1# 查看文件位置ceph osd map mytest test-object-1# 删除文件rados rm test-object-1 --pool=mytest# 删除poolceph osd pool rm mytest mytest --yes-i-really-really-mean-it 块存储1234567891011121314151617181920212223242526272829# 安装cephyum install -y ceph# 复制相关文件到要使用ceph-client的机器ceph-deploy admin lab4# 创建块设备镜像rbd create foo --size 4096 --image-feature layeringrbd info foorados -p rbd ls# 映射镜像到块设备sudo rbd map foo --name client.admin# 使用块设备创建文件系统sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo# 挂载使用sudo mkdir /mnt/ceph-block-devicesudo mount /dev/rbd/rbd/foo /mnt/ceph-block-devicecd /mnt/ceph-block-deviceecho 'hello ceph block storage' &gt; testfile.txt# 清理cd ~sudo umount -lf /mnt/ceph-block-devicesudo rbd unmap foorbd remove foorados -p rbd ls S3对象存储 11.11.11.111 为安装了 RGW 的机器 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# 安装yum install -y ceph ceph-radosgw# 复制相关文件到要使用ceph-client的机器ceph-deploy admin lab4# 创建S3所需要的poolceph osd pool create .rgw 128 128ceph osd pool create .rgw.root 128 128ceph osd pool create .rgw.control 128 128ceph osd pool create .rgw.gc 128 128ceph osd pool create .rgw.buckets 128 128ceph osd pool create .rgw.buckets.index 128 128ceph osd pool create .rgw.buckets.extra 128 128ceph osd pool create .log 128 128ceph osd pool create .intent-log 128 128ceph osd pool create .usage 128 128ceph osd pool create .users 128 128ceph osd pool create .users.email 128 128ceph osd pool create .users.swift 128 128ceph osd pool create .users.uid 128 128# 查看rados lspools# 访问测试curl -I http://11.11.11.111/# 创建S3用户# 保存如下命令返回的 user access_key secret_keyradosgw-admin user create --uid=foo --display-name=foo --email=foo@foo.com# 创建admin用户radosgw-admin user create --uid=admin --display-name=admin# 允许admin读写users信息radosgw-admin caps add --uid=admin --caps="users=*"# 允许admin读写所有的usage信息radosgw-admin caps add --uid=admin --caps="usage=read,write"# 安装s3测试工具yum install -y s3cmd# 配置s3cmd, 只需指定Access Key和Secret Key，其他默认即可s3cmd --configure# 修该生成的配置文件vim $HOME/.s3cfghost_base = 11.11.11.111host_bucket = 11.11.11.111/%(bucket)use_https = False# 创建Buckets3cmd mb s3://mybuckets3cmd ls# 上传Objectecho 'hello ceph block storage s3' &gt; hello.txts3cmd put hello.txt s3://mybucket# 查看Objects3cmd ls s3://mybucket# 下载Objectcd /tmps3cmd get s3://mybucket/hello.txtcd ~# 删除bucket下所有对象s3cmd del -rf s3://mybucket/s3cmd ls -r s3://mybucket# 删除Buckets3cmd mb s3://mybucket1s3cmd rb s3://mybucket1# 删除S3用户radosgw-admin user rm --uid=fooradosgw-admin user rm --uid=admin# 删除poolceph osd pool delete .rgw .rgw --yes-i-really-really-mean-itceph osd pool delete .rgw.root .rgw.root --yes-i-really-really-mean-itceph osd pool delete .rgw.control .rgw.control --yes-i-really-really-mean-itceph osd pool delete .rgw.gc .rgw.gc --yes-i-really-really-mean-itceph osd pool delete .rgw.buckets .rgw.buckets --yes-i-really-really-mean-itceph osd pool delete .rgw.buckets.index .rgw.buckets.index --yes-i-really-really-mean-itceph osd pool delete .rgw.buckets.extra .rgw.buckets.extra --yes-i-really-really-mean-itceph osd pool delete .log .log --yes-i-really-really-mean-itceph osd pool delete .intent-log .intent-log --yes-i-really-really-mean-itceph osd pool delete .usage .usage --yes-i-really-really-mean-itceph osd pool delete .users .users --yes-i-really-really-mean-itceph osd pool delete .users.email .users.email --yes-i-really-really-mean-itceph osd pool delete .users.swift .users.swift --yes-i-really-really-mean-itceph osd pool delete .users.uid .users.uid --yes-i-really-really-mean-it CephFS存储123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 安装cephyum install -y ceph ceph-fuse# 复制相关文件到要使用ceph-client的机器ceph-deploy admin lab4# CephFS需要使用两个Pool来分别存储数据和元数据ceph osd pool create fs_data 128ceph osd pool create fs_metadata 128ceph osd lspools# 创建一个CephFSceph fs new cephfs fs_metadata fs_data# 查看ceph fs ls# 使用内核提供的功能 挂载CephFS# 由于可能会有bug，建议使用 4.0 以上的内核# 优点是性能比使用ceph-fuse更好# name，secret 为 /etc/ceph/ceph.client.admin.keyring 里的内容mkdir /mnt/mycephfsmount -t ceph lab1:6789,lab2:6789,lab3:6789:/ /mnt/mycephfs -o name=admin,secret=AQBoclRaiilZJBAACLjqg2OUOOB/FNa20UJXYA==df -hcd /mnt/mycephfsecho 'hello ceph CephFS' &gt; hello.txtcd ~umount -lf /mnt/mycephfsrm -rf /mnt/mycephfs# 使用 ceph-fuse 挂载CephFSmkdir /mnt/mycephfsceph-fuse -m lab1:6789 /mnt/mycephfsdf -hcd /mnt/mycephfsecho 'hello ceph CephFS' &gt; hello.txtcd ~umount -lf /mnt/mycephfsrm -rf /mnt/mycephfs# 清理# 停止 metadata server# 本次部署在lab1，去lab1停止服务systemctl stop ceph-mds@lab1ceph fs rm cephfs --yes-i-really-mean-itceph osd pool delete fs_data fs_data --yes-i-really-really-mean-itceph osd pool delete fs_metadata fs_metadata --yes-i-really-really-mean-it# 开启 metadata server# 方便以后使用 cephfssystemctl start ceph-mds@lab1 参考文档 http://docs.ceph.com/docs/master/cephfs/best-practices/ http://docs.ceph.com/docs/master/start/ http://docs.ceph.org.cn/start/ http://docs.ceph.com/docs/master/start/quick-rbd/ http://www.xuxiaopang.com/2016/10/13/easy-ceph-RBD/ http://docs.ceph.com/docs/master/start/quick-rgw/ https://blog.frognew.com/tags/ceph.html https://www.centos.bz/2017/10/%E7%94%A8ceph-deploy%E5%AE%89%E8%A3%85ceph%E5%B9%B6%E9%83%A8%E7%BD%B2%E9%9B%86%E7%BE%A4/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[istio-1.0微服务实验]]></title>
    <url>%2Fposts%2F46%2F</url>
    <content type="text"><![CDATA[由于1.0版本和0.8版本没有改动api，可以直接使用0.8版本的微服务实验]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>service mesh</tag>
        <tag>istio</tag>
        <tag>k8s</tag>
        <tag>microservice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s日志收集实战]]></title>
    <url>%2Fposts%2F45%2F</url>
    <content type="text"><![CDATA[简介本文主要介绍在k8s中收集应用的日志方案，应用运行中日志，一般情况下都需要收集存储到一个集中的日志管理系统中，可以方便对日志进行分析统计，监控，甚至用于机器学习，智能分析应用系统问题，及时修复应用所存在的问题。 在k8s集群中应用一般有如下日志输出方式 直接遵循docker官方建议把日志输出到标准输出或者标准错误输出 输出日志到容器内指定目录中 应用直接发送日志给日志收集系统 本文会综合部署上述日志收集方案。日志收集组件说明 elastisearch 存储收集到的日志 kibana 可视化收集到的日志 logstash 汇总处理日志发送给elastisearch 存储 filebeat 读取容器或者应用日志文件处理发送给elastisearch或者logstash，也可用于汇总日志 fluentd 读取容器或者应用日志文件处理发送给elastisearch，也可用于汇总日志 fluent-bit 读取容器或者应用日志文件处理发送给elastisearch或者fluentd 部署 本次实验使用了3台虚拟机做k8s集群，每台虚拟机3G内存 部署前的准备1234567# 拉取文件git clone https://github.com/mgxian/k8s-log.gitcd k8s-loggit checkout v1# 创建 logging namespacekubectl apply -f logging-namespace.yaml 部署elastisearch1234567891011121314151617181920# 本次部署虽然使用 StatefulSet 但是没有使用pv进行持久化数据存储# pod重启之后，数据会丢失，生产环境一定要使用pv持久化存储数据# 部署kubectl apply -f elasticsearch.yaml# 查看状态kubectl get pods,svc -n logging -o wide# 等待所有pod变成running状态 # 访问测试# 如果测试都有数据返回代表部署成功kubectl run curl -n logging --image=radial/busyboxplus:curl -i --ttynslookup elasticsearch-loggingcurl 'http://elasticsearch-logging:9200/_cluster/health?pretty'curl 'http://elasticsearch-logging:9200/_cat/nodes'exit# 清理测试kubectl delete deploy curl -n logging 部署kibana1234567891011# 部署kubectl apply -f kibana.yaml# 查看状态kubectl get pods,svc -n logging -o wide# 访问测试# 浏览器访问下面输出的地址 看到 kibana 界面代表正常# 11.11.11.112 为集群中某个 node 节点ipKIBANA_NODEPORT=$(kubectl get svc -n logging | grep kibana-logging | awk '&#123;print $(NF-1)&#125;' | awk -F[:/] '&#123;print $2&#125;')echo "http://11.11.11.112:$KIBANA_NODEPORT/" 部署fluentd收集日志12345678910111213# fluentd 以 daemoset 方式部署# 在每个节点上启动fluentd容器，收集k8s组件，docker以及容器的日志# 给每个需要启动fluentd的节点打相关label# kubectl label node lab1 beta.kubernetes.io/fluentd-ds-ready=truekubectl label nodes --all beta.kubernetes.io/fluentd-ds-ready=true# 部署kubectl apply -f fluentd-es-configmap.yamlkubectl apply -f fluentd-es-ds.yaml# 查看状态kubectl get pods,svc -n logging -o wide kibana查看日志 创建index fluentd-k8s-*，由于需要拉取镜像启动容器，可能需要等待几分钟才能看到索引和数据 查看日志 应用日志收集测试应用日志输出到标准输出测试1234567891011# 启动测试日志输出kubectl run echo-test --image=radial/busyboxplus:curl -- sh -c 'count=1;while true;do echo log to stdout $count;sleep 1;count=$(($count+1));done'# 查看状态kubectl get pods -o wide# 命令行查看日志ECHO_TEST_POD=$(kubectl get pods | grep echo-test | awk '&#123;print $1&#125;')kubectl logs -f $ECHO_TEST_POD# 刷新 kibana 查看是否有新日志进入 应用日志输出到容器指定目录(filebeat收集)12345# 部署kubectl apply -f log-contanier-file-filebeat.yaml# 查看kubectl get pods -o wide 添加index filebeat-k8s-* 查看日志 应用日志输出到容器指定目录(fluent-bit收集)12345# 部署kubectl apply -f log-contanier-file-fluentbit.yaml# 查看kubectl get pods -o wide 添加index fluentbit-k8s-* 查看日志 应用直接发送日志到日志系统1234567# 本次测试应用直接输出日志到 elasticsearch# 部署kubectl apply -f log-contanier-es.yaml# 查看kubectl get pods -o wide 添加index k8s-app-* 查看日志 清理1234kubectl delete -f log-contanier-es.yamlkubectl delete -f log-contanier-file-fluentbit.yamlkubectl delete -f log-contanier-file-filebeat.yamlkubectl delete deploy echo-test 日志收集系统总结 本小节的图表以ELK技术栈展示说明，实际使用过程中可以使用EFK技术栈，使用fluentd代替logstash，使用fluent-bit代替filebeat。由于fluentd在内存占用和性能上有更好的优势，推荐使用fluentd替代logstash ，fluent-bit和filebeat性能和内存占用相差不大 k8s集群日志通用收集方案 集群内相关组件日志使用fluentd/filebeat收集 应用输出到标准输出或标准错误输出的日志使用fluentd/filebeat收集 应用输出到容器中指定文件日志使用fluent-bit/filebeat收集 通用日志收集系统 通用日志收集系统架构 架构说明 日志收集与处理解耦 由于收集和处理过程间加入了队列，当日志出现暴增时，可以避免分析处理节点被打垮，给分析处理节点足够时间消化日志数据 日志分析处理节点可以动态伸缩 大流量日志收集系统 大流量日志收集系统架构图 架构说明 当日志流量过大时，如果每一个日志收集节点都直连队列写数据，由于有很多分散的连接及写请求，会给队列造成压力。如果日志都发送到logstash收集节点，再集中写入队列，会减轻队列压力。 应用日志收集实验(ELK技术栈)以收集nginx日志为例，进行日志收集分析实验， 复用之前实验创建的elasticsearch，kibana应用。实验采用大流量日志收集架构 部署redis队列12345# 部署kubectl apply -f redis.yaml# 查看kubectl get pods -n logging 部署indexer分析日志12345# 部署kubectl apply -f logstash-indexer.yaml# 查看kubectl get pods -n logging 部署shipper集中日志12345# 部署kubectl apply -f logstash-shipper.yaml# 查看kubectl get pods -n logging 部署nginx测试日志收集12345# 部署kubectl apply -f nginx-log-filebeat.yaml# 查看kubectl get pods 持续访问nginx生成日志12345# 部署kubectl run curl-test --image=radial/busyboxplus:curl -- sh -c 'count=1;while true;do curl -s -H "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.89 Safari/537.36 $count" http://nginx-log-filebeat/ &gt;/dev/null;sleep 1;count=$(($count+1));done'# 查看kubectl get pods 访问kibana查看日志 添加index k8s-logging-elk-* 由于 logstash 启动较慢，可能需要等待数分钟才能看到数据 清理12345kubectl delete -f redis.yamlkubectl delete -f logstash-indexer.yamlkubectl delete -f logstash-shipper.yamlkubectl delete -f nginx-log-filebeat.yamlkubectl delete deploy curl-test 应用日志收集实验(EFK技术栈)由于fluentd官方不提供redis队列的支持，本次实验移除了redis队列。 部署indexer分析日志12345# 部署kubectl apply -f fluentd-indexer.yaml# 查看kubectl get pods -n logging 部署shipper集中日志12345# 部署kubectl apply -f fluentd-shipper.yaml# 查看kubectl get pods -n logging 部署nginx测试日志收集12345# 部署kubectl apply -f nginx-log-fluentbit.yaml# 查看kubectl get pods 持续访问nginx生成日志12345# 部署kubectl run curl-test --image=radial/busyboxplus:curl -- sh -c 'count=1;while true;do curl -s -H "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.89 Safari/537.36 $count" http://nginx-log-fluentbit/ &gt;/dev/null;sleep 1;count=$(($count+1));done'# 查看kubectl get pod 访问kibana查看日志 添加index k8s-logging-efk-* 清理1234kubectl delete -f fluentd-indexer.yamlkubectl delete -f fluentd-shipper.yamlkubectl delete -f nginx-log-fluentbit.yamlkubectl delete deploy curl-test 应用日志可视化部署日志收集需要的组件12345678# 部署 indexer shipper fluentbitkubectl apply -f fluentd-indexer.yamlkubectl apply -f fluentd-shipper.yamlkubectl apply -f nginx-log-fluentbit.yaml# 查看kubectl get podskubectl get pods -n logging 模拟用户访问12345# 部署kubectl apply -f web-load-gen.yaml# 查看kubectl get pods 访问kibana查看日志 添加index k8s-logging-efk-* 创建图表创建 Search制作 Visualize 的时候需要使用 按指定条件搜索日志 保存 Search 创建 Visualize创建好的 Visualize 可以添加到 Dashboard 中 选择制作 Visualize 选择 Visualize 类型 选择使用上面步骤保存的 Search 选择指定的 bucket 选择 code 字段进行统计 保存 Visualize 使用如上的步骤创建多个 Visualize 创建 Dashboard选择创建 Dashboard 把 Visualize 添加到 Dashboard 保存 Dashboard 编辑调整位置和大小 最终图表展示 如果快速体验可以在 菜单 Managerment 的 Saved Ojects 标签直接使用导入功能，导入本次实验下载目录k8s-log下的k8s-kibana-all.json文件 参考文档 https://kubernetes.io/docs/concepts/cluster-administration/logging/ https://banzaicloud.com/blog/k8s-logging/ https://docs.fluentd.org/v0.12/articles/kubernetes-fluentd https://jimmysong.io/kubernetes-handbook/practice/app-log-collection.html https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/README.md https://www.elastic.co/blog/shipping-kubernetes-logs-to-elasticsearch-with-filebeat https://github.com/elastic/beats/blob/master/deploy/kubernetes/filebeat/README.md https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-docker.html https://www.elastic.co/guide/en/beats/filebeat/current/add-kubernetes-metadata.html https://github.com/fluent/fluentd-kubernetes-daemonset https://github.com/fluent/fluent-bit-kubernetes-logging https://github.com/fluent/fluent-bit https://www.docker.elastic.co/ https://fluentbit.io/documentation/0.13/ https://docs.fluentd.org/v1.0/articles/quickstart https://www.elastic.co/guide/en/logstash/6.3/deploying-and-scaling.html]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>log</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s使用openebs实现动态持久化存储]]></title>
    <url>%2Fposts%2F44%2F</url>
    <content type="text"><![CDATA[简介本文章介绍如何使用openebs为k8s提供动态申请pv的功能。iscsi提供底层存储功能，openebs管理iscsi。目前只支持pv的ReadWriteOnce访问模式 访问模式只是能力描述，并不是强制执行的，对于没有按pvc声明的方式使用pv，存储提供者应该负责访问时的运行错误。例如如果设置pvc的访问模式为ReadOnlyMany ，pod挂载后依然可写，如果需要真正的不可写，申请pvc是需要指定 readOnly: true 参数 安装实验用的Vagrantfile123456789101112131415161718192021222324252627# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..3).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "3096"] file_to_disk = "lab#&#123;i&#125;_vdb.vdi" unless File.exist?(file_to_disk) # 50GB v.customize ['createhd', '--filename', file_to_disk, '--size', 50 * 1024] end v.customize ['storageattach', :id, '--storagectl', 'IDE', '--port', 1, '--device', 0, '--type', 'hdd', '--medium', file_to_disk] end end endend 安装配置iscsi123456789101112# 安装 iscsiyum install iscsi-initiator-utils -y# 查看 InitiatorName 是否正常配置cat /etc/iscsi/initiatorname.iscsi# 启动查看状态systemctl start iscsid.servicesystemctl status iscsid.servicesystemctl start iscsi.servicesystemctl status iscsi.service 安装openebs1234567891011# 部署mkdir openebs &amp;&amp; cd openebswget https://raw.githubusercontent.com/openebs/openebs/v0.6/k8s/openebs-operator.yamlwget https://raw.githubusercontent.com/openebs/openebs/v0.6/k8s/openebs-storageclasses.yamlkubectl apply -f openebs-operator.yamlkubectl apply -f openebs-storageclasses.yaml# 查看 openebs 状态kubectl get pods -n openebs -o widekubectl get svc -n openebskubectl get crd 测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 查看 storage classkubectl get sc# 创建pvc测试cat &gt;openebs-pvc-test.yaml&lt;&lt;EOFapiVersion: v1kind: PersistentVolumeClaimmetadata: name: openebs1spec: storageClassName: openebs-standard accessModes: - ReadWriteOnce resources: requests: storage: 5GiEOFkubectl apply -f openebs-pvc-test.yaml # 查看kubectl get pvckubectl get pv # 创建 nginx pod 挂载测试cat &gt;nginx-pod.yaml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-pod1 labels: name: nginx-pod1spec: containers: - name: nginx-pod1 image: nginx:alpine ports: - name: web containerPort: 80 volumeMounts: - name: openebs1-vol1 mountPath: /usr/share/nginx/html volumes: - name: openebs1-vol1 persistentVolumeClaim: claimName: openebs1EOFkubectl apply -f nginx-pod.yaml # 查看kubectl get pods -o wide # 修改文件内容kubectl exec -ti nginx-pod1 -- /bin/sh -c 'echo Hello World from Openebs!!! &gt; /usr/share/nginx/html/index.html' # 访问测试POD_ID=$(kubectl get pods -o wide | grep nginx-pod1 | awk '&#123;print $(NF-1)&#125;')curl http://$POD_ID 参考文档 https://github.com/heketi/heketi/blob/master/docs/admin/install-kubernetes.md https://github.com/gluster/gluster-kubernetes/blob/master/docs/setup-guide.md https://github.com/gluster/gluster-kubernetes/blob/master/docs/examples/hello_world/README.md https://jimmysong.io/kubernetes-handbook/practice/using-heketi-gluster-for-persistent-storage.html https://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://docs.openshift.com/enterprise/3.1/architecture/additional_concepts/storage.html#pv-access-modes]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s使用glusterfs实现动态持久化存储]]></title>
    <url>%2Fposts%2F43%2F</url>
    <content type="text"><![CDATA[简介本文章介绍如何使用glusterfs为k8s提供动态申请pv的功能。glusterfs提供底层存储功能，heketi为glusterfs提供restful风格的api，方便管理glusterfs。支持k8s的pv的3种访问模式ReadWriteOnce，ReadOnlyMany ，ReadWriteMany 访问模式只是能力描述，并不是强制执行的，对于没有按pvc声明的方式使用pv，存储提供者应该负责访问时的运行错误。例如如果设置pvc的访问模式为ReadOnlyMany ，pod挂载后依然可写，如果需要真正的不可写，申请pvc是需要指定 readOnly: true 参数 安装实验用的Vagrantfile123456789101112131415161718192021222324252627# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..3).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "3096"] file_to_disk = "lab#&#123;i&#125;_vdb.vdi" unless File.exist?(file_to_disk) # 50GB v.customize ['createhd', '--filename', file_to_disk, '--size', 50 * 1024] end v.customize ['storageattach', :id, '--storagectl', 'IDE', '--port', 1, '--device', 0, '--type', 'hdd', '--medium', file_to_disk] end end endend 环境配置说明12345678910# 安装 glusterfs 每节点需要提前加载 dm_thin_pool 模块modprobe dm_thin_pool# 配置开启自加载cat &gt;/etc/modules-load.d/glusterfs.conf&lt;&lt;EOFdm_thin_poolEOF# 安装 glusterfs-fuseyum install -y glusterfs-fuse 安装glusterfs与heketi123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148# 安装 heketi client# https://github.com/heketi/heketi/releases# 去github下载相关的版本wget https://github.com/heketi/heketi/releases/download/v7.0.0/heketi-client-v7.0.0.linux.amd64.tar.gztar xf heketi-client-v7.0.0.linux.amd64.tar.gzcp heketi-client/bin/heketi-cli /usr/local/bin# 查看版本heketi-cli -v# 如下部署步骤都在如下目录执行cd heketi-client/share/heketi/kubernetes# 在k8s中部署 glusterfskubectl create -f glusterfs-daemonset.json# 查看 node 节点kubectl get nodes# 给提供存储 node 节点打 labelkubectl label node lab1 lab2 lab3 storagenode=glusterfs# 查看 glusterfs 状态kubectl get pods -o wide# 部署 heketi server # 配置 heketi server 的权限kubectl create -f heketi-service-account.jsonkubectl create clusterrolebinding heketi-gluster-admin --clusterrole=edit --serviceaccount=default:heketi-service-account# 创建 cofig secretkubectl create secret generic heketi-config-secret --from-file=./heketi.json# 初始化部署kubectl create -f heketi-bootstrap.json# 查看 heketi bootstrap 状态kubectl get pods -o widekubectl get svc# 配置端口转发 heketi serverHEKETI_BOOTSTRAP_POD=$(kubectl get pods | grep deploy-heketi | awk '&#123;print $1&#125;')kubectl port-forward $HEKETI_BOOTSTRAP_POD 58080:8080# 测试访问# 另起一终端curl http://localhost:58080/hello# 配置 glusterfs# hostnames/manage 字段里必须和 kubectl get node 一致# hostnames/storage 指定存储网络 ip 本次实验使用与k8s集群同一个ipcat &gt;topology.json&lt;&lt;EOF&#123; "clusters": [ &#123; "nodes": [ &#123; "node": &#123; "hostnames": &#123; "manage": [ "lab1" ], "storage": [ "11.11.11.111" ] &#125;, "zone": 1 &#125;, "devices": [ &#123; "name": "/dev/sdb", "destroydata": false &#125; ] &#125;, &#123; "node": &#123; "hostnames": &#123; "manage": [ "lab2" ], "storage": [ "11.11.11.112" ] &#125;, "zone": 1 &#125;, "devices": [ &#123; "name": "/dev/sdb", "destroydata": false &#125; ] &#125;, &#123; "node": &#123; "hostnames": &#123; "manage": [ "lab3" ], "storage": [ "11.11.11.113" ] &#125;, "zone": 1 &#125;, "devices": [ &#123; "name": "/dev/sdb", "destroydata": false &#125; ] &#125; ] &#125; ]&#125;EOFexport HEKETI_CLI_SERVER=http://localhost:58080heketi-cli topology load --json=topology.json# 使用 Heketi 创建一个用于存储 Heketi 数据库的 volumeheketi-cli setup-openshift-heketi-storagekubectl create -f heketi-storage.json# 查看状态# 等所有job完成 即状态为 Completed# 才能进行如下的步骤kubectl get podskubectl get job# 删除部署时产生的相关资源kubectl delete all,service,jobs,deployment,secret --selector="deploy-heketi"# 部署 heketi serverkubectl create -f heketi-deployment.json# 查看 heketi server 状态kubectl get pods -o widekubectl get svc# 查看 heketi 状态信息# 配置端口转发 heketi serverHEKETI_BOOTSTRAP_POD=$(kubectl get pods | grep heketi | awk '&#123;print $1&#125;')kubectl port-forward $HEKETI_BOOTSTRAP_POD 58080:8080export HEKETI_CLI_SERVER=http://localhost:58080heketi-cli cluster listheketi-cli volume list 测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# 创建 StorageClass# 由于没有开启认证# restuser restuserkey 可以随意写HEKETI_SERVER=$(kubectl get svc | grep heketi | head -1 | awk '&#123;print $3&#125;')echo $HEKETI_SERVERcat &gt;storageclass-glusterfs.yaml&lt;&lt;EOFkind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: gluster-heketiprovisioner: kubernetes.io/glusterfsparameters: resturl: "http://$HEKETI_SERVER:8080" restauthenabled: "false" restuser: "will" restuserkey: "will" gidMin: "40000" gidMax: "50000" volumetype: "replicate:3"EOFkubectl create -f storageclass-glusterfs.yaml# 查看kubectl get sc# 创建pvc测试cat &gt;gluster-pvc-test.yaml&lt;&lt;EOFapiVersion: v1kind: PersistentVolumeClaimmetadata: name: gluster1 annotations: volume.beta.kubernetes.io/storage-class: gluster-heketispec: accessModes: - ReadWriteOnce resources: requests: storage: 5GiEOFkubectl apply -f gluster-pvc-test.yaml # 查看kubectl get pvckubectl get pv # 创建 nginx pod 挂载测试cat &gt;nginx-pod.yaml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-pod1 labels: name: nginx-pod1spec: containers: - name: nginx-pod1 image: nginx:alpine ports: - name: web containerPort: 80 volumeMounts: - name: gluster-vol1 mountPath: /usr/share/nginx/html volumes: - name: gluster-vol1 persistentVolumeClaim: claimName: gluster1EOFkubectl apply -f nginx-pod.yaml # 查看kubectl get pods -o wide # 修改文件内容kubectl exec -ti nginx-pod1 -- /bin/sh -c 'echo Hello World from GlusterFS!!! &gt; /usr/share/nginx/html/index.html' # 访问测试POD_ID=$(kubectl get pods -o wide | grep nginx-pod1 | awk '&#123;print $(NF-1)&#125;')curl http://$POD_ID # node 节点查看文件内容GLUSTERFS_POD=$(kubectl get pod | grep glusterfs | head -1 | awk '&#123;print $1&#125;')kubectl exec -ti $GLUSTERFS_POD /bin/shmount | grep heketicat /var/lib/heketi/mounts/vg_56033aa8a9131e84faa61a6f4774d8c3/brick_1ac5f3a0730457cf3fcec6d881e132a2/brick/index.html 参考文档 https://github.com/heketi/heketi/blob/master/docs/admin/install-kubernetes.md https://github.com/gluster/gluster-kubernetes/blob/master/docs/setup-guide.md https://github.com/gluster/gluster-kubernetes/blob/master/docs/examples/hello_world/README.md https://jimmysong.io/kubernetes-handbook/practice/using-heketi-gluster-for-persistent-storage.html https://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://docs.openshift.com/enterprise/3.1/architecture/additional_concepts/storage.html#pv-access-modes]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s全栈监控]]></title>
    <url>%2Fposts%2F42%2F</url>
    <content type="text"><![CDATA[简介整体概括本文章主要介绍如何全面监控k8s 使用metric-server收集数据给k8s集群内使用，如kubectl,hpa,scheduler等 使用prometheus-operator部署prometheus，存储监控数据 使用kube-state-metrics收集k8s集群内资源对象数据 使用node_exporter收集集群中各节点的数据 使用prometheus收集apiserver，scheduler，controller-manager，kubelet组件数据 使用alertmanager实现监控报警 使用grafana实现数据可视化 prometheus-operator简介prometheus-operator是一个整合prometheus和operator的项目，prometheus是一个集数据收集存储，数据查询，数据图表显示于一身的开源监控组件。operator是由coreos开源一套在k8s上管理应用的软件，通过operator可以方便的实现部署，扩容，删除应用等功能。 prometheus-operator利用k8s的CustomResourceDefinitions功能实现了只需要像写原生kubectl支持的yaml文件一样，轻松收集应用数据，配置报警规则等，包含如下CRDs ： Prometheus 用于部署Prometheus 实例 ServiceMonitor 用于配置数据收集，创建之后会根据DNS自动发现并收集数据 PrometheusRule 用于配置Prometheus 规则，处理规整数据和配置报警规则 Alertmanager 用于部署报警实例 安装环境说明 收集kube-controller-manager，kube-scheduler数据，需要配置组件监听0.0.0.0地址 二进制安装启动时添加如下参数 –address=0.0.0.0 如果使用kubeadm启动的集群，初始化时加入如下参数1234controllerManagerExtraArgs: address: 0.0.0.0schedulerExtraArgs: address: 0.0.0.0 如果是已经启动之后的集群，可以使用如下命令修改12sed -e "s/- --address=127.0.0.1/- --address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-controller-manager.yamlsed -e "s/- --address=127.0.0.1/- --address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-scheduler.yaml 收集kubelet相关数据时需要配置kubelet使用如下认证方式。使用kubeadm默认情况下已经开启 12--authentication-token-webhook=true--authorization-mode=Webhook 部署metric-server1234567891011121314151617181920212223242526# 下载mkdir k8s-monitor &amp;&amp; cd k8s-monitorgit clone https://github.com/kubernetes-incubator/metrics-server.gitcd metrics-server &amp;&amp; git checkout v0.2.1 &amp;&amp; cd ../# 修改配置（当前版本有bug）sed -ri 's@gcr.io/google_containers/metrics-server-amd64:(.*)@mirrorgooglecontainers/metrics-server-amd64:\1@g' metrics-server/deploy/1.8+/metrics-server-deployment.yamlsed -ri 's@--source=kubernetes.summary_api:.*@--source=kubernetes.summary_api:https://kubernetes.default?kubeletHttps=true\&amp;kubeletPort=10250\&amp;insecure=true@' metrics-server/deploy/1.8+/metrics-server-deployment.yaml# 部署kubectl create -f metrics-server/deploy/1.8+/# 查看状态kubectl get pods -n kube-system# 测试获取数据# 由于采集数据间隔为1分钟# 等待数分钟后查看数据NODE=$(kubectl get nodes | grep 'Ready' | head -1 | awk '&#123;print $1&#125;')METRIC_SERVER_POD=$(kubectl get pods -n kube-system | grep 'metrics-server' | awk '&#123;print $1&#125;')kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodeskubectl get --raw /apis/metrics.k8s.io/v1beta1/podskubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/$NODEkubectl top node $NODEkubectl top pod $METRIC_SERVER_POD -n kube-system 下载相关部署文件12git clone https://github.com/mgxian/k8s-monitor.gitcd k8s-monitor 部署prometheus-operator12345678910# 创建 namespacekubectl apply -f monitoring-namespace.yaml# 部署kubectl apply -f prometheus-operator.yaml# 查看kubectl get pods -n monitoringkubectl get svc -n monitoringkubectl get crd 部署k8s组件服务12345# 部署kubectl apply -f kube-k8s-service.yaml# 查看kubectl get svc -n kube-system 部署node_exporter123456# 部署kubectl apply -f node_exporter.yaml# 查看kubectl get pods -n monitoringkubectl get svc -n monitoring 部署kube-state-metrics123456# 部署kubectl apply -f kube-state-metrics.yaml# 查看kubectl get pods -n monitoringkubectl get svc -n monitoring 部署prometheus123456# 部署kubectl apply -f prometheus.yaml# 查看kubectl get pods -n monitoringkubectl get svc -n monitoring 配置数据收集12345# 部署kubectl apply -f kube-servicemonitor.yaml# 查看kubectl get servicemonitors -n monitoring 查看prometheus中的数据12345678# 查看 nodeportkubectl get svc -n monitoring | grep prometheus-k8s# 获取访问链接# 11.11.11.111 为其中一个node ipNODE_IP='11.11.11.112'PROMETHEUS_NODEPORT=$(kubectl get svc -n monitoring | grep prometheus-k8s | awk '&#123;print $(NF-1)&#125;' | cut -d ':' -f 2 | cut -d '/' -f 1)echo "http://$NODE_IP:$PROMETHEUS_NODEPORT/" prometheus主页 生成图表container_network_receive_bytes_total{namespace=”monitoring”, name=~”.prometheus.“} 查看收集数据的端点 查看数据收集服务发现 部署grafana123456789101112131415# 部署kubectl apply -f grafana.yaml# 查看kubectl get pods -n monitoringkubectl get svc -n monitoring# 查看 nodeportkubectl get svc -n monitoring | grep grafana# 获取访问链接# 11.11.11.111 为其中一个node ipNODE_IP='11.11.11.112'GRAFANA_NODEPORT=$(kubectl get svc -n monitoring | grep grafana | awk '&#123;print $(NF-1)&#125;' | cut -d ':' -f 2 | cut -d '/' -f 1)echo "http://$NODE_IP:$GRAFANA_NODEPORT/" 部署alertmanager123456789101112131415# 部署kubectl apply -f alertmanager.yaml# 查看kubectl get pods -n monitoringkubectl get svc -n monitoring# 查看 nodeportkubectl get svc -n monitoring | grep alertmanager-main# 获取访问链接# 11.11.11.111 为其中一个node ipNODE_IP='11.11.11.112'ALERTMANAGER_MAIN_NODEPORT=$(kubectl get svc -n monitoring | grep alertmanager-main | awk '&#123;print $(NF-1)&#125;' | cut -d ':' -f 2 | cut -d '/' -f 1)echo "http://$NODE_IP:$ALERTMANAGER_MAIN_NODEPORT/" 查看图表 集群状态 集群状态以命名空间视角 POD状态 参考文档 https://github.com/coreos/prometheus-operator https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/cluster-monitoring.md https://github.com/coreos/prometheus-operator/tree/master/contrib/kube-prometheus]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>monitor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重构手法]]></title>
    <url>%2Fposts%2F41%2F</url>
    <content type="text"><![CDATA[重构 改善既有代码的设计]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>重构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[helm安装配置]]></title>
    <url>%2Fposts%2F40%2F</url>
    <content type="text"><![CDATA[简介helm是kubernetes的包管理工具，用于简化部署和管理 Kubernetes 应用。用来管理charts——预先配置好的安装包资源。Helm和charts的主要作用： 应用程序封装 版本管理 依赖检查 便于应用程序分发 helm是一个C/S框架的软件，helm相当于一个客户端，tiller是一个服务端 Helm CLI 是 Helm 客户端，可以在本地执行 Tiller 是服务器端组件，在 Kubernetes 群集上运行，并管理 Kubernetes 应用程序的生命周期 Repository 是 Chart 仓库，Helm客户端通过HTTP协议来访问仓库中Chart的索引文件和压缩包 安装 安装之前需要先配置好kubeconfig，kubectl可以正常使用就表示kubeconfig已经配置正确，也可以通过环境变量KUBECONFIG指定helm使用的kubeconfig 安装helm1234567891011# 如下地址下载helm二进制文件 # 解压之后放在自己的PATH路径下# https://github.com/helm/helm/releasesmkdir -pv helm &amp;&amp; cd helmwget https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gztar xf helm-v2.9.1-linux-amd64.tar.gzsudo mv linux-amd64/helm /usr/local/binrm -rf linux-amd64# 查看版本，不显示出server版本，因为还没有安装serverhelm version 简单安装tiller(本次实验采用)1234567# 部署 tillerhelm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 \--stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts# 查看kubectl get pods -n kube-system -l app=helmkubectl get svc -n kube-system -l app=helm 安全性更高的安装tiller(tls)1234567891011helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 \--stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts \--tiller-tls \--tiller-tls-verify \--tiller-tls-cert=cert.pem \--tiller-tls-key=key.pem \--tls-ca-cert=ca.pem \--service-account=tiller# 其他命令都需要使用 tls# --tls 配置rbac123456789101112131415161718192021222324cat &gt;helm-rbac-config.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tillerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: tiller namespace: kube-systemEOFkubectl create -f helm-rbac-config.yaml# 配置tiller使用创建的ServiceAccountkubectl patch deploy --namespace kube-system tiller-deploy -p '&#123;"spec":&#123;"template":&#123;"spec":&#123;"serviceAccount":"tiller"&#125;&#125;&#125;&#125;' 查看状态12345# 查看pod启动情况kubectl get pod -n kube-system -l app=helm# 再次查看版本，显示出server版本helm version 简单使用查看可用charts123456# 更新charts列表helm repo update # 搜索可用chartshelm searchhelm search mysql 查看已经安装的charts12helm list# helm ls 安装测试 以下两种部署方式任选一种 使用命令行模式部署1234# 使用命令行模式部署helm install -n mariadb-test \--set persistence.enabled=false,mariadbRootPassword=will,mariadbUser=will,mariadbPassword=will,mariadbDatabase=will \stable/mariadb 使用配置文件定制部署1234567891011121314151617181920# 使用配置文件方式部署# 查看可配置项helm inspect values stable/mariadb# 获取所有可配置项（去行空行和注释）helm inspect values stable/mariadb | egrep -v '^\s*#|^$'# 配置cat &gt;config.yml&lt;&lt;EOFusePassword: truemariadbRootPassword: willmariadbUser: willmariadbPassword: willmariadbDatabase: willpersistence: enabled: falseEOF# 部署helm install -n mariadb-test -f config.yml stable/mariadb 测试1234567891011121314151617# 查看状态helm lskubectl get podskubectl get svc# 连接测试kubectl run mariadb-cli --image=bitnami/mariadb:10.1.28-r1 -i --tty bashmysql -hmariadb-test-mariadb -uwill -pwill willshow databases;select version();select user();# 删除 mariadb-clikubectl delete deploy mariadb-cli# 删除部署helm delete mariadb-test --purge 自定义chart创建chart1234567891011121314151617# 创建helm create hello# chart目录结构介绍hello├── charts # 本chart依赖的chart├── Chart.yaml # 描述chart的基本信息，如名称版本等├── templates # kubernetes manifest文件模板目录│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── ingress.yaml│ ├── NOTES.txt # 纯文本文件，可在其中填写chart的使用说明│ └── service.yaml└── values.yaml # chart配置的默认值# 对chart的模板和配置进行测试helm install --dry-run --debug ./ 测试安装chart1234567891011121314151617# 安装helm install -n hello-test ./# 查看helm lskubectl get podskubectl get svc# 端口转发export POD_NAME=$(kubectl get pods --namespace default -l "app=hello,release=hello-test" -o jsonpath="&#123;.items[0].metadata.name&#125;")kubectl port-forward $POD_NAME 8080:80# 访问curl http://127.0.0.1:8080# 删除helm delete hello-test --purge chart分发12# 打包为压缩包helm package ./ 参考文档 https://blog.frognew.com/2017/12/its-time-to-use-helm.html https://jimmysong.io/posts/manage-kubernetes-native-app-with-helm/ https://yq.aliyun.com/articles/159601 https://docs.helm.sh/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>helm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[istio-1.0安装测试]]></title>
    <url>%2Fposts%2F39%2F</url>
    <content type="text"><![CDATA[简介istio是一个service mesh开源实现，由Google/IBM/Lyft共同开发。架构图如下： 安装安装k8s集群参考文章 安装istioctl1234567891011121314# 本次实验时使用 vagrant 用户登录 具有 sudo 权限# vagrant 用户配置了连接管理 k8s 集群# 去下面的地址下载压缩包# https://github.com/istio/istio/releaseswget https://github.com/istio/istio/releases/download/1.0.0/istio-1.0.0-linux.tar.gztar xf istio-1.0.0-linux.tar.gz# 安装配置环境变量sudo mv istio-1.0.0 /usr/local/sudo ln -sv /usr/local/istio-1.0.0 /usr/local/istioecho 'export PATH=/usr/local/istio/bin:$PATH' | sudo tee /etc/profile.d/istio.shsource /etc/profile.d/istio.shistioctl version 在k8s集群中安装istio123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 如果环境不是云环境，不支持LoadBalancer# 作如下修改，使得 ingressgateway 监听在80和443端口# 修改使用主机端口映射# 使用此修改版本之后，每台机器只能运行单个实例# 大概在3027行左右cd /usr/local/istiosudo cp install/kubernetes/istio-demo.yaml install/kubernetes/istio-demo.yaml.orisudo vim install/kubernetes/istio-demo.yaml...apiVersion: extensions/v1beta1# kind: Deployment# 使用DaemonSet部署方式kind: DaemonSetmetadata: name: istio-ingressgateway namespace: istio-system labels: app: ingressgateway chart: gateways-1.0.0 release: RELEASE-NAME heritage: Tiller app: istio-ingressgateway istio: ingressgatewayspec: # DaemonSet不支持replicas # replicas: 1 template: metadata: labels: app: istio-ingressgateway istio: ingressgateway annotations: sidecar.istio.io/inject: "false" scheduler.alpha.kubernetes.io/critical-pod: "" spec: serviceAccountName: istio-ingressgateway-service-account containers: - name: ingressgateway image: "gcr.io/istio-release/proxyv2:1.0.0" imagePullPolicy: IfNotPresent ports: - containerPort: 80 # 主机80端口映射 hostPort: 80 - containerPort: 443 # 主机443端口映射 hostPort: 443...# 替换镜像地址sudo sed -i 's@gcr.io/istio-release@docker.io/istio@g' install/kubernetes/istio-demo.yamlsudo sed -i 's@quay.io/coreos/hyperkube:v1.7.6_coreos.0@registry.cn-shanghai.aliyuncs.com/gcr-k8s/hyperkube:v1.7.6_coreos.0@g' install/kubernetes/istio-demo.yaml# 查看镜像地址grep 'image:' install/kubernetes/istio-demo.yaml# 安装 CRDs# 等待数秒kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml -n istio-systemkubectl get crd# 安装不使用认证（不使用tls）# 如果机器内存过小会无法成功启动# 实验使用3台虚拟机每台3G内存kubectl apply -f install/kubernetes/istio-demo.yaml# 查看状态kubectl get svc -n istio-systemkubectl get pods -n istio-system 注意 istio-1.0.0 默认已经开启了自动注入功能以及其他日志监控和追踪的相关组件如 istio-tracing istio-telemetry grafana prometheus servicegraph 启用自动注入 sidecar 不开启自动注入部署应用需要使用如下方式的命令kubectl apply -f &lt;(istioctl kube-inject -f samples/bookinfo/kube/bookinfo.yaml) 开启自动注入后，使用正常命令即可部署应用kubectl apply -f samples/bookinfo/kube/bookinfo.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041# istio-1.0.0 默认已经开启了自动注入功能# k8s 1.9 及之后的版本才能使用自动注入功能# 查看是否支持kubectl api-versions | grep admissionregistration# 除了要满足以上条件外还需要检查kube-apiserver启动的参数# k8s 1.9 版本要确保 --admission-control 里有 MutatingAdmissionWebhook,ValidatingAdmissionWebhook# k8s 1.9 之后的版本要确保 --enable-admission-plugins 里有MutatingAdmissionWebhook,ValidatingAdmissionWebhook# 测试自动注入# 创建kubectl apply -f samples/sleep/sleep.yaml kubectl get deployment -o widekubectl get pod# 设置 default namespace 开启自动注入kubectl label namespace default istio-injection=enabledkubectl get namespace -L istio-injection# 删除创建的pod，等待重建kubectl delete pod $(kubectl get pod | grep sleep | cut -d ' ' -f 1)# 查看重建后的pod# 查看是否有istio-proxy容器kubectl get podkubectl describe pod $(kubectl get pod | grep sleep | cut -d ' ' -f 1)# 清理kubectl delete -f samples/sleep/sleep.yaml # 关闭自动注入kubectl label namespace default istio-injection-# 关闭部分pod的自动注入功能... template: metadata: annotations: sidecar.istio.io/inject: "false"... 部署官方测试用例12345678910111213# default开启自动注入kubectl label namespace default istio-injection=enabled# 部署 bookinfokubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml# 创建 gatewaykubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml# 查看状态kubectl get serviceskubectl get podsistioctl get gateway 访问测试123456789101112131415161718# 命令行访问测试export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='&#123;.spec.ports[?(@.name=="http2")].nodePort&#125;')NODE_NAME=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')NODE_IP=$(ping -c 1 $NODE_NAME | grep PING | awk '&#123;print $3&#125;' | tr -d '()')export GATEWAY_URL=$NODE_IP:$INGRESS_PORTecho $GATEWAY_URLcurl -o /dev/null -s -w "%&#123;http_code&#125;\n" http://$&#123;GATEWAY_URL&#125;/productpage# 浏览器访问测试echo "http://$&#123;GATEWAY_URL&#125;/productpage"# 使用daemonset方式部署可以使用如下方式访问# 11.11.11.112为其中一个node节点的ipcurl http://11.11.11.112/productpage# 清理samples/bookinfo/platform/kube/cleanup.sh 清理12345# 清理istiokubectl delete -f install/kubernetes/helm/istio/templates/crds.yaml -n istio-systemkubectl delete -f install/kubernetes/istio-demo.yaml# kubectl delete -f install/kubernetes/istio-demo-auth.yaml 使用helm安装istio安装helm参考文章 安装istio123456789101112131415161718192021222324252627# 查看配置cd /usr/local/istioegrep -v "^$|#" install/kubernetes/helm/istio/values.yaml# 安装 CRDskubectl apply -f install/kubernetes/helm/istio/templates/crds.yamlkubectl get crd# 根据上面查看的配置和需求配置相关参数# 部署helm install install/kubernetes/helm/istio --name istio --namespace istio-system \--set ingress.enabled=false \--set global.hub="docker.io/istio" \--set global.hyperkube.hub="registry.cn-shanghai.aliyuncs.com/gcr-k8s" \--set gateways.istio-ingressgateway.type=NodePort \--set gateways.istio-egressgateway.type=NodePort# 查看helm lskubectl get pods -n istio-systemkubectl get svc -n istio-system# 运行之前的测试# 清理helm delete --purge istiokubectl delete -f install/kubernetes/helm/istio/templates/crds.yaml -n istio-system 参考文档 https://istio.io/docs/setup/kubernetes/quick-start.html https://istio.io/docs/guides/bookinfo.html https://istio.io/docs/setup/kubernetes/sidecar-injection.html#automatic-sidecar-injection]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>istio</tag>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>servicemesh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s使用kube-router构建高可用可扩展ingress]]></title>
    <url>%2Fposts%2F38%2F</url>
    <content type="text"><![CDATA[简介使用kube-router实现k8s集群的ingress功能，高可用易扩展 环境说明本实验在已经安装配置好k8s集群基础之上进行实验，k8s安装参考博客其他文章。lab4作为一个路由器，转发lab5的请求 实验架构12345lab1: master 11.11.11.111lab2: node 11.11.11.112lab3: node 11.11.11.113lab4: router 11.11.11.114lab5: client 11.11.11.115 安装12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 本次实验重新创建了集群，使用之前测试其他网络插件的集群环境没有成功# 可能是由于环境干扰，实验时需要注意# 创建kube-router目录下载相关文件mkdir kube-router &amp;&amp; cd kube-routerrm -f generic-kuberouter-all-features-dsr.yamlwget https://raw.githubusercontent.com/mgxian/kube-router/master/generic-kuberouter-all-features-dsr.yaml# 启用pod网络通信，网络隔离策略，服务代理所有功能# CLUSTERCIDR kube-controller-manager 启动参数 --cluster-cidr 的值# APISERVER kube-apiserver 启动参数 --advertise-address 值CLUSTERCIDR='10.244.0.0/16'APISERVER='https://11.11.11.111:6443'sed -i "s;%APISERVER%;$APISERVER;g" generic-kuberouter-all-features-dsr.yamlsed -i "s;%CLUSTERCIDR%;$CLUSTERCIDR;g" generic-kuberouter-all-features-dsr.yaml# 修改配置 containers: - name: kube-router image: cloudnativelabs/kube-router imagePullPolicy: Always args: ... - --peer-router-ips=11.11.11.114 - --peer-router-asns=64513 - --cluster-asn=64512 - --advertise-external-ip=true ...# 部署kubectl apply -f generic-kuberouter-all-features-dsr.yaml# 删除kube-proxykubectl -n kube-system delete ds kube-proxy# 在每个节点上执行# 如果是二进制安装使用如下命令systemctl stop kube-proxy# 在每个节点上执行# 清理kube-proxy留下的规则docker run --privileged --net=host registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.10.2 kube-proxy --cleanup# 查看kubectl get pods -n kube-systemkubectl get svc -n kube-system 测试12345678910111213141516# 测试之前请先安装配置好kube-dns或者coredns# 启动用于测试的deploymentkubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service# 查看kubectl get pods -o widekubectl get svc -o wide# dns及访问测试kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service 在lab4配置quagga123456789101112131415161718192021222324252627282930# 安装yum install -y quagga# 配置cat &gt;/etc/quagga/bgpd.conf&lt;&lt;EOF! -*- bgp -*-!! BGPd sample configuratin file!! $Id: bgpd.conf.sample,v 1.1 2002/12/13 20:15:29 paul Exp $!hostname lab4password password!router bgp 64513 bgp router-id 11.11.11.114 maximum-paths 4 neighbor 11.11.11.111 remote-as 64512 neighbor 11.11.11.112 remote-as 64512 neighbor 11.11.11.113 remote-as 64512log stdoutEOF# 启动systemctl start bgpdsystemctl status bgpdsystemctl enable bgpd# 查看路由信息ip route 在lab4测试12345678910111213141516171819# 在lab1上修改 example-service 配置 external ipkubectl edit svc example-service...spec: clusterIP: 10.111.34.147 externalIPs: - 11.11.111.111...# 在lab1上查看svc信息# 可以看到 example-service 有了 external ipkubectl get svc# 查看lab4路由# 可以看到有 11.11.111.111 相关的路由ip route# 在lab4上访问测试curl 11.11.111.111 在lab5测试123456789# 在lab5添加路由ip route add 11.11.111.111 via 11.11.11.114ip route# 在lab5访问测试curl 11.11.111.111# 在lab1查看ipvsipvsadm -L -n 使用DSR1234567891011121314# DSR实验没有成功，实验环境是vagrant配合virtualbox# 在lab1设置 example-service 使用 DSR 模式# 服务的响应直接发送到客户端不经过lvs中转kubectl annotate svc example-service "kube-router.io/service.dsr=tunnel"# 在lab1查看ipvs# 可以看到 Tunnel 转发类型ipvsadm -L -n# 在lab5访问测试curl 11.11.111.111# 在集群中的节点抓包分析tcpdump -i kube-bridge proto 4 清理123# 清理kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 参考文档 https://cloudnativelabs.github.io/post/2017-11-01-kube-high-available-ingress/ https://github.com/cloudnativelabs/kube-router/blob/master/docs/generic.md]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s使用kube-router暴露集群中的pod和svc到外部]]></title>
    <url>%2Fposts%2F37%2F</url>
    <content type="text"><![CDATA[简介使用kube-router把k8s集群中的pod ip和cluter ip暴露集群外部，实现集群外的节点直接访问k8s的pod和svc 环境说明本实验在已经安装配置好k8s集群基础之上进行实验，k8s安装参考博客其他文章。 实验架构1234lab1: master 11.11.11.111lab2: node 11.11.11.112lab3: node 11.11.11.113lab4: external 11.11.11.114 安装12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 本次实验重新创建了集群，使用之前测试其他网络插件的集群环境没有成功# 可能是由于环境干扰，实验时需要注意# 创建kube-router目录下载相关文件mkdir kube-router &amp;&amp; cd kube-routerrm -f generic-kuberouter-all-features.yamlwget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter-all-features-advertise-routes.yaml# 启用pod网络通信，网络隔离策略，服务代理所有功能# CLUSTERCIDR kube-controller-manager 启动参数 --cluster-cidr 的值# APISERVER kube-apiserver 启动参数 --advertise-address 值CLUSTERCIDR='10.244.0.0/16'APISERVER='https://11.11.11.111:6443'sed -i "s;%APISERVER%;$APISERVER;g" generic-kuberouter-all-features-advertise-routes.yamlsed -i "s;%CLUSTERCIDR%;$CLUSTERCIDR;g" generic-kuberouter-all-features-advertise-routes.yaml# 修改配置 containers: - name: kube-router image: cloudnativelabs/kube-router imagePullPolicy: Always args: ... - "--peer-router-ips=11.11.11.114" - "--peer-router-asns=64513" - "--cluster-asn=64512" - "--advertise-cluster-ip=true" ...# 部署kubectl apply -f generic-kuberouter-all-features-advertise-routes.yaml# 删除kube-proxykubectl -n kube-system delete ds kube-proxy# 在每个节点上执行# 如果是二进制安装使用如下命令systemctl stop kube-proxy# 在每个节点上执行# 清理kube-proxy留下的规则docker run --privileged --net=host registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.10.2 kube-proxy --cleanup# 查看kubectl get pods -n kube-systemkubectl get svc -n kube-system 测试12345678910111213141516# 测试之前请先安装配置好kube-dns或者coredns# 启动用于测试的deploymentkubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service# 查看kubectl get pods -o widekubectl get svc -o wide# dns及访问测试kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service 在lab4配置quagga1234567891011121314151617181920212223242526272829# 安装yum install -y quagga# 配置cat &gt;/etc/quagga/bgpd.conf&lt;&lt;EOF! -*- bgp -*-!! BGPd sample configuratin file!! $Id: bgpd.conf.sample,v 1.1 2002/12/13 20:15:29 paul Exp $!hostname lab4password zebra!router bgp 64513 bgp router-id 11.11.11.114 neighbor 11.11.11.111 remote-as 64512 neighbor 11.11.11.112 remote-as 64512 neighbor 11.11.11.113 remote-as 64512log stdoutEOF# 启动systemctl start bgpdsystemctl status bgpdsystemctl enable bgpd# 查看路由信息ip route 在lab4测试访问k8s集群中的pod和svc123456789# 在lab1上获取pod和svc信息kubectl get pods -o widekubectl get svc# 在lab4上访问# 10.244.2.11 其中一个 nginx pod 的ip# 10.106.123.190 为 example-service 的 cluster ipcurl 10.244.2.11curl 10.106.123.190 清理123# 清理kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 参考文档 https://cloudnativelabs.github.io/post/2017-05-22-kube-pod-networking/ https://github.com/cloudnativelabs/kube-router/blob/master/docs/generic.md]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s使用kube-router网络组件并实现网络隔离]]></title>
    <url>%2Fposts%2F36%2F</url>
    <content type="text"><![CDATA[简介本文章主要介绍k8s如何使用kube-router实现pod通信，服务代理，网络策略隔离等功能 kube-router是一个新的k8s的网络插件，使用lvs做服务的代理及负载均衡，使用iptables来做网络的隔离策略。部署简单，只需要在每个节点部署一个daemonset即可，高性能，易维护。支持pod间通信，以及服务的代理。 环境说明本实验在已经安装配置好k8s集群基础之上进行实验，k8s安装参考博客其他文章。 实验架构123lab1: master 11.11.11.111lab2: node 11.11.11.112lab3: node 11.11.11.113 安装12345678910111213141516171819202122232425262728293031# 本次实验重新创建了集群，使用之前测试其他网络插件的集群环境没有成功# 可能是由于环境干扰，实验时需要注意# 创建kube-router目录下载相关文件mkdir kube-router &amp;&amp; cd kube-routerrm -f generic-kuberouter-all-features.yamlwget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter-all-features.yaml# 启用pod网络通信，网络隔离策略，服务代理所有功能# CLUSTERCIDR kube-controller-manager 启动参数 --cluster-cidr 的值# APISERVER kube-apiserver 启动参数 --advertise-address 值CLUSTERCIDR='10.244.0.0/16'APISERVER='https://11.11.11.111:6443'sed -i "s;%APISERVER%;$APISERVER;g" generic-kuberouter-all-features.yamlsed -i "s;%CLUSTERCIDR%;$CLUSTERCIDR;g" generic-kuberouter-all-features.yamlkubectl apply -f generic-kuberouter-all-features.yaml# 删除kube-proxykubectl -n kube-system delete ds kube-proxy# 在每个节点上执行# 如果是二进制安装使用如下命令systemctl stop kube-proxy# 在每个节点上执行# 清理kube-proxy留下的规则docker run --privileged --net=host registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.10.2 kube-proxy --cleanup# 查看kubectl get pods -n kube-systemkubectl get svc -n kube-system 测试1234567891011121314151617181920# 测试之前请先安装配置好kube-dns或者coredns# 启动用于测试的deploymentkubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service# 查看kubectl get pods -o widekubectl get svc -o wide# dns及访问测试kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service# 清理kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 网络隔离策略部署应用123456789101112131415# 创建 production staging 命名空间kubectl create namespace productionkubectl create namespace staging# 在每个命名空间各部署一套服务cd kube-routerwget https://raw.githubusercontent.com/mgxian/istio-test/master/service/node/v1/node-v1.ymlwget https://raw.githubusercontent.com/mgxian/istio-test/master/service/go/v1/go-v1.ymlkubectl apply -f node-v1.yml -n productionkubectl apply -f go-v1.yml -n productionkubectl apply -f node-v1.yml -n stagingkubectl apply -f go-v1.yml -n staging# 查看状态kubectl get pods --all-namespaces -o wide 测试pod通信123456789101112131415161718# 获取相关POD信息PRODUCTION_NODE_NAME=$(kubectl get pods -n production | grep Running | grep service-node | awk '&#123;print $1&#125;')STAGING_NODE_NAME=$(kubectl get pods -n staging | grep Running | grep service-node | awk '&#123;print $1&#125;')PRODUCTION_GO_IP=$(kubectl get pods -n production -o wide | grep Running | grep service-go | awk '&#123;print $6&#125;')STAGING_GO_IP=$(kubectl get pods -n staging -o wide | grep Running | grep service-go | awk '&#123;print $6&#125;')echo $PRODUCTION_NODE_NAME $PRODUCTION_GO_IPecho $STAGING_NODE_NAME $STAGING_GO_IP# 同namespace的pod通信kubectl exec -it $PRODUCTION_NODE_NAME --namespace=production -- ping -c4 $PRODUCTION_GO_IP kubectl exec -it $STAGING_NODE_NAME --namespace=staging -- ping -c4 $STAGING_GO_IP # 不同namespace的pod通信kubectl exec -it $PRODUCTION_NODE_NAME --namespace=production -- ping -c4 $STAGING_GO_IPkubectl exec -it $STAGING_NODE_NAME --namespace=staging -- ping -c4 $PRODUCTION_GO_IP# 结论：任何namespace的任何pod间都可以直接通信 设置默认策略测试123456789101112131415161718192021222324# 设置默认策略为拒绝所有流量cat &gt;default-deny.yml&lt;&lt;EOFapiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: default-denyspec: podSelector: &#123;&#125; policyTypes: - IngressEOFkubectl apply -f default-deny.yml -n productionkubectl apply -f default-deny.yml -n staging# 测试通信# 同namespace的pod通信kubectl exec -it $PRODUCTION_NODE_NAME --namespace=production -- ping -c4 $PRODUCTION_GO_IP kubectl exec -it $STAGING_NODE_NAME --namespace=staging -- ping -c4 $STAGING_GO_IP # 不同namespace的pod通信kubectl exec -it $PRODUCTION_NODE_NAME --namespace=production -- ping -c4 $STAGING_GO_IPkubectl exec -it $STAGING_NODE_NAME --namespace=staging -- ping -c4 $PRODUCTION_GO_IP# 结论：所有pod间都不能通信 设置允许规则1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 设置 service-go 允许从 service-node 访问cat &gt;service-go-allow-service-node.yml&lt;&lt;EOFapiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: service-go-allow-service-nodespec: podSelector: matchLabels: app: service-go ingress: - from: - podSelector: matchLabels: app: service-nodeEOFkubectl apply -f service-go-allow-service-node.yml -n productionkubectl apply -f service-go-allow-service-node.yml -n staging# 设置 service-node 允许 访问 tcp 80 端口cat &gt;service-node-allow-tcp-80.yml&lt;&lt;EOFapiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: service-node-allow-tcp-80spec: podSelector: matchLabels: app: service-node ingress: - from: ports: - protocol: TCP port: 80EOFkubectl apply -f service-node-allow-tcp-80.yml -n productionkubectl apply -f service-node-allow-tcp-80.yml -n staging# 测试通信# 同namespace的pod通信kubectl exec -it $PRODUCTION_NODE_NAME --namespace=production -- ping -c4 $PRODUCTION_GO_IP kubectl exec -it $STAGING_NODE_NAME --namespace=staging -- ping -c4 $STAGING_GO_IP # 不同namespace的pod通信kubectl exec -it $PRODUCTION_NODE_NAME --namespace=production -- ping -c4 $STAGING_GO_IPkubectl exec -it $STAGING_NODE_NAME --namespace=staging -- ping -c4 $PRODUCTION_GO_IP# 通过service测试PRODUCTION_GO_SVC=$(kubectl get svc -n production | grep service-go | awk '&#123;print $3&#125;')STAGING_GO_SVC=$(kubectl get svc -n staging | grep service-go | awk '&#123;print $3&#125;')echo $PRODUCTION_GO_SVC $STAGING_GO_SVCcurl $PRODUCTION_GO_SVCcurl $STAGING_GO_SVC# 结论：同一namespace的pod间可以通信，不同namespace的pod间不可以通信，只允许配置了网络规则的pod间通信# 通过 service 也无法绕过网络隔离策略 清理123# 删除 namespace 自动删除相关资源kubectl delete ns productionkubectl delete ns staging 参考文档 https://github.com/cloudnativelabs/kube-router/blob/master/docs/generic.md https://kubernetes.io/docs/concepts/services-networking/network-policies/ https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7手动安装k8s-1.11版本]]></title>
    <url>%2Fposts%2F35%2F</url>
    <content type="text"><![CDATA[简介本文章主要介绍如何通过使用官方提供的二进制包安装配置k8s集群 实验环境说明实验架构123lab1: master 11.11.11.111lab2: node 11.11.11.112lab3: node 11.11.11.113 实验使用的Vagrantfile123456789101112131415161718192021# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..3).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "2048"] end end endend 安装关闭防火墙12systemctl stop firewalldsystemctl disable firewalld 配置系统相关参数 如下操作在所有节点操作 1234567891011121314151617181920212223242526272829303132333435363738394041# 临时禁用selinux# 永久关闭 修改/etc/sysconfig/selinux文件设置sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinuxsetenforce 0# 临时关闭swap# 永久关闭 注释/etc/fstab文件里swap相关的行swapoff -a# 开启forward# Docker从1.13版本开始调整了默认的防火墙规则# 禁用了iptables filter表中FOWARD链# 这样会引起Kubernetes集群中跨Node的Pod无法通信iptables -P FORWARD ACCEPT# 配置转发相关参数，否则可能会出错cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1vm.swappiness=0EOFsysctl --system# 加载ipvs相关内核模块# 如果重新开机，需要重新加载modprobe ip_vsmodprobe ip_vs_rrmodprobe ip_vs_wrrmodprobe ip_vs_shmodprobe nf_conntrack_ipv4lsmod | grep ip_vs# 配置开启自加载cat &gt;/etc/modules-load.d/k8s-ipvs.conf&lt;&lt;EOFip_vsip_vs_rrip_vs_wrrip_vs_shnf_conntrack_ipv4EOF 配置hosts解析 如下操作在所有节点操作 12345cat &gt;&gt;/etc/hosts&lt;&lt;EOF11.11.11.111 lab111.11.11.112 lab211.11.11.113 lab3EOF 安装配置docker v1.11.0版本推荐使用docker v17.03,v1.11,v1.12,v1.13, 也可以使用，再高版本的docker可能无法正常使用。测试发现17.09无法正常使用，不能使用资源限制(内存CPU) 如下操作在所有节点操作 安装docker12345# 卸载安装指定版本docker-ceyum remove -y docker-ce docker-ce-selinux container-selinuxyum install -y --setopt=obsoletes=0 \docker-ce-17.03.1.ce-1.el7.centos \docker-ce-selinux-17.03.1.ce-1.el7.centos 启动docker1systemctl enable docker &amp;&amp; systemctl restart docker 安装CFSSL 只在lab1节点操作 1234567891011121314# 下载# 百度云链接：https://pan.baidu.com/s/1kgV40nwHy1IKnnLD6zH4cQ 密码：alyjmkdir -pv /server/software/k8scd /server/software/k8syum install -y wgetwget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64# 安装mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfomv cfssl_linux-amd64 /usr/local/bin/cfsslmv cfssljson_linux-amd64 /usr/local/bin/cfssljsonchmod +x /usr/local/bin/cfssl* 配置CA 只在lab1节点操作 此处的CA配置，后面配置etcd和k8s时都需要使用 123456789101112131415161718192021mkdir -pv $HOME/ssl &amp;&amp; cd $HOME/sslcat &gt;ca-config.json&lt;&lt;EOF&#123; "signing": &#123; "default": &#123; "expiry": "87600h" &#125;, "profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "87600h" &#125; &#125; &#125;&#125;EOF 配置etcd集群生成etcd-ca 只在lab1节点操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 写入配置cat &gt;etcd-ca-csr.json&lt;&lt;EOF&#123; "CN": "etcd", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "etcd", "OU": "Etcd Security" &#125; ]&#125;EOF# 生成 etcd root cacfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-cacat &gt;etcd-csr.json&lt;&lt;EOF&#123; "CN": "etcd", "hosts": [ "127.0.0.1", "11.11.11.111", "11.11.11.112", "11.11.11.113" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "etcd", "OU": "Etcd Security" &#125; ]&#125;EOF# 生成 etcd cacfssl gencert -ca=etcd-ca.pem -ca-key=etcd-ca-key.pem -config=ca-config.json \-profile=kubernetes etcd-csr.json | cfssljson -bare etcdmkdir -pv /etc/etcd/sslcp etcd*.pem /etc/etcd/sslls /etc/etcd/ssl/etcd*.pem# 复制到其他节点cd /etc/etcd &amp;&amp; tar cvzf etcd-ssl.tgz ssl/scp /etc/etcd/etcd-ssl.tgz lab2:~/scp /etc/etcd/etcd-ssl.tgz lab3:~/ssh lab2 'mkdir -pv /etc/etcd &amp;&amp; tar xf etcd-ssl.tgz -C /etc/etcd &amp;&amp; ls -l /etc/etcd/ssl'ssh lab3 'mkdir -pv /etc/etcd &amp;&amp; tar xf etcd-ssl.tgz -C /etc/etcd &amp;&amp; ls -l /etc/etcd/ssl' 安装启动etcd 如下操作在所有节点操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 安装# 百度云链接：https://pan.baidu.com/s/1IVHyMqiJrlq9gmbF49Ly3Q 密码：w5nxmkdir -pv /server/software/k8scd /server/software/k8syum install -y wgetwget https://github.com/coreos/etcd/releases/download/v3.2.18/etcd-v3.2.18-linux-amd64.tar.gztar xf etcd-v3.2.18-linux-amd64.tar.gzmv etcd-v3.2.18-linux-amd64 /usr/local/etcd-v3.2.18ln -sv /usr/local/etcd-v3.2.18 /usr/local/etcdcd /usr/local/etcd &amp;&amp; mkdir bin &amp;&amp; mv etcd etcdctl bin/usr/local/etcd/bin/etcd --versioncd $HOME# 配置启动脚本export ETCD_NAME=$(hostname)export INTERNAL_IP=$(hostname -i | awk '&#123;print $NF&#125;')export ECTD_CLUSTER='lab1=https://11.11.11.111:2380,lab2=https://11.11.11.112:2380,lab3=https://11.11.11.113:2380'mkdir -pv /data/etcdcat &gt; /etc/systemd/system/etcd.service &lt;&lt;EOF[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/data/etcdEnvironmentFile=-/etc/etcd/etcd.confExecStart=/usr/local/etcd/bin/etcd \\ --name $&#123;ETCD_NAME&#125; \\ --cert-file=/etc/etcd/ssl/etcd.pem \\ --key-file=/etc/etcd/ssl/etcd-key.pem \\ --peer-cert-file=/etc/etcd/ssl/etcd.pem \\ --peer-key-file=/etc/etcd/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/etcd/ssl/etcd-ca.pem \\ --peer-trusted-ca-file=/etc/etcd/ssl/etcd-ca.pem \\ --initial-advertise-peer-urls https://$&#123;INTERNAL_IP&#125;:2380 \\ --listen-peer-urls https://$&#123;INTERNAL_IP&#125;:2380 \\ --listen-client-urls https://$&#123;INTERNAL_IP&#125;:2379,https://127.0.0.1:2379 \\ --advertise-client-urls https://$&#123;INTERNAL_IP&#125;:2379 \\ --initial-cluster-token my-etcd-token \\ --initial-cluster $ECTD_CLUSTER \\ --initial-cluster-state new \\ --data-dir=/data/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF# 启动并设置开机启动systemctl daemon-reloadsystemctl start etcdsystemctl enable etcd 查看etcd集群状态12345/usr/local/etcd/bin/etcdctl --endpoints "https://127.0.0.1:2379" \ --ca-file=/etc/etcd/ssl/etcd-ca.pem \ --cert-file=/etc/etcd/ssl/etcd.pem \ --key-file=/etc/etcd/ssl/etcd-key.pem \ cluster-health 生成k8s集群的CA123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189# 进入相关目录cd $HOME/ssl# 配置 root cacat &gt;ca-csr.json&lt;&lt;EOF&#123; "CN": "kubernetes", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ], "ca": &#123; "expiry": "87600h" &#125;&#125;EOF# 生成 root cacfssl gencert -initca ca-csr.json | cfssljson -bare cals ca*.pem# 配置 kube-apiserver ca# 10.96.0.1 是 kube-apiserver 指定的 service-cluster-ip-range 网段的第一个IPcat &gt;kube-apiserver-csr.json&lt;&lt;EOF&#123; "CN": "kube-apiserver", "hosts": [ "127.0.0.1", "11.11.11.111", "11.11.11.112", "11.11.11.113", "10.96.0.1", "kubernetes", "kubernetes.default", "kubernetes.default.svc", "kubernetes.default.svc.cluster", "kubernetes.default.svc.cluster.local" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOF# 生成 kube-apiserver cacfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserverls kube-apiserver*.pem# 配置 kube-controller-manager cacat &gt;kube-controller-manager-csr.json&lt;&lt;EOF&#123; "CN": "system:kube-controller-manager", "hosts": [ "127.0.0.1", "11.11.11.111", "11.11.11.112", "11.11.11.113" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "system:kube-controller-manager", "OU": "System" &#125; ]&#125;EOF# 生成 kube-controller-manager cacfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-managerls kube-controller-manager*.pem# 配置 kube-scheduler cacat &gt;kube-scheduler-csr.json&lt;&lt;EOF&#123; "CN": "system:kube-scheduler", "hosts": [ "127.0.0.1", "11.11.11.111", "11.11.11.112", "11.11.11.113" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "system:kube-scheduler", "OU": "System" &#125; ]&#125;EOF# 生成 kube-scheduler cacfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-schedulerls kube-scheduler*.pem# 配置 kube-proxy cacat &gt;kube-proxy-csr.json&lt;&lt;EOF&#123; "CN": "system:kube-proxy", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "system:kube-proxy", "OU": "System" &#125; ]&#125;EOF# 生成 kube-proxy cacfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxyls kube-proxy*.pem# 配置 admin cacat &gt;admin-csr.json&lt;&lt;EOF&#123; "CN": "admin", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "system:masters", "OU": "System" &#125; ]&#125;EOF# 生成 admin cacfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes admin-csr.json | cfssljson -bare adminls admin*.pem# 复制生成的camkdir -pv /etc/kubernetes/pkicp ca*.pem admin*.pem kube-proxy*.pem kube-scheduler*.pem kube-controller-manager*.pem kube-apiserver*.pem /etc/kubernetes/pkicd /etc/kubernetes &amp;&amp; tar cvzf pki.tgz pki/scp /etc/kubernetes/pki.tgz lab2:~/scp /etc/kubernetes/pki.tgz lab3:~/ssh lab2 'mkdir -pv /etc/kubernetes &amp;&amp; tar xf pki.tgz -C /etc/kubernetes &amp;&amp; ls -l /etc/kubernetes/pki'ssh lab3 'mkdir -pv /etc/kubernetes &amp;&amp; tar xf pki.tgz -C /etc/kubernetes &amp;&amp; ls -l /etc/kubernetes/pki'cd $HOME 安装k8s文件12345678910111213# 下载文件# 需要翻墙，如果不能翻墙使用如下链接下载# 链接：https://pan.baidu.com/s/1OI9Q4BRp7jNJUmsA8IAkbA 密码：tnx5cd /server/software/k8swget https://dl.k8s.io/v1.11.0/kubernetes-server-linux-amd64.tar.gztar xf kubernetes-server-linux-amd64.tar.gzcd kubernetes/server/binmkdir -pv /usr/local/kubernetes-v1.11.0/bincp kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet kubectl /usr/local/kubernetes-v1.11.0/binln -sv /usr/local/kubernetes-v1.11.0 /usr/local/kubernetescp /usr/local/kubernetes/bin/kubectl /usr/local/bin/kubectlkubectl versioncd $HOME 生成kubeconfig123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899# 使用 TLS Bootstrapping export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')cat &gt; /etc/kubernetes/token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,"system:kubelet-bootstrap"EOF# 创建 kubelet bootstrapping kubeconfigcd /etc/kubernetesexport KUBE_APISERVER="https://11.11.11.111:6443"kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/pki/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kubelet-bootstrap.confkubectl config set-credentials kubelet-bootstrap \ --token=$&#123;BOOTSTRAP_TOKEN&#125; \ --kubeconfig=kubelet-bootstrap.confkubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=kubelet-bootstrap.confkubectl config use-context default --kubeconfig=kubelet-bootstrap.conf# 创建 kube-controller-manager kubeconfigexport KUBE_APISERVER="https://11.11.11.111:6443"kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/pki/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-controller-manager.confkubectl config set-credentials kube-controller-manager \ --client-certificate=/etc/kubernetes/pki/kube-controller-manager.pem \ --client-key=/etc/kubernetes/pki/kube-controller-manager-key.pem \ --embed-certs=true \ --kubeconfig=kube-controller-manager.confkubectl config set-context default \ --cluster=kubernetes \ --user=kube-controller-manager \ --kubeconfig=kube-controller-manager.confkubectl config use-context default --kubeconfig=kube-controller-manager.conf# 创建 kube-scheduler kubeconfigexport KUBE_APISERVER="https://11.11.11.111:6443"kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/pki/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-scheduler.confkubectl config set-credentials kube-scheduler \ --client-certificate=/etc/kubernetes/pki/kube-scheduler.pem \ --client-key=/etc/kubernetes/pki/kube-scheduler-key.pem \ --embed-certs=true \ --kubeconfig=kube-scheduler.confkubectl config set-context default \ --cluster=kubernetes \ --user=kube-scheduler \ --kubeconfig=kube-scheduler.confkubectl config use-context default --kubeconfig=kube-scheduler.conf# 创建 kube-proxy kubeconfigexport KUBE_APISERVER="https://11.11.11.111:6443"kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/pki/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-proxy.confkubectl config set-credentials kube-proxy \ --client-certificate=/etc/kubernetes/pki/kube-proxy.pem \ --client-key=/etc/kubernetes/pki/kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.confkubectl config set-context default \ --cluster=kubernetes \ --user=kube-proxy \ --kubeconfig=kube-proxy.confkubectl config use-context default --kubeconfig=kube-proxy.conf# 创建 admin kubeconfigexport KUBE_APISERVER="https://11.11.11.111:6443"kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/pki/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=admin.confkubectl config set-credentials admin \ --client-certificate=/etc/kubernetes/pki/admin.pem \ --client-key=/etc/kubernetes/pki/admin-key.pem \ --embed-certs=true \ --kubeconfig=admin.confkubectl config set-context default \ --cluster=kubernetes \ --user=admin \ --kubeconfig=admin.confkubectl config use-context default --kubeconfig=admin.conf# 把 kube-proxy.conf 复制到其他节点scp kubelet-bootstrap.conf kube-proxy.conf lab2:/etc/kubernetesscp kubelet-bootstrap.conf kube-proxy.conf lab3:/etc/kubernetescd $HOME 配置master相关组件 只在lab1节点操作 配置启动kube-apiserver123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 复制 etcd camkdir -pv /etc/kubernetes/pki/etcdcd /etc/etcd/sslcp etcd-ca.pem etcd-key.pem etcd.pem /etc/kubernetes/pki/etcd# 生成 service account keyopenssl genrsa -out /etc/kubernetes/pki/sa.key 2048openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.publs /etc/kubernetes/pki/sa.*cd $HOME# 启动文件cat &gt;/etc/systemd/system/kube-apiserver.service&lt;&lt;EOF[Unit]Description=Kubernetes API ServiceDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/apiserverExecStart=/usr/local/kubernetes/bin/kube-apiserver \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBE_ETCD_ARGS \\ \$KUBE_API_ADDRESS \\ \$KUBE_SERVICE_ADDRESSES \\ \$KUBE_ADMISSION_CONTROL \\ \$KUBE_APISERVER_ARGSRestart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF# 该配置文件同时被 kube-apiserver, kube-controller-manager# kube-scheduler, kubelet, kube-proxy 使用cat &gt;/etc/kubernetes/config&lt;&lt;EOFKUBE_LOGTOSTDERR="--logtostderr=true"KUBE_LOG_LEVEL="--v=2"EOFcat &gt;/etc/kubernetes/apiserver&lt;&lt;EOFKUBE_API_ADDRESS="--advertise-address=11.11.11.111"KUBE_ETCD_ARGS="--etcd-servers=https://11.11.11.111:2379,https://11.11.11.112:2379,https://11.11.11.113:2379 --etcd-cafile=/etc/kubernetes/pki/etcd/etcd-ca.pem --etcd-certfile=/etc/kubernetes/pki/etcd/etcd.pem --etcd-keyfile=/etc/kubernetes/pki/etcd/etcd-key.pem"KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.96.0.0/12"KUBE_ADMISSION_CONTROL="--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"KUBE_APISERVER_ARGS="--allow-privileged=true --authorization-mode=Node,RBAC --enable-bootstrap-token-auth=true --token-auth-file=/etc/kubernetes/token.csv --service-node-port-range=30000-32767 --tls-cert-file=/etc/kubernetes/pki/kube-apiserver.pem --tls-private-key-file=/etc/kubernetes/pki/kube-apiserver-key.pem --client-ca-file=/etc/kubernetes/pki/ca.pem --service-account-key-file=/etc/kubernetes/pki/sa.pub --enable-swagger-ui=true --secure-port=6443 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --anonymous-auth=false --kubelet-client-certificate=/etc/kubernetes/pki/admin.pem --kubelet-client-key=/etc/kubernetes/pki/admin-key.pem"EOF# 启动systemctl daemon-reloadsystemctl enable kube-apiserversystemctl start kube-apiserversystemctl status kube-apiserver# 浏览器访问测试https://11.11.11.111:6443/swaggerapi 配置启动kube-controller-manager123456789101112131415161718192021222324252627282930# 启动文件cat &gt;/etc/systemd/system/kube-controller-manager.service&lt;&lt;EOFDescription=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/controller-managerExecStart=/usr/local/kubernetes/bin/kube-controller-manager \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBECONFIG \\ \$KUBE_CONTROLLER_MANAGER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFcat &gt;/etc/kubernetes/controller-manager&lt;&lt;EOFKUBECONFIG="--kubeconfig=/etc/kubernetes/kube-controller-manager.conf"KUBE_CONTROLLER_MANAGER_ARGS="--address=127.0.0.1 --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem --service-account-private-key-file=/etc/kubernetes/pki/sa.key --root-ca-file=/etc/kubernetes/pki/ca.pem --leader-elect=true --use-service-account-credentials=true --node-monitor-grace-period=10s --pod-eviction-timeout=10s --allocate-node-cidrs=true --controllers=*,bootstrapsigner,tokencleaner"EOF# 启动systemctl daemon-reloadsystemctl enable kube-controller-managersystemctl start kube-controller-managersystemctl status kube-controller-manager 配置启动kube-scheduler123456789101112131415161718192021222324252627282930cat &gt;/etc/systemd/system/kube-scheduler.service&lt;&lt;EOF[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/schedulerExecStart=/usr/local/kubernetes/bin/kube-scheduler \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBECONFIG \\ \$KUBE_SCHEDULER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFcat &gt;/etc/kubernetes/scheduler&lt;&lt;EOFKUBECONFIG="--kubeconfig=/etc/kubernetes/kube-scheduler.conf"KUBE_SCHEDULER_ARGS="--leader-elect=true --address=127.0.0.1"EOF# 启动systemctl daemon-reloadsystemctl enable kube-schedulersystemctl start kube-schedulersystemctl status kube-scheduler 配置kubectl使用12345rm -rf $HOME/.kubemkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/configkubectl get no 查看组件状态1kubectl get componentstatuses 配置kubelet使用bootstrap1234# 将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色kubectl create clusterrolebinding kubelet-bootstrap \--clusterrole=system:node-bootstrapper \--user=kubelet-bootstrap 配置node相关组件 如下操作在所有节点操作 安装cni12345678# 安装 cni# 百度云链接：https://pan.baidu.com/s/1-PputObLs5jouXLnuBCI6Q 密码：tzqmcd /server/software/k8swget https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgzmkdir -pv /opt/cni/bintar xf cni-plugins-amd64-v0.7.1.tgz -C /opt/cni/binls -l /opt/cni/bincd $HOME 配置启动kubelet1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 启动文件mkdir -pv /data/kubeletcat &gt;/etc/systemd/system/kubelet.service&lt;&lt;EOF[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/kubernetes/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/data/kubeletEnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/local/kubernetes/bin/kubelet \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBELET_CONFIG \\ \$KUBELET_HOSTNAME \\ \$KUBELET_POD_INFRA_CONTAINER \\ \$KUBELET_ARGSRestart=on-failure[Install]WantedBy=multi-user.targetEOFcat &gt;/etc/kubernetes/config&lt;&lt;EOFKUBE_LOGTOSTDERR="--logtostderr=true"KUBE_LOG_LEVEL="--v=2"EOF# 注意修改相关ipcat &gt;/etc/kubernetes/kubelet&lt;&lt;EOFKUBELET_HOSTNAME="--hostname-override=11.11.11.111"KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1"KUBELET_CONFIG="--config=/etc/kubernetes/kubelet-config.yml"KUBELET_ARGS="--bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.conf --kubeconfig=/etc/kubernetes/kubelet.conf --cert-dir=/etc/kubernetes/pki --network-plugin=cni --cni-bin-dir=/opt/cni/bin --cni-conf-dir=/etc/cni/net.d"EOF# 注意修改相关ip# lab1 lab2 lab3 使用各自ipcat &gt;/etc/kubernetes/kubelet-config.yml&lt;&lt;EOFkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 11.11.11.111port: 10250cgroupDriver: cgroupfsclusterDNS: - 10.96.0.10clusterDomain: cluster.local.hairpinMode: promiscuous-bridgeserializeImagePulls: falseauthentication: x509: clientCAFile: /etc/kubernetes/pki/ca.pemEOF# 启动systemctl daemon-reloadsystemctl enable kubeletsystemctl start kubeletsystemctl status kubelet 通过证书请求123456789101112131415# 在配置了kubectl的节点上执行如下操作# 查看kubectl get csr# 通过kubectl certificate approve node-csr-Yiiv675wUCvQl3HH11jDr0cC9p3kbrXWrxvG3EjWGoE# 查看节点# 此时节点状态为 NotReadykubectl get nodes# 在node节点查看生成的文件ls -l /etc/kubernetes/kubelet.confls -l /etc/kubernetes/pki/kubelet* 配置启动kube-proxy12345678910111213141516171819202122232425262728293031323334353637383940# 安装yum install -y conntrack-tools# 启动文件cat &gt;/etc/systemd/system/kube-proxy.service&lt;&lt;EOF[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/proxyExecStart=/usr/local/kubernetes/bin/kube-proxy \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBECONFIG \\ \$KUBE_PROXY_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF# 注意修改相关ip# lab1 lab2 lab3 使用各自ip# 由于 1.11.0 ipvs 在centos7上有bug无法正常使用# 实验使用 iptables 模式# 以后版本可以使用 ipvs 模式cat &gt;/etc/kubernetes/proxy&lt;&lt;EOFKUBECONFIG="--kubeconfig=/etc/kubernetes/kube-proxy.conf"KUBE_PROXY_ARGS="--bind-address=11.11.11.111 --proxy-mode=iptables --hostname-override=11.11.11.111 --cluster-cidr=10.244.0.0/16"EOF# 启动systemctl daemon-reloadsystemctl enable kube-proxysystemctl start kube-proxysystemctl status kube-proxy 设置集群角色12345678910111213# 设置 lab1 为 masterkubectl label nodes 11.11.11.111 node-role.kubernetes.io/master=# 设置 lab2 lab3 为 nodekubectl label nodes 11.11.11.112 node-role.kubernetes.io/node=kubectl label nodes 11.11.11.113 node-role.kubernetes.io/node=# 设置 master 一般情况下不接受负载kubectl taint nodes 11.11.11.111 node-role.kubernetes.io/master=true:NoSchedule# 查看节点# 此时节点状态为 NotReadykubectl get no 配置使用flannel网络 在lab1操作 123456789101112131415161718192021222324252627282930313233343536373839404142# 下载配置mkdir flannel &amp;&amp; cd flannelwget https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml# 修改配置# 此处的ip配置要与上面kubeadm的pod-network一致 net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1# 启动kubectl apply -f kube-flannel.yml# 查看kubectl get pods -n kube-systemkubectl get svc -n kube-system# 查看节点状态# 当 flannel pod 全部启动之后，节点状态为 Readykubectl get no 配置使用coredns 在lab1操作 123456789101112# 安装# 10.96.0.10 kubelet中配置的dnscd $HOME &amp;&amp; mkdir coredns &amp;&amp; cd corednswget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sedwget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/deploy.shchmod +x deploy.sh./deploy.sh -i 10.96.0.10 &gt; coredns.ymlkubectl apply -f coredns.yml# 查看kubectl get pods -n kube-systemkubectl get svc -n kube-system 测试启动1234kubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-servicekubectl scale --replicas=3 deployment/nginx 查看状态1234kubectl get deploy -o widekubectl get pods -o widekubectl get svc -o widekubectl describe svc example-service DNS解析1234kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service 访问测试1234567# 10.96.59.56 为查看svc时获取到的clusteripcurl "10.107.91.153:80"# 32223 为查看svc时获取到的 nodeporthttp://11.11.11.111:32223/http://11.11.11.112:32223/http://11.11.11.113:32223/ 清理12kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 参考文档 https://jimmysong.io/kubernetes-handbook/practice/install-kubernetes-on-centos.html https://zhangguanzhang.github.io/2018/05/05/Kubernetes_install/ https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ https://kairen.github.io/2018/04/05/kubernetes/deploy/manual-v1.10/ http://www.maogx.win/posts/7/ https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ https://kubernetes-v1-4.github.io/docs/user-guide/kubectl/kubectl_label/ https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://github.com/gjmzj/kubeasz]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes基础概念总结]]></title>
    <url>%2Fposts%2F34%2F</url>
    <content type="text"><![CDATA[简介kubernetes是由google主导开发的开源容器管理平台，提供多主机集群，容器编排，容器伸缩等功能 基础组件介绍docker容器运行环境的一种实现，封装底层容器 etcdetcd是kubernetes集群用来存储集群相关数据的数据仓库 master节点组件master节点是主集群中的大脑，负责处理外部的api请求，分配调度任务以及管理容器的副本数等 kube-apiserver kubernetes对外的服务入口，其他组件通信的纽带，服务无状态，可水平扩容 kube-scheduler 负责pod的任务调度 kube-controller-manager 处理node节点当机情况 负责保证pod的副本数 管理endpoint，连接service和pod 为新namespace创建默认api token和accounts node节点组件node节点负责干活，执行master节点指派的相关任务 kubelet 负责启动停止容器，保证容器运行。 kube-proxy 负责根据service生成网络规则，生成路由规则 组件通信 架构图 组件间的通信全依赖于kube-apiserver，其他组件通过http协议与kube-apiserver交互。 当使用kubectl或者直接调用kube-apiserver提供的api请求创建pod里和service时工作流程如下： kube-apiserver把相关的pod和service配置存储到etcd中 kube-scheduler从kube-apiserver获取到相关pod的配置，根据集群中的资源和条件限制把pod调度到相应的node节点上 kube-controller-manager从kube-apiserver获取到相关pod和service的配置，定期检查pod的状态，保证有用户配置的足够数量的pod副本在运行，生成service到pod的规则关系。 kubelet从kube-apiserver获取分配到本节点的相关pod配置，在本地启动容器并定期检查返回容器状态 kube-proxy从kube-apiserver获取service到pod的规则，在本节点维护iptable或者ipvs相关路由规则 基础概念pod kubernetes的最小调度单元，一个pod中可以有多个容器，多个容器共享网络和存储卷 service kubernetes抽象出来一个概念，可以理解为负载均衡器，后端接pod cluster ip service在集群中的ip，相当负载均衡器的ip ingress 对集群外部暴露集群内部service的一种方式 nodeport 对集群外部暴露服务的第二种方式，跟随service配置，让集群中的node节点都监听相应的端口，可以通过node节点访问集群内部service loadbalancer 对集群外部暴露集群内部service的一种方式，一般只有在云平台才能使用 deployment 封装了pod/replicaset，可以实现指定pod副本数量，滚动更新，扩容pod，一般一个应用（服务）一个deployment statefullSet 相当于有状态服务的deployment，重启后，主机名和pod的名称不会改变 daemonSet 在每个node节点都需要运行的pod可以使用daemonSet job 一次性任务 cronJob 类似于crontab定时执行任务 dns kube-dns/coredns提供集群中的dns服务，可以解析service到cluster ip，实现服务发现 pv 管理员用来提前创建好的存储空间，供用户申请使用 pvc 用户用来申请存储空间 storageClass 定义存储类供pvc使用，当用户通过pvc并指定storageClass请求pv时kubernetes可以根据storageClass动态创建pv 排错技巧查看日志查看pod日志 kubectl logs pod-name 查看事件 kubectl describe pod-name 查看docker日志 通过kubectl get pod -o wide找到容器运行的node节点 在node节点上通过docker ps -a找到出错的容器 docker logs container-id 查看kubelet及其他组件日志 journalctl -u kubelet]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7使用kubeadm安装k8s-1.11版本多主高可用]]></title>
    <url>%2Fposts%2F33%2F</url>
    <content type="text"><![CDATA[实验环境说明实验架构图12345678lab1: etcd master haproxy keepalived 11.11.11.111lab2: etcd master haproxy keepalived 11.11.11.112lab3: etcd master haproxy keepalived 11.11.11.113lab4: node 11.11.11.114lab5: node 11.11.11.115lab6: node 11.11.11.116vip(loadblancer ip): 11.11.11.110 实验使用的Vagrantfile123456789101112131415161718192021# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..6).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "2048"] end end endend 关闭防火墙12systemctl stop firewalldsystemctl disable firewalld 安装配置docker v1.11.0版本推荐使用docker v17.03,v1.11,v1.12,v1.13, 也可以使用，再高版本的docker可能无法正常使用。测试发现17.09无法正常使用，不能使用资源限制(内存CPU) 如下操作在所有节点操作 安装docker12345# 卸载安装指定版本docker-ceyum remove -y docker-ce docker-ce-selinux container-selinuxyum install -y --setopt=obsoletes=0 \docker-ce-17.03.1.ce-1.el7.centos \docker-ce-selinux-17.03.1.ce-1.el7.centos 启动docker1systemctl enable docker &amp;&amp; systemctl restart docker 安装 kubeadm, kubelet 和 kubectl 如下操作在所有节点操作 使用阿里镜像安装12345678910111213# 配置源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 安装yum install -y kubelet-1.11.2 kubeadm-1.11.2 kubectl-1.11.2 ipvsadm 配置系统相关参数1234567891011121314151617181920212223242526272829303132# 临时禁用selinux# 永久关闭 修改/etc/sysconfig/selinux文件设置sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinuxsetenforce 0# 临时关闭swap# 永久关闭 注释/etc/fstab文件里swap相关的行swapoff -a# 开启forward# Docker从1.13版本开始调整了默认的防火墙规则# 禁用了iptables filter表中FOWARD链# 这样会引起Kubernetes集群中跨Node的Pod无法通信iptables -P FORWARD ACCEPT# 配置转发相关参数，否则可能会出错cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1vm.swappiness=0EOFsysctl --system# 加载ipvs相关内核模块# 如果重新开机，需要重新加载modprobe ip_vsmodprobe ip_vs_rrmodprobe ip_vs_wrrmodprobe ip_vs_shmodprobe nf_conntrack_ipv4lsmod | grep ip_vs 配置hosts解析 如下操作在所有节点操作 12345678cat &gt;&gt;/etc/hosts&lt;&lt;EOF11.11.11.111 lab111.11.11.112 lab211.11.11.113 lab311.11.11.114 lab411.11.11.115 lab511.11.11.116 lab6EOF 配置haproxy代理和keepalived 如下操作在节点lab1,lab2,lab3操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# 拉取haproxy镜像docker pull haproxy:1.7.8-alpinemkdir /etc/haproxycat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOFglobal log 127.0.0.1 local0 err maxconn 50000 uid 99 gid 99 #daemon nbproc 1 pidfile haproxy.piddefaults mode http log 127.0.0.1 local0 err maxconn 50000 retries 3 timeout connect 5s timeout client 30s timeout server 30s timeout check 2slisten admin_stats mode http bind 0.0.0.0:1080 log 127.0.0.1 local0 err stats refresh 30s stats uri /haproxy-status stats realm Haproxy\ Statistics stats auth will:will stats hide-version stats admin if TRUEfrontend k8s-https bind 0.0.0.0:8443 mode tcp #maxconn 50000 default_backend k8s-httpsbackend k8s-https mode tcp balance roundrobin server lab1 11.11.11.111:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab2 11.11.11.112:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab3 11.11.11.113:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3EOF# 启动haproxydocker run -d --name my-haproxy \-v /etc/haproxy:/usr/local/etc/haproxy:ro \-p 8443:8443 \-p 1080:1080 \--restart always \haproxy:1.7.8-alpine# 查看日志docker logs my-haproxy# 浏览器查看状态http://11.11.11.111:1080/haproxy-statushttp://11.11.11.112:1080/haproxy-status# 拉取keepalived镜像docker pull osixia/keepalived:1.4.4# 启动# 载入内核相关模块lsmod | grep ip_vsmodprobe ip_vs# 启动keepalived# eth1为本次实验11.11.11.0/24网段的所在网卡docker run --net=host --cap-add=NET_ADMIN \-e KEEPALIVED_INTERFACE=eth1 \-e KEEPALIVED_VIRTUAL_IPS="#PYTHON2BASH:['11.11.11.110']" \-e KEEPALIVED_UNICAST_PEERS="#PYTHON2BASH:['11.11.11.111','11.11.11.112','11.11.11.113']" \-e KEEPALIVED_PASSWORD=hello \--name k8s-keepalived \--restart always \-d osixia/keepalived:1.4.4# 查看日志# 会看到两个成为backup 一个成为masterdocker logs k8s-keepalived# 此时会配置 11.11.11.110 到其中一台机器# ping测试ping -c4 11.11.11.110# 如果失败后清理后，重新实验docker rm -f k8s-keepalivedip a del 11.11.11.110/32 dev eth1 配置启动kubelet 如下操作在所有节点操作 123456789101112# 配置kubelet使用国内pause镜像# 配置kubelet的cgroups# 获取docker的cgroupsDOCKER_CGROUPS=$(docker info | grep 'Cgroup' | cut -d' ' -f3)echo $DOCKER_CGROUPScat &gt;/etc/sysconfig/kubelet&lt;&lt;EOFKUBELET_EXTRA_ARGS="--cgroup-driver=$DOCKER_CGROUPS --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1"EOF# 启动systemctl daemon-reloadsystemctl enable kubelet &amp;&amp; systemctl restart kubelet 配置master配置第一个master节点 如下操作在lab1节点操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 1.11.0 版本 centos 下使用 ipvs 模式会出问题# 参考 https://github.com/kubernetes/kubernetes/issues/65461# 生成配置文件CP0_IP="11.11.11.111"CP0_HOSTNAME="lab1"cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.2imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapiServerCertSANs:- "lab1"- "lab2"- "lab3"- "11.11.11.111"- "11.11.11.112"- "11.11.11.113"- "11.11.11.110"- "127.0.0.1"api: advertiseAddress: $CP0_IP controlPlaneEndpoint: 11.11.11.110:8443etcd: local: extraArgs: listen-client-urls: "https://127.0.0.1:2379,https://$CP0_IP:2379" advertise-client-urls: "https://$CP0_IP:2379" listen-peer-urls: "https://$CP0_IP:2380" initial-advertise-peer-urls: "https://$CP0_IP:2380" initial-cluster: "$CP0_HOSTNAME=https://$CP0_IP:2380" serverCertSANs: - $CP0_HOSTNAME - $CP0_IP peerCertSANs: - $CP0_HOSTNAME - $CP0_IPcontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16 kubeProxy: config: # mode: ipvs mode: iptablesEOF# 提前拉取镜像# 如果执行失败 可以多次执行kubeadm config images pull --config kubeadm-master.config# 初始化# 注意保存返回的 join 命令kubeadm init --config kubeadm-master.config# 打包ca相关文件上传至其他master节点cd /etc/kubernetes &amp;&amp; tar cvzf k8s-key.tgz admin.conf pki/ca.* pki/sa.* pki/front-proxy-ca.* pki/etcd/ca.*scp k8s-key.tgz lab2:~/scp k8s-key.tgz lab3:~/ssh lab2 'tar xf k8s-key.tgz -C /etc/kubernetes/'ssh lab3 'tar xf k8s-key.tgz -C /etc/kubernetes/' 配置第二个master节点 如下操作在lab2节点操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# 1.11.0 版本 centos 下使用 ipvs 模式会出问题# 参考 https://github.com/kubernetes/kubernetes/issues/65461# 生成配置文件CP0_IP="11.11.11.111"CP0_HOSTNAME="lab1"CP1_IP="11.11.11.112"CP1_HOSTNAME="lab2"cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.2imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapiServerCertSANs:- "lab1"- "lab2"- "lab3"- "11.11.11.111"- "11.11.11.112"- "11.11.11.113"- "11.11.11.110"- "127.0.0.1"api: advertiseAddress: $CP1_IP controlPlaneEndpoint: 11.11.11.110:8443etcd: local: extraArgs: listen-client-urls: "https://127.0.0.1:2379,https://$CP1_IP:2379" advertise-client-urls: "https://$CP1_IP:2379" listen-peer-urls: "https://$CP1_IP:2380" initial-advertise-peer-urls: "https://$CP1_IP:2380" initial-cluster: "$CP0_HOSTNAME=https://$CP0_IP:2380,$CP1_HOSTNAME=https://$CP1_IP:2380" initial-cluster-state: existing serverCertSANs: - $CP1_HOSTNAME - $CP1_IP peerCertSANs: - $CP1_HOSTNAME - $CP1_IPcontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16 kubeProxy: config: # mode: ipvs mode: iptablesEOF# 配置kubeletkubeadm alpha phase certs all --config kubeadm-master.configkubeadm alpha phase kubelet config write-to-disk --config kubeadm-master.configkubeadm alpha phase kubelet write-env-file --config kubeadm-master.configkubeadm alpha phase kubeconfig kubelet --config kubeadm-master.configsystemctl restart kubelet# 添加etcd到集群中CP0_IP="11.11.11.111"CP0_HOSTNAME="lab1"CP1_IP="11.11.11.112"CP1_HOSTNAME="lab2"KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP1_HOSTNAME&#125; https://$&#123;CP1_IP&#125;:2380kubeadm alpha phase etcd local --config kubeadm-master.config# 提前拉取镜像# 如果执行失败 可以多次执行kubeadm config images pull --config kubeadm-master.config# 部署kubeadm alpha phase kubeconfig all --config kubeadm-master.configkubeadm alpha phase controlplane all --config kubeadm-master.configkubeadm alpha phase mark-master --config kubeadm-master.config 配置第三个master节点 如下操作在lab3节点操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# 1.11.0 版本 centos 下使用 ipvs 模式会出问题# 参考 https://github.com/kubernetes/kubernetes/issues/65461# 生成配置文件CP0_IP="11.11.11.111"CP0_HOSTNAME="lab1"CP1_IP="11.11.11.112"CP1_HOSTNAME="lab2"CP2_IP="11.11.11.113"CP2_HOSTNAME="lab3"cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.2imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapiServerCertSANs:- "lab1"- "lab2"- "lab3"- "11.11.11.111"- "11.11.11.112"- "11.11.11.113"- "11.11.11.110"- "127.0.0.1"api: advertiseAddress: $CP2_IP controlPlaneEndpoint: 11.11.11.110:8443etcd: local: extraArgs: listen-client-urls: "https://127.0.0.1:2379,https://$CP2_IP:2379" advertise-client-urls: "https://$CP2_IP:2379" listen-peer-urls: "https://$CP2_IP:2380" initial-advertise-peer-urls: "https://$CP2_IP:2380" initial-cluster: "$CP0_HOSTNAME=https://$CP0_IP:2380,$CP1_HOSTNAME=https://$CP1_IP:2380,$CP2_HOSTNAME=https://$CP2_IP:2380" initial-cluster-state: existing serverCertSANs: - $CP2_HOSTNAME - $CP2_IP peerCertSANs: - $CP2_HOSTNAME - $CP2_IPcontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16 kubeProxy: config: # mode: ipvs mode: iptablesEOF# 配置kubeletkubeadm alpha phase certs all --config kubeadm-master.configkubeadm alpha phase kubelet config write-to-disk --config kubeadm-master.configkubeadm alpha phase kubelet write-env-file --config kubeadm-master.configkubeadm alpha phase kubeconfig kubelet --config kubeadm-master.configsystemctl restart kubelet# 添加etcd到集群中CP0_IP="11.11.11.111"CP0_HOSTNAME="lab1"CP2_IP="11.11.11.113"CP2_HOSTNAME="lab3"KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP2_HOSTNAME&#125; https://$&#123;CP2_IP&#125;:2380kubeadm alpha phase etcd local --config kubeadm-master.config# 提前拉取镜像# 如果执行失败 可以多次执行kubeadm config images pull --config kubeadm-master.config# 部署kubeadm alpha phase kubeconfig all --config kubeadm-master.configkubeadm alpha phase controlplane all --config kubeadm-master.configkubeadm alpha phase mark-master --config kubeadm-master.config 配置使用kubectl 如下操作在任意master节点操作 123456789101112rm -rf $HOME/.kubemkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 查看node节点kubectl get nodes# 只有网络插件也安装配置完成之后，才能会显示为ready状态# 设置master允许部署应用pod，参与工作负载，现在可以部署其他系统组件# 如 dashboard, heapster, efk等kubectl taint nodes --all node-role.kubernetes.io/master- 配置使用网络插件 如下操作在任意master节点操作 1234567891011121314151617181920212223242526272829303132333435363738# 下载配置mkdir flannel &amp;&amp; cd flannelwget https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml# 修改配置# 此处的ip配置要与上面kubeadm的pod-network一致 net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1# 启动kubectl apply -f kube-flannel.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system 配置node节点加入集群 如下操作在所有node节点操作 12# 此命令为初始化master成功后返回的结果kubeadm join 11.11.11.110:8443 --token yzb7v7.dy40mhlljt1d48i9 --discovery-token-ca-cert-hash sha256:61ec309e6f942305006e6622dcadedcc64420e361231eff23cb535a183c0e77a 基础测试测试容器间的通信和DNS 配置好网络之后，kubeadm会自动部署coredns 如下测试可以在配置kubectl的节点上操作 启动123kubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service 查看状态1234kubectl get deploykubectl get podskubectl get svckubectl describe svc example-service DNS解析1234kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service 访问测试123456# 10.96.59.56 为查看svc时获取到的clusteripcurl "10.96.59.56:80"# 32223 为查看svc时获取到的 nodeporthttp://11.11.11.112:32223/http://11.11.11.113:32223/ 清理删除12kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 高可用测试关闭任一master节点测试集群是能否正常执行上一步的基础测试，查看相关信息，不能同时关闭两个节点，因为3个节点组成的etcd集群，最多只能有一个当机。 12345678910111213141516# 查看组件状态kubectl get pod --all-namespaces -o widekubectl get pod --all-namespaces -o wide | grep lab1kubectl get pod --all-namespaces -o wide | grep lab2kubectl get pod --all-namespaces -o wide | grep lab3kubectl get nodes -o widekubectl get deploykubectl get podskubectl get svc# 访问测试CURL_POD=$(kubectl get pods | grep curl | grep Running | cut -d ' ' -f1)kubectl exec -ti $CURL_POD -- sh --ttynslookup kubernetesnslookup example-servicecurl example-service 小技巧忘记初始master节点时的node节点加入集群命令怎么办 123456# 简单方法kubeadm token create --print-join-command# 第二种方法token=$(kubeadm token generate)kubeadm token create $token --print-join-command --ttl=0 参考文档 https://kubernetes.io/docs/setup/independent/install-kubeadm/ https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/ https://kubernetes.io/docs/setup/independent/high-availability/ https://sealyun.com/post/k8s-ipvs/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7使用kubeadm安装k8s-1.11版本]]></title>
    <url>%2Fposts%2F32%2F</url>
    <content type="text"><![CDATA[实验环境说明实验架构123lab1: master 11.11.11.111lab2: node 11.11.11.112lab3: node 11.11.11.113 实验使用的Vagrantfile123456789101112131415161718192021# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..3).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "2048"] end end endend 关闭防火墙12systemctl stop firewalldsystemctl disable firewalld 安装配置docker v1.11.0版本推荐使用docker v17.03,v1.11,v1.12,v1.13, 也可以使用，再高版本的docker可能无法正常使用。测试发现17.09无法正常使用，不能使用资源限制(内存CPU) 如下操作在所有节点操作 安装docker12345# 卸载安装指定版本docker-ceyum remove -y docker-ce docker-ce-selinux container-selinuxyum install -y --setopt=obsoletes=0 \docker-ce-17.03.1.ce-1.el7.centos \docker-ce-selinux-17.03.1.ce-1.el7.centos 启动docker1systemctl enable docker &amp;&amp; systemctl restart docker 安装 kubeadm, kubelet 和 kubectl 如下操作在所有节点操作 使用阿里镜像安装12345678910111213# 配置源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 安装yum install -y kubelet-1.11.2 kubeadm-1.11.2 kubectl-1.11.2 ipvsadm 配置系统相关参数1234567891011121314151617181920212223242526272829303132# 临时禁用selinux# 永久关闭 修改/etc/sysconfig/selinux文件设置sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinuxsetenforce 0# 临时关闭swap# 永久关闭 注释/etc/fstab文件里swap相关的行swapoff -a# 开启forward# Docker从1.13版本开始调整了默认的防火墙规则# 禁用了iptables filter表中FOWARD链# 这样会引起Kubernetes集群中跨Node的Pod无法通信iptables -P FORWARD ACCEPT# 配置转发相关参数，否则可能会出错cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1vm.swappiness=0EOFsysctl --system# 加载ipvs相关内核模块# 如果重新开机，需要重新加载modprobe ip_vsmodprobe ip_vs_rrmodprobe ip_vs_wrrmodprobe ip_vs_shmodprobe nf_conntrack_ipv4lsmod | grep ip_vs 配置hosts解析 如下操作在所有节点操作 12345cat &gt;&gt;/etc/hosts&lt;&lt;EOF11.11.11.111 lab111.11.11.112 lab211.11.11.113 lab3EOF 配置启动kubelet 如下操作在所有节点操作 123456789101112# 配置kubelet使用国内pause镜像# 配置kubelet的cgroups# 获取docker的cgroupsDOCKER_CGROUPS=$(docker info | grep 'Cgroup' | cut -d' ' -f3)echo $DOCKER_CGROUPScat &gt;/etc/sysconfig/kubelet&lt;&lt;EOFKUBELET_EXTRA_ARGS="--cgroup-driver=$DOCKER_CGROUPS --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1"EOF# 启动systemctl daemon-reloadsystemctl enable kubelet &amp;&amp; systemctl start kubelet 配置master节点 如下操作在master节点操作 12345678910111213141516171819202122232425262728293031# 1.11.0 版本 centos 下使用 ipvs 模式会出问题# 参考 https://github.com/kubernetes/kubernetes/issues/65461# 生成配置文件cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.2imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapi: advertiseAddress: 11.11.11.111controllerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16 kubeProxy: config: mode: ipvs # mode: iptablesEOF# 提前拉取镜像# 如果执行失败 可以多次执行kubeadm config images pull --config kubeadm-master.config# 初始化kubeadm init --config kubeadm-master.config 配置使用kubectl 如下操作在master节点操作 123456789101112rm -rf $HOME/.kubemkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 查看node节点kubectl get nodes# 只有网络插件也安装配置完成之后，才能会显示为ready状态# 设置master允许部署应用pod，参与工作负载，现在可以部署其他系统组件# 如 dashboard, heapster, efk等kubectl taint nodes --all node-role.kubernetes.io/master- 配置使用网络插件 如下操作在master节点操作 1234567891011121314151617181920212223242526272829303132333435363738# 下载配置mkdir flannel &amp;&amp; cd flannelwget https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml# 修改配置# 此处的ip配置要与上面kubeadm的pod-network一致 net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1# 启动kubectl apply -f kube-flannel.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system 配置node节点加入集群 如下操作在所有node节点操作 12# 此命令为初始化master成功后返回的结果kubeadm join 11.11.11.111:6443 --token yl53pn.wpx4mvx6a6jfkjhw --discovery-token-ca-cert-hash sha256:17751fcda3e79da63f5d0c4a3586e97de8b8b1d017c1a6977c88136409af5240 测试容器间的通信和DNS 配置好网络之后，kubeadm会自动部署coredns 启动123kubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service 查看状态1234kubectl get deploykubectl get podskubectl get svckubectl describe svc example-service DNS解析1234kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service 访问测试123456# 10.96.59.56 为查看svc时获取到的clusteripcurl "10.96.59.56:80"# 32223 为查看svc时获取到的 nodeporthttp://11.11.11.112:32223/http://11.11.11.113:32223/ 清理删除12kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 小技巧忘记初始master节点时的node节点加入集群命令怎么办 123456# 简单方法kubeadm token create --print-join-command# 第二种方法token=$(kubeadm token generate)kubeadm token create $token --print-join-command --ttl=0 参考文档 https://kubernetes.io/docs/setup/independent/install-kubeadm/ https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/ https://sealyun.com/post/k8s-ipvs/ https://blog.frognew.com/2017/12/kubeadm-install-kubernetes-1.9.html]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[istio-0.8长期支持版微服务实验]]></title>
    <url>%2Fposts%2F31%2F</url>
    <content type="text"><![CDATA[简介本实验通过在k8s上部署istio，实现微服务的基础功能。其中会涉及到服务的限流，超时，熔断，降级，流量分隔，A/B测试等功能。实验之前需要安装k8s和istio，请参考之前文章。注意开启istio的自动注入功能，并在 default namespace 启用自动注入功能。 本实验的服务间调用关系如下： 本实验采用时下流行的前后端分离模式 前端项目基于vue/react实现 前端调用python实现的API接口 python服务调用后端node实现的服务和lua实现的服务 node服务调用go实现的服务 —-&gt;service-js —-&gt;service-python —-&gt;service-lua —-&gt;service-node —-&gt;service-go 本实验使用的语言技术栈： vue/react python2/3 node8/10 openresty1.11 /1.13 go1.10/1.9 架构图如下： istio-0.8版本配置发生很大的变化，由原来的v1alpha1升级到了v1alpha3，主要变化如下 使用virtualservice和destinationrule 代替原来的routerule 使用gateway代替了原来的ingress 每个virtualservice都要指定要去向哪一个destinationrule ，virtualservice指定访问哪个地址时会使用这个路由，相当于nginx上配置的vhosts 下载实验仓库12git clone https://github.com/mgxian/istio-testcd istio-test &amp;&amp; git checkout v2 部署服务12345678910kubectl apply -f service/go/v1/go-v1.ymlkubectl apply -f service/go/v2/go-v2.ymlkubectl apply -f service/python/v1/python-v1.ymlkubectl apply -f service/python/v2/python-v2.ymlkubectl apply -f service/js/v1/js-v1.ymlkubectl apply -f service/js/v2/js-v2.ymlkubectl apply -f service/node/v1/node-v1.ymlkubectl apply -f service/node/v2/node-v2.ymlkubectl apply -f service/lua/v1/lua-v1.ymlkubectl apply -f service/lua/v2/lua-v2.yml 创建Gateway123456789101112131415161718# 使用istio提供的Gateway功能# 暴露js和python服务让k8s集群外部访问istioctl create -f istio/gateway.ymlistioctl create -f istio/gateway-virtualservice.yml# 查看istioctl get gatewayistioctl get virtualservice# 测试访问INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='&#123;.spec.ports[?(@.name=="http2")].nodePort&#125;')NODE_NAME=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')NODE_IP=$(ping -c 1 $NODE_NAME | grep PING | awk '&#123;print $3&#125;' | tr -d '()')export GATEWAY_URL=$NODE_IP:$INGRESS_PORTecho "curl -I http://$GATEWAY_URL/"echo "curl -I http://$NODE_IP/"# 访问返回404表示正确 配置测试访问环境123456789101112# 配置hosts解析# 11.11.11.112为其中一个node的ip11.11.11.112 istio-test.willcurl -I http://istio-test.will/# 使用curlcurl -I istio-test.willcurl -s istio-test.will | egrep "vue|React"# 此时如果用浏览器，可能会出会页面显示不正常的情况。# 因为此时请求会轮流分发到后端js服务的v1/v2版本，因此css/js并不能正常加载 流量管理根据请求的信息，把流量路由到服务的不同版本。实验过程如果没有达到预期效果，很有可能是因为存在路由规则冲突，而且没有设置优先级，可以先删除之前设置的路由规则或者把优先级设置高一点。 把所有流量导向v1版本1234567891011121314151617181920212223# 清理之前创建的gateway相关的路由规则istioctl delete -f istio/gateway-virtualservice.yml# 创建路由规则istioctl create -f istio/gateway-virtualservice-v1.ymlistioctl create -f istio/route-rule-all-v1.yml# 查看路由规则istioctl get virtualserviceistioctl get destinationrule# 访问浏览器测试http://istio-test.will/# 此时你会看到react app的界面# 点击发射按钮，会发送ajax请求到python服务# 由于把所有流量都导向了v1版本# 多次点击发射按钮会得到一样的内容# react-----&gt;Python2.7.15-----&gt;Gogo1.9.6# 清除路由规则istioctl delete -f istio/route-rule-all-v1.ymlistioctl delete -f istio/gateway-virtualservice-v1.yml 根据请求把流量导向不同版本（A/B测试）123456789101112131415161718192021# 创建路由规则# 根据浏览器的不同返回不同内容istioctl create -f istio/route-rule-js-by-agent.yml# 查看路由规则istioctl get virtualserviceistioctl get destinationrule# 使用访问浏览器# 如果你用chrome浏览器你会看到react app的界面# 如果你用firefox浏览器你会看到vue app的界面# 多次点击发射按钮，会获取到不同的内容# 清除路由规则istioctl delete -f istio/route-rule-js-by-agent.yml# 根据前端app不同使用不同版本的python服务istioctl create -f istio/route-rule-python-by-header.yml# 清除路由规则istioctl delete -f istio/route-rule-python-by-header.yml 根据源服务把流量导向不同版本12345678910# 先创建如下路由方便测试访问# 根据浏览器的不同返回不同内容istioctl create -f istio/route-rule-js-by-agent.yml# 创建路由规则istioctl create -f istio/route-rule-go-by-source.yml# 清除路由规则istioctl delete -f istio/route-rule-js-by-agent.ymlistioctl delete -f istio/route-rule-go-by-source.yml 指定权重进行流量分隔1234567891011121314# 指定权重把流量分隔# 25%流量路由到v1版本# 75%流量路由到v2版本# 先创建如下路由方便测试访问# 根据浏览器的不同返回不同内容istioctl create -f istio/route-rule-js-by-agent.yml# 创建路由规则istioctl create -f istio/route-rule-go-v1-v2.yaml# 清除路由规则istioctl delete -f istio/route-rule-js-by-agent.ymlistioctl delete -f istio/route-rule-go-v1-v2.yaml 集群内访问公开服务1234567891011121314151617181920212223242526272829303132# 默认情况下，启用了istio的服务是无法访问外部url的# 如果需要访问外部url，需要使用egress进行配置# egress同样支持设置路由规则# httpistioctl create -f istio/egress-rule-http-bin.yml# tcpistioctl create -f istio/egress-rule-tcp-wikipedia.yml# 查看istioctl get serviceentry# 测试# 使用exec进入作为测试源使用的podkubectl apply -f istio/sleep.yamlkubectl get podsexport SOURCE_POD=$(kubectl get pod -l app=sleep -o jsonpath=&#123;.items..metadata.name&#125;)kubectl exec -it $SOURCE_POD -c sleep bash# http测试curl http://httpbin.org/headerscurl http://httpbin.org/delay/5# tcp测试curl -o /dev/null -s -w "%&#123;http_code&#125;\n" https://www.wikipedia.orgcurl -s https://en.wikipedia.org/wiki/Main_Page | grep articlecount | grep 'Special:Statistics'# 清理istioctl delete -f istio/egress-rule-http-bin.ymlistioctl delete -f istio/egress-rule-tcp-wikipedia.ymlkubectl delete -f istio/sleep.yaml 故障管理 调用超时设置和重试设置 故障注入，模拟服务故障 设置超时时间与模拟服务超时故障123456789101112131415161718# 先创建如下路由方便测试访问# 根据浏览器的不同返回不同内容istioctl create -f istio/route-rule-js-by-agent.yml# 设置python服务超时时间istioctl create -f istio/route-rule-node-timeout.yml# 模拟go服务超时故障istioctl create -f istio/route-rule-go-delay.yml# 使用浏览器访问并打开调试面板查看网络标签（按F12键）# 多次点击发射按钮观察响应时间# 会看到部分50%的请求会返回500错误# 清除路由规则istioctl delete -f istio/route-rule-js-by-agent.ymlistioctl delete -f istio/route-rule-node-timeout.ymlistioctl delete -f istio/route-rule-go-delay.yml 设置重试与模拟服务500故障123456789101112131415161718# 先创建如下路由方便测试访问# 根据浏览器的不同返回不同内容istioctl create -f istio/route-rule-js-by-agent.yml# 设置python服务超时时间istioctl create -f istio/route-rule-node-retry.yml# 模拟go服务超时故障istioctl create -f istio/route-rule-go-abort.yml# 使用浏览器访问并打开调试面板查看网络标签（按F12键）# 多次点击发射按钮观察响应时间# 会看到部分请求会返回500错误# 清除路由规则istioctl delete -f istio/route-rule-js-by-agent.ymlistioctl delete -f istio/route-rule-node-retry.ymlistioctl delete -f istio/route-rule-go-abort.yml 超时和服务故障模拟配合使用1234567891011# 所有请求延迟5秒钟，然后失败其中的10％... route: - labels: version: v1 httpFault: delay: fixedDelay: 5s abort: percent: 10 httpStatus: 400 熔断器12345678910111213141516171819202122232425262728# 设置熔断规则istioctl create -f istio/route-rule-go-cb.yml# 查看规则istioctl get destinationrule# 创建测试用的fortiokubectl apply -f istio/fortio-deploy.yaml# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-go/env# 测试熔断 2并发kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -c 2 -qps 0 -n 20 -loglevel Warning http://service-go/env# 测试熔断 3并发kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -c 3 -qps 0 -n 20 -loglevel Warning http://service-go/env# 增加并发会看到失败的请求占比增高# 查看状态# upstream_rq_pending_overflow 表示被熔断的请求数kubectl exec -it $FORTIO_POD -c istio-proxy -- sh -c 'curl localhost:15000/stats' | grep service-go | grep pending# 清理kubectl delete -f istio/fortio-deploy.yamlistioctl delete -f istio/route-rule-go-cb.yml 限流动态设置服务qps https://github.com/istio/istio/blob/master/samples/bookinfo/kube/mixer-rule-ratings-ratelimit.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 配置 memquota, quota, rule, QuotaSpec, QuotaSpecBinding 启用限速# 默认设置500qpsistioctl create -f istio/ratelimit-handler.yaml# 配置速率限制实例和规则istioctl create -f istio/ratelimit-rule-service-go.yaml# 查看kubectl get memquota -n istio-systemkubectl get quota -n istio-systemkubectl get rule -n istio-systemkubectl get quotaspec -n istio-systemkubectl get quotaspecbinding -n istio-system# 创建测试用的fortiokubectl apply -f istio/fortio-deploy.yaml# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-node/env# 测试# 会出现部分请求不正常# node 返回 code 500# go 返回 code 429kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 20 -n 100 -loglevel Warning http://service-node/envkubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 50 -n 100 -loglevel Warning http://service-go/env# 清理istioctl delete -f istio/ratelimit-handler.yamlistioctl delete -f istio/ratelimit-rule-service-go.yamlkubectl delete -f istio/fortio-deploy.yaml# 带条件的速率限制apiVersion: config.istio.io/v1alpha2kind: rulemetadata: name: quota namespace: istio-systemspec: match: source.namespace != destination.namespace actions: - handler: handler.memquota instances: - requestcount.quota 流量镜像复制服务的流量到别一个镜像服务，一般用于线上新上服务的测试。 1234567891011121314151617181920212223# 创建测试用的fortiokubectl apply -f istio/fortio-deploy.yaml# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-go/env# 查看v1的日志kubectl logs -f $(kubectl get pods | grep service-go-v1 | awk '&#123;print $1&#125;'| head -n 1) -c service-go# 查看v2的日志# 再开一个终端查看日志kubectl logs -f $(kubectl get pods | grep service-go-v2 | awk '&#123;print $1&#125;'| head -n 1) -c service-go# 创建镜像规则istioctl create -f istio/route-rule-go-mirror.yml# 测试多次访问kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -c 10 -qps 0 -t 10s -loglevel Warning http://service-go/env# 清理kubectl delete -f istio/fortio-deploy.yamlistioctl delete -f istio/route-rule-go-mirror.yml 清理1234567891011121314151617# 删除相关deploy和svckubectl delete -f service/go/v1/go-v1.ymlkubectl delete -f service/go/v2/go-v2.ymlkubectl delete -f service/python/v1/python-v1.ymlkubectl delete -f service/python/v2/python-v2.ymlkubectl delete -f service/js/v1/js-v1.ymlkubectl delete -f service/js/v2/js-v2.ymlkubectl delete -f service/node/v1/node-v1.ymlkubectl delete -f service/node/v2/node-v2.ymlkubectl delete -f service/lua/v1/lua-v1.ymlkubectl delete -f service/lua/v2/lua-v2.yml# 清除路由规则kubectl delete -f istio/gateway.ymlkubectl delete -f istio/gateway-virtualservice.ymlistioctl delete destinationrule $(istioctl get destinationrule | grep 'service-' | awk '&#123;print $1&#125;')istioctl delete virtualservice $(istioctl get virtualservice | grep 'service-' | awk '&#123;print $1&#125;') 参考文档 http://istio.doczh.cn https://istio.io/docs https://istio.io/docs/reference/config/istio.networking.v1alpha3.html https://istio.io/docs/reference/config/istio.routing.v1alpha1.html]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>service mesh</tag>
        <tag>istio</tag>
        <tag>k8s</tag>
        <tag>microservice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[istio-0.8长期支持版安装测试]]></title>
    <url>%2Fposts%2F30%2F</url>
    <content type="text"><![CDATA[简介istio是一个service mesh开源实现，由Google/IBM/Lyft共同开发。与之类似的还有conduit，但是功能不如istio丰富稳定。架构图如下： istio-0.8版本是第一个长期支持版本，相对于之前的版本配置改动较大。 安装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# 去下面的地址下载压缩包# https://github.com/istio/istio/releaseswget https://github.com/istio/istio/releases/download/0.8.0/istio-0.8.0-linux.tar.gztar xf istio-0.8.0-linux.tar.gz# 使用官方的安装脚本安装curl -L https://git.io/getLatestIstio | sh -# 安装配置环境变量mv istio-0.8.0 /usr/local/ln -sv /usr/local/istio-0.8.0 /usr/local/istioecho 'export PATH=/usr/local/istio/bin:$PATH' &gt; /etc/profile.d/istio.shsource /etc/profile.d/istio.shistioctl version# 如果环境不是云环境，不支持LoadBalancer# 作如下修改，使得 ingressgateway 监听在80和443端口# 修改使用主机端口映射# 使用此修改版本之后，每台机器只能运行单个实例# 大概在2661行左右cd /usr/local/istiocp install/kubernetes/istio-demo.yaml install/kubernetes/istio-demo.yaml.orivim install/kubernetes/istio-demo.yaml...# Source: istio/charts/ingressgateway/templates/deployment.yamlapiVersion: extensions/v1beta1# kind: Deployment# 使用DaemonSet部署方式kind: DaemonSetmetadata: name: istio-ingressgateway namespace: istio-system labels: app: ingressgateway chart: ingressgateway-0.8.0 release: RELEASE-NAME heritage: Tiller istio: ingressgatewayspec: # DaemonSet不支持replicas # replicas: template: metadata: labels: istio: ingressgateway annotations: sidecar.istio.io/inject: "false" spec: serviceAccountName: istio-ingressgateway-service-account containers: - name: ingressgateway image: "docker.io/istio/proxyv2:0.8.0" imagePullPolicy: IfNotPresent ports: - containerPort: 80 #主机80端口映射 hostPort: 80 - containerPort: 443 #主机443端口映射 hostPort: 443 - containerPort: 31400 #主机443端口映射 hostPort: 31400...# 由于镜像问题，提前拉取镜像# 在所有节点上执行如下命令输出的命令# 可能会失败，需要多次执行image=$(grep 'quay.io/coreos/hyperkube' install/kubernetes/istio-demo.yaml | head -1 | awk '&#123;print $2&#125;' | tr -d '"')echo "docker pull $image"# 以下两种选择一种安装方式# 安装不使用认证（不使用tls）kubectl apply -f install/kubernetes/istio-demo.yaml# 安装使用认证（使用tls）kubectl apply -f install/kubernetes/istio-demo-auth.yaml# 查看状态kubectl get svc -n istio-systemkubectl get pods -n istio-system# 访问测试nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "curl -I http://$nodeIP" 注意 istio-0.8.0 默认已经开启了自动注入功能以及其他日志监控和追踪的相关组件如 istio-tracing istio-telemetry grafana prometheus servicegraph 启用自动注入 sidecar 不开启自动注入部署应用需要使用如下方式的命令 kubectl apply -f &lt;(istioctl kube-inject -f samples/bookinfo/kube/bookinfo.yaml) 开启自动注入后，使用正常命令即可部署应用 kubectl apply -f samples/bookinfo/kube/bookinfo.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041# istio-0.8.0默认已经开启了自动注入功能# k8s 1.9 及之后的版本才能使用自动注入功能# 查看是否支持kubectl api-versions | grep admissionregistration# 除了要满足以上条件外还需要检查kube-apiserver启动的参数# k8s 1.9 版本要确保 --admission-control 里有 MutatingAdmissionWebhook,ValidatingAdmissionWebhook# k8s 1.9 之后的版本要确保 --enable-admission-plugins 里有MutatingAdmissionWebhook,ValidatingAdmissionWebhook# 测试自动注入# 创建kubectl apply -f samples/sleep/sleep.yaml kubectl get deployment -o widekubectl get pod# 设置 default namespace 开启自动注入kubectl label namespace default istio-injection=enabledkubectl get namespace -L istio-injection# 删除创建的pod，等待重建kubectl delete pod $(kubectl get pod | grep sleep | cut -d ' ' -f 1)# 查看重建后的pod# 查看是否有istio-proxy容器kubectl get podkubectl describe pod $(kubectl get pod | grep sleep | cut -d ' ' -f 1)# 清理kubectl delete -f samples/sleep/sleep.yaml # 关闭自动注入kubectl label namespace default istio-injection-# 关闭部分pod的自动注入功能... template: metadata: annotations: sidecar.istio.io/inject: "false"... 部署官方测试用例12345678910111213# 启动（未开启自动注入）kubectl apply -f &lt;(istioctl kube-inject -f samples/bookinfo/kube/bookinfo.yaml)# 启动（已开启自动注入）kubectl apply -f samples/bookinfo/kube/bookinfo.yaml# 创建gatewayistioctl create -f samples/bookinfo/routing/bookinfo-gateway.yaml# 查看状态kubectl get serviceskubectl get podsistioctl get gateway 访问测试123456789101112131415# 命令行访问测试export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='&#123;.spec.ports[?(@.name=="http")].nodePort&#125;')NODE_NAME=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')NODE_IP=$(ping -c 1 $NODE_NAME | grep PING | awk '&#123;print $3&#125;' | tr -d '()')export GATEWAY_URL=$NODE_IP:$INGRESS_PORTecho $GATEWAY_URLcurl -o /dev/null -s -w "%&#123;http_code&#125;\n" http://$&#123;GATEWAY_URL&#125;/productpage# 浏览器访问测试echo "http://$&#123;GATEWAY_URL&#125;/productpage"# 使用daemonset方式部署可以使用如下方式访问# 11.11.11.112为其中一个node节点的ipcurl http://11.11.11.112/productpage 清理123456# 清理官方用例samples/bookinfo/kube/cleanup.sh# 清理istiokubectl delete -f install/kubernetes/istio-demo.yaml# kubectl delete -f install/kubernetes/istio-demo-auth.yaml 参考文档 https://istio.io/docs/setup/kubernetes/quick-start.html https://istio.io/docs/guides/bookinfo.html https://istio.io/docs/setup/kubernetes/sidecar-injection.html#automatic-sidecar-injection]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>service mesh</tag>
        <tag>istio</tag>
        <tag>k8s</tag>
        <tag>microservice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kata-containers安装配置]]></title>
    <url>%2Fposts%2F29%2F</url>
    <content type="text"><![CDATA[简介kata-containers是新的虚拟机实现，可以实现和现在容器生态无缝连接，与时下最流行的容器编排工具k8s完美结合，提供容器的快速启动，和虚拟机的安全隔离，与docker技术相比，容器之间不共用内核，使得隔离性更好。 kata与docker对比图 与传统容器对比 架构图 安装本实验在centos7环境下 安装docker123yum -y install yum-utilsyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum -y install docker-ce 安装kata-containers12345source /etc/os-releaseecho $VERSION_IDVERSION_ID=$VERSION_ID yum-config-manager --add-repo \"http://download.opensuse.org/repositories/home:/katacontainers:/release/CentOS_$&#123;VERSION_ID&#125;/home:katacontainers:release.repo"yum -y install kata-runtime kata-proxy kata-shim 配置docker123456mkdir -p /etc/systemd/system/docker.service.d/cat &lt;&lt;EOF | sudo tee /etc/systemd/system/docker.service.d/kata-containers.conf[Service]ExecStart=ExecStart=/usr/bin/dockerd -D --add-runtime kata-runtime=/usr/bin/kata-runtime --default-runtime=kata-runtimeEOF 启动docker12systemctl daemon-reloadsystemctl restart docker 查看机器是否支持kata-containers1234567kata-runtime kata-check# 经过测试使用virtualbox创建的虚拟机来进行实验都不能成功# 因为virtualbox创建的虚拟机，不允许在虚拟机里再使用虚拟化# 而kata-containers需要使用虚拟化# 虽然使用vmware创建的虚拟机使用些步骤检查时，仍然会报部分错误# 但是依然可以成功启动容器 使用测试12345678910# 启动docker run -d busybox sh -c 'sleep 9999999'# 查看docker ps# 查看qemups -ef | grep qemu# 可以看到启动了一个轻量的虚拟机]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7使用kubeadm配置高可用k8s集群的另一种方式]]></title>
    <url>%2Fposts%2F28%2F</url>
    <content type="text"><![CDATA[简介使用kubeadm配置多master节点，实现高可用。 安装实验环境说明实验架构12345678lab1: etcd master keepalived 11.11.11.111lab2: etcd master keepalived 11.11.11.112lab3: etcd master keepalived 11.11.11.113lab4: node 11.11.11.114lab5: node 11.11.11.115lab6: node 11.11.11.116vip: 11.11.11.110 实验使用的Vagrantfile123456789101112131415161718192021# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..6).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "2048"] end end endend 在所有机器上安装kubeadm参考之前的文章《centos7安装kubeadm》 配置所有节点的kubelet12345678910# 配置kubelet使用国内可用镜像# 修改/etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 添加如下配置 Environment="KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-shanghai.aliyuncs.com/gcr-k8s/pause-amd64:3.0"# 使用命令sed -i '/ExecStart=$/i Environment="KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-shanghai.aliyuncs.com/gcr-k8s/pause-amd64:3.0"' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 重新载入配置systemctl daemon-reload 配置hosts12345678cat &gt;&gt;/etc/hosts&lt;&lt;EOF11.11.11.111 lab111.11.11.112 lab211.11.11.113 lab311.11.11.114 lab411.11.11.115 lab511.11.11.116 lab6EOF 启动etcd集群在lab1,lab2,lab3节点上启动etcd集群 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# lab1docker stop etcd &amp;&amp; docker rm etcdrm -rf /data/etcdmkdir -p /data/etcddocker run -d \--restart always \-v /etc/etcd/ssl/certs:/etc/ssl/certs \-v /data/etcd:/var/lib/etcd \-p 2380:2380 \-p 2379:2379 \--name etcd \registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.1.12 \etcd --name=etcd0 \--advertise-client-urls=http://11.11.11.111:2379 \--listen-client-urls=http://0.0.0.0:2379 \--initial-advertise-peer-urls=http://11.11.11.111:2380 \--listen-peer-urls=http://0.0.0.0:2380 \--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \--initial-cluster=etcd0=http://11.11.11.111:2380,etcd1=http://11.11.11.112:2380,etcd2=http://11.11.11.113:2380 \--initial-cluster-state=new \--auto-tls \--peer-auto-tls \--data-dir=/var/lib/etcd# lab2docker stop etcd &amp;&amp; docker rm etcdrm -rf /data/etcdmkdir -p /data/etcddocker run -d \--restart always \-v /etc/etcd/ssl/certs:/etc/ssl/certs \-v /data/etcd:/var/lib/etcd \-p 2380:2380 \-p 2379:2379 \--name etcd \registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.1.12 \etcd --name=etcd1 \--advertise-client-urls=http://11.11.11.112:2379 \--listen-client-urls=http://0.0.0.0:2379 \--initial-advertise-peer-urls=http://11.11.11.112:2380 \--listen-peer-urls=http://0.0.0.0:2380 \--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \--initial-cluster=etcd0=http://11.11.11.111:2380,etcd1=http://11.11.11.112:2380,etcd2=http://11.11.11.113:2380 \--initial-cluster-state=new \--auto-tls \--peer-auto-tls \--data-dir=/var/lib/etcd# lab3docker stop etcd &amp;&amp; docker rm etcdrm -rf /data/etcdmkdir -p /data/etcddocker run -d \--restart always \-v /etc/etcd/ssl/certs:/etc/ssl/certs \-v /data/etcd:/var/lib/etcd \-p 2380:2380 \-p 2379:2379 \--name etcd \registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.1.12 \etcd --name=etcd2 \--advertise-client-urls=http://11.11.11.113:2379 \--listen-client-urls=http://0.0.0.0:2379 \--initial-advertise-peer-urls=http://11.11.11.113:2380 \--listen-peer-urls=http://0.0.0.0:2380 \--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \--initial-cluster=etcd0=http://11.11.11.111:2380,etcd1=http://11.11.11.112:2380,etcd2=http://11.11.11.113:2380 \--initial-cluster-state=new \--auto-tls \--peer-auto-tls \--data-dir=/var/lib/etcd# 验证查看集群docker exec -ti etcd ashetcdctl member listetcdctl cluster-healthexit 配置keepalived在3台master节点操作 1234567891011121314151617181920212223242526# 载入内核相关模块lsmod | grep ip_vsmodprobe ip_vs# 启动keepalived# eth1为本次实验11.11.11.0/24网段的所在网卡docker run --net=host --cap-add=NET_ADMIN \-e KEEPALIVED_INTERFACE=eth1 \-e KEEPALIVED_VIRTUAL_IPS="#PYTHON2BASH:['11.11.11.110']" \-e KEEPALIVED_UNICAST_PEERS="#PYTHON2BASH:['11.11.11.111','11.11.11.112','11.11.11.113']" \-e KEEPALIVED_PASSWORD=hello \--name k8s-keepalived \--restart always \-d osixia/keepalived:1.4.4# 查看日志# 会看到两个成为backup 一个成为masterdocker logs k8s-keepalived# 此时会配置 11.11.11.110 到其中一台机器# ping测试ping -c4 11.11.11.110# 如果失败后清理后，重新实验docker rm -f k8s-keepalivedip a del 11.11.11.110/32 dev eth1 在第一台master节点初始化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# 生成token# 保留token后面还要使用token=$(kubeadm token generate)echo $token# 生成配置文件# advertiseAddress 配置为VIP地址cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationkubernetesVersion: v1.10.3imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapi: advertiseAddress: 11.11.11.110apiServerExtraArgs: endpoint-reconciler-type: leasecontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16etcd: endpoints: - "http://11.11.11.111:2379" - "http://11.11.11.112:2379" - "http://11.11.11.113:2379"apiServerCertSANs:- "lab1"- "lab2"- "lab3"- "11.11.11.111"- "11.11.11.112"- "11.11.11.113"- "11.11.11.110"- "127.0.0.1"token: $tokentokenTTL: "0"featureGates: CoreDNS: trueEOF# 初始化kubeadm init --config kubeadm-master.configsystemctl enable kubelet# 保存初始化完成之后的join命令# kubeadm join 11.11.11.110:6443 --token nevmjk.iuh214fc8i0k3iue --discovery-token-ca-cert-hash sha256:0e4f738348be836ff810bce754e059054845f44f01619a37b817eba83282d80f# 配置kubectl使用mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 安装网络插件# 下载配置mkdir flannel &amp;&amp; cd flannelwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 修改配置# 此处的ip配置要与上面kubeadm的pod-network一致 net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1# 启动kubectl apply -f kube-flannel.yml# 查看kubectl get pods -n kube-systemkubectl get svc -n kube-system# 设置master允许部署应用pod，参与工作负载，现在可以部署其他系统组件# 如 dashboard, heapster, efk等kubectl taint nodes --all node-role.kubernetes.io/master- 启动其他master节点12345678910111213141516171819202122# 打包第一台master初始化之后的/etc/kubernetes/pki目录cd /etc/kubernetes &amp;&amp; tar czvf /root/pki.tgz pki/ &amp;&amp; cd ~# 上传到其他master的/etc/kubernetes目录下tar xf pki.tgz -C /etc/kubernetes/# 复制启动第一台master时的配置文件到其他master节点# 初始化kubeadm init --config kubeadm-master.configsystemctl enable kubelet# 配置kubectl使用mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 在第一台配置master节点查看kubectl get pod --all-namespaces -o wide | grep lab1kubectl get pod --all-namespaces -o wide | grep lab2kubectl get pod --all-namespaces -o wide | grep lab3kubectl get nodes -o wide 启动node节点1234# 加入master节点# 这个命令是之前初始化master完成时，输出的命令kubeadm join 11.11.11.110:6443 --token nevmjk.iuh214fc8i0k3iue --discovery-token-ca-cert-hash sha256:0e4f738348be836ff810bce754e059054845f44f01619a37b817eba83282d80fsystemctl enable kubelet 测试重建多个coredns副本12345678910111213# 删除coredns的podskubectl get pods -n kube-system -o wide | grep corednsall_coredns_pods=$(kubectl get pods -n kube-system -o wide | grep coredns | awk '&#123;print $1&#125;' | xargs)echo $all_coredns_podskubectl delete pods $all_coredns_pods -n kube-system# 修改副本数# replicas: 3# 可以修改为node节点的个数kubectl edit deploy coredns -n kube-system# 查看状态kubectl get pods -n kube-system -o wide | grep coredns 基础测试1. 启动12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# 直接使用命令测试kubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service# 使用配置文件测试cat &gt;example-nginx.yml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginxspec: replicas: 2 template: metadata: labels: app: nginx spec: restartPolicy: Always containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 livenessProbe: httpGet: path: / port: 80 initialDelaySeconds: 10 periodSeconds: 3 readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 10 periodSeconds: 3---kind: ServiceapiVersion: v1metadata: name: example-servicespec: selector: app: nginx ports: - name: http port: 80 targetPort: 80---kind: ServiceapiVersion: v1metadata: name: example-service-nodeportspec: selector: app: nginx type: NodePort ports: - name: http-nodeport port: 80 nodePort: 32223EOFkubectl apply -f example-nginx.yml 2. 查看状态1234kubectl get deploykubectl get podskubectl get svckubectl describe svc example-service 3. DNS解析12345678kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service# 如果时间过长会返回错误，可以使用如下方式再进入测试curlPod=$(kubectl get pod | grep curl | awk '&#123;print $1&#125;')kubectl exec -ti $curlPod -- sh 4. 访问测试123456# 10.96.59.56 为查看svc时获取到的clusteripcurl "10.96.59.56:80"# 32223 为查看svc时获取到的 nodeporthttp://11.11.11.114:32223/http://11.11.11.115:32223/ 3. 清理删除12kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 高可用测试任意关闭master节点测试集群是能否正常执行上一步的基础测试，查看相关信息，只关闭到只一台master，因为etcd部署在相应的master节点上，如果关闭了两台，会造成etcd不可用，进而让整个集群不可用。 12345kubectl get pod --all-namespaces -o widekubectl get pod --all-namespaces -o wide | grep lab1kubectl get pod --all-namespaces -o wide | grep lab2kubectl get pod --all-namespaces -o wide | grep lab3kubectl get nodes -o wide 注意事项 当直接把node节点关闭时，只有过了5分钟之后，上面的pod才会被检测到有问题，并迁移到其他节点 如果想快速迁移可以执行 kubectl delete node 也可以修改controller-manager的的pod-eviction-timeout参数，默认5m node-monitor-grace-period参数，默认40s 此方案和之前文章中写的高可用方案相比，缺点就是不能使用 kube-apiserver 多节点负载均衡的功能。所有对kube-apiserver的请求都只会发给一个master节点，只有当这个master节点挂掉之后，才会把所有有请求发给另外的master。 参考文档 https://www.kubernetes.org.cn/3808.html https://kubernetes.io/docs/admin/high-availability/ https://www.kubernetes.org.cn/3536.html https://github.com/indiketa/kubeadm-ha https://zhuanlan.zhihu.com/p/34740013 https://github.com/cookeem/kubeadm-ha/blob/master/README_CN.md https://blog.frognew.com/2017/04/install-etcd-cluster.html https://blog.frognew.com/2017/04/install-ha-kubernetes-1.6-cluster.html https://medium.com/@bambash/ha-kubernetes-cluster-via-kubeadm-b2133360b198 https://github.com/kubernetes/kubeadm/issues/546 https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s使用kube-router网络插件并监控流量状态]]></title>
    <url>%2Fposts%2F27%2F</url>
    <content type="text"><![CDATA[简介kube-router是一个新的k8s的网络插件，使用lvs做服务的代理及负载均衡，使用iptables来做网络的隔离策略。部署简单，只需要在每个节点部署一个daemonset即可，高性能，易维护。支持pod间通信，以及服务的代理。 安装123456789101112131415161718192021222324# 本次实验重新创建了集群，使用之前测试其他网络插件的集群环境没有成功# 可能是由于环境干扰，实验时需要注意# 创建kube-router目录下载相关文件mkdir kube-router &amp;&amp; cd kube-routerwget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yamlwget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter-all-features.yaml# 以下两种部署方式任选其一# 1. 只启用 pod网络通信，网络隔离策略 功能kubectl apply -f kubeadm-kuberouter.yaml# 2. 启用 pod网络通信，网络隔离策略，服务代理 所有功能# 删除kube-proxy和其之前配置的服务代理kubectl apply -f kubeadm-kuberouter-all-features.yamlkubectl -n kube-system delete ds kube-proxy# 在每个节点上执行docker run --privileged --net=host registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.10.2 kube-proxy --cleanup# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system 测试1234567891011121314# 启动用于测试的deploymentkubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service# dns及访问测试kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service# 清理kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 监控相关数据并可视化重新部署kube-router123456789101112131415161718192021222324252627282930313233343536373839# 修改yml文件cp kubeadm-kuberouter-all-features.yaml kubeadm-kuberouter-all-features.yaml.orivim kubeadm-kuberouter-all-features.yaml...spec: template: metadata: labels: k8s-app: kube-router tier: node annotations: scheduler.alpha.kubernetes.io/critical-pod: '' # 添加如下参数，让prometheus收集数据 prometheus.io/scrape: "true" prometheus.io/path: "/metrics" prometheus.io/port: "8080" spec: serviceAccountName: kube-router serviceAccount: kube-router containers: - name: kube-router image: cloudnativelabs/kube-router imagePullPolicy: Always args: # 添加如下参数开启metrics - --metrics-path=/metrics - --metrics-port=8080 - --run-router=true - --run-firewall=true - --run-service-proxy=true - --kubeconfig=/var/lib/kube-router/kubeconfig...# 重新部署kubectl delete ds kube-router -n kube-systemkubectl apply -f kubeadm-kuberouter-all-features.yaml# 测试获取metricscurl http://127.0.0.1:8080/metrics 部署prometheus复制如下内容到prometheus.yml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226---apiVersion: v1kind: ConfigMapmetadata: name: prometheus namespace: kube-systemdata: prometheus.yml: |- global: scrape_interval: 15s scrape_configs: # scrape config for API servers - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https # scrape config for nodes (kubelet) - job_name: 'kubernetes-nodes' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics # Scrape config for Kubelet cAdvisor. # # This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics # (those whose names begin with 'container_') have been removed from the # Kubelet metrics endpoint. This job scrapes the cAdvisor endpoint to # retrieve those metrics. # # In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor # HTTP endpoint; use "replacement: /api/v1/nodes/$&#123;1&#125;:4194/proxy/metrics" # in that case (and ensure cAdvisor's HTTP server hasn't been disabled with # the --cadvisor-port=0 Kubelet flag). # # This job is not necessary and should be removed in Kubernetes 1.6 and # earlier versions, or it will cause the metrics to be scraped twice. - job_name: 'kubernetes-cadvisor' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor # scrape config for service endpoints. - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name # Example scrape config for pods - job_name: 'kubernetes-pods' kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: pod_name---apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' labels: name: prometheus name: prometheus namespace: kube-systemspec: selector: app: prometheus type: NodePort ports: - name: prometheus protocol: TCP port: 9090---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: prometheus namespace: kube-systemspec: replicas: 1 selector: matchLabels: app: prometheus template: metadata: name: prometheus labels: app: prometheus annotations: sidecar.istio.io/inject: "false" spec: serviceAccountName: prometheus containers: - name: prometheus image: docker.io/prom/prometheus:v2.2.1 imagePullPolicy: IfNotPresent args: - '--storage.tsdb.retention=6h' - '--config.file=/etc/prometheus/prometheus.yml' ports: - name: web containerPort: 9090 volumeMounts: - name: config-volume mountPath: /etc/prometheus volumes: - name: config-volume configMap: name: prometheus---apiVersion: v1kind: ServiceAccountmetadata: name: prometheus namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: prometheusrules:- apiGroups: [""] resources: - nodes - services - endpoints - pods - nodes/proxy verbs: ["get", "list", "watch"]- apiGroups: [""] resources: - configmaps verbs: ["get"]- nonResourceURLs: ["/metrics"] verbs: ["get"]---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: prometheusroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheussubjects:- kind: ServiceAccount name: prometheus namespace: kube-system--- 部署测试 12345678910111213# 部署kubectl apply -f prometheus.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system# 访问prometheus# 输入 kube_router 关键字查找 看有无提示出现prometheusNodePort=$(kubectl get svc -n kube-system | grep prometheus | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$prometheusNodePort 部署grafana复制如下内容到grafana.yml文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647---apiVersion: v1kind: Servicemetadata: name: grafana namespace: kube-systemspec: type: NodePort ports: - port: 3000 protocol: TCP name: http selector: app: grafana---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: grafana namespace: kube-systemspec: replicas: 1 template: metadata: labels: app: grafana spec: serviceAccountName: grafana containers: - name: grafana image: grafana/grafana imagePullPolicy: IfNotPresent ports: - containerPort: 3000 volumeMounts: - mountPath: /var/lib/grafana name: grafana-data volumes: - name: grafana-data emptyDir: &#123;&#125;---apiVersion: v1kind: ServiceAccountmetadata: name: grafana namespace: kube-system--- 部署测试 12345678910111213141516# 部署kubectl apply -f grafana.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system# 访问grafanagrafanaNodePort=$(kubectl get svc -n kube-system | grep grafana | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$grafanaNodePort# 默认用户密码admin/admin 导入并查看dashboard12# 下载官方dashboard的json文件wget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/dashboard/kube-router.json 创建名为Prometheus类型也为Prometheus的数据源，连接地址为http://prometheus:9090/ 选择刚刚下载的json文件导入dashboard 查看dashboard]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[istio微服务实验之监控日志与可视化]]></title>
    <url>%2Fposts%2F26%2F</url>
    <content type="text"><![CDATA[简介本文是istio微服务实验的后续文章，实验前请先参考之前文章。 分布式调用链追踪安装12345678910111213141516171819202122232425262728# 下载yml文件mkdir jaeger &amp;&amp; cd jaegerwget https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/all-in-one/jaeger-all-in-one-template.yml# 实验环境不支持 LoadBalancer# 可以修改jaeger-all-in-one-template.yml使用nodeport# 也可以不修改，这样的会使用随机的nodeport# 启动kubectl apply -n istio-system -f jaeger-all-in-one-template.yml# 查看kubectl get pods -n istio-systemkubectl get svc -n istio-system# 多次访问之前的vue react界面并点击发射按钮# 访问jaegerNodePort=$(kubectl get svc -n istio-system | grep jaeger-query | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$jaegerNodePort# 选择 istio-ingress 可以方便查看整个调用链# 清理cd jaegerkubectl delete -n istio-system -f jaeger-all-in-one-template.yml jaeger的dashboard界面 调用链 服务树展示 日志与指标收集安装12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 安装prometheuscd /usr/local/istio# 修改支持nodeportcp install/kubernetes/addons/prometheus.yaml install/kubernetes/addons/prometheus.yaml.orivim install/kubernetes/addons/prometheus.yaml...apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' labels: name: prometheus name: prometheus namespace: istio-systemspec: selector: app: prometheus ports: - name: prometheus protocol: TCP port: 9090 # 设置使用 nodeport type: NodePort...# 部署kubectl apply -f install/kubernetes/addons/prometheus.yaml# 配置收集istioctl create -f istio/new_telemetry.yml# 多次访问之前的vue react界面并点击发射按钮# 访问web测试prometheusNodePort=$(kubectl get svc -n istio-system | grep prometheus | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$prometheusNodePort# 使用 istio_double_request_count 关键字查询# 查看日志kubectl -n istio-system logs $(kubectl -n istio-system get pods -l istio=mixer -o jsonpath='&#123;.items[0].metadata.name&#125;') mixer | grep \"instance\":\"newlog.logentry.istio-system\"# 清理kubectl delete -f install/kubernetes/addons/prometheus.yamlistioctl delete -f istio/new_telemetry.yml 收集TCP服务的指标安装12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 安装prometheuscd /usr/local/istio# 修改支持nodeportcp install/kubernetes/addons/prometheus.yaml install/kubernetes/addons/prometheus.yaml.orivim install/kubernetes/addons/prometheus.yaml...apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' labels: name: prometheus name: prometheus namespace: istio-systemspec: selector: app: prometheus ports: - name: prometheus protocol: TCP port: 9090 # 设置使用 nodeport type: NodePort...# 部署kubectl apply -f install/kubernetes/addons/prometheus.yaml# 配置收集istioctl create -f istio/tcp_telemetry.yml# 部署使用mongodb应用测试# 访问web测试prometheusNodePort=$(kubectl get svc -n istio-system | grep prometheus | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$prometheusNodePort# 使用 istio_mongo_received_bytes 关键字查询# 清理kubectl delete -f install/kubernetes/addons/prometheus.yamlistioctl delete -f istio/tcp_telemetry.yml TCP数据流图 使用grafana可视化指标安装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 安装prometheuscd /usr/local/istio# 修改支持nodeportcp install/kubernetes/addons/prometheus.yaml install/kubernetes/addons/prometheus.yaml.orivim install/kubernetes/addons/prometheus.yaml...apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' labels: name: prometheus name: prometheus namespace: istio-systemspec: selector: app: prometheus ports: - name: prometheus protocol: TCP port: 9090 # 设置使用 nodeport type: NodePort...cp install/kubernetes/addons/grafana.yaml install/kubernetes/addons/grafana.yaml.orivim install/kubernetes/addons/grafana.yamlkind: Servicemetadata: name: grafana namespace: istio-systemspec: # 设置使用 nodeport type: NodePort ports: - port: 3000 protocol: TCP name: http selector: app: grafana# 部署kubectl apply -f install/kubernetes/addons/prometheus.yamlkubectl apply -f install/kubernetes/addons/grafana.yaml# 访问web测试grafanaNodePort=$(kubectl get svc -n istio-system | grep grafana | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$grafanaNodePort# 压力测试查看图表# 创建测试用的fortiokubectl apply -f &lt;(istioctl kube-inject -f istio/fortio-deploy.yaml)# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-python/env# 加大压力测试kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 20 -t 100s -loglevel Warning http://service-python/envkubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 50 -t 100s -loglevel Warning http://service-go/env# 清理kubectl delete -f install/kubernetes/addons/prometheus.yamlkubectl delete -f install/kubernetes/addons/grafana.yamlkubectl delete -f istio/fortio-deploy.yaml service mesh 数据监控展示 pilot数据监控展示 生成服务树安装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 修改使用nodeportcd /usr/local/istiocp install/kubernetes/addons/servicegraph.yaml install/kubernetes/addons/servicegraph.yaml.orivim install/kubernetes/addons/servicegraph.yaml...apiVersion: v1kind: Servicemetadata: name: servicegraph namespace: istio-systemspec: # 设置使用 nodeport type: NodePort ports: - name: http port: 8088 selector: app: servicegraph...# 安装prometheuscd /usr/local/istio# 修改支持nodeportcp install/kubernetes/addons/prometheus.yaml install/kubernetes/addons/prometheus.yaml.orivim install/kubernetes/addons/prometheus.yaml...apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' labels: name: prometheus name: prometheus namespace: istio-systemspec: selector: app: prometheus ports: - name: prometheus protocol: TCP port: 9090 # 设置使用 nodeport type: NodePort...# 部署kubectl apply -f install/kubernetes/addons/prometheus.yamlkubectl apply -f install/kubernetes/addons/servicegraph.yaml# 多次访问之前的vue react界面并点击发射按钮# 访问web测试servicegraphNodePort=$(kubectl get svc -n istio-system | grep servicegraph | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$servicegraphNodePort/force/forcegraph.html# 可使用url# /force/forcegraph.html# /dotviz# /dotgraph# /d3graph# /graph# 清理kubectl delete -f install/kubernetes/addons/prometheus.yamlkubectl delete -f install/kubernetes/addons/servicegraph.yaml 服务树 使用Fluentd收集日志安装1234567891011121314151617# 安装efkkubectl apply -f istio/logging-stack.yml# 配置istio使用efkistioctl create -f istio/fluentd-istio.yml# 多次访问之前的vue react界面并点击发射按钮# 访问web测试kibanaNodePort=$(kubectl get svc -n istio-system | grep kibana | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$kibanaNodePort# 清理kubectl delete -f istio/logging-stack.ymlistio delete -f istio/fluentd-istio.yml]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>service mesh</tag>
        <tag>istio</tag>
        <tag>k8s</tag>
        <tag>microservice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[istio微服务实验]]></title>
    <url>%2Fposts%2F25%2F</url>
    <content type="text"><![CDATA[简介本实验通过在k8s上部署istio，实现微服务的基础功能。其中会涉及到服务的限流，超时，熔断，降级，流量分隔，A/B测试等功能。实验之前需要安装k8s和istio，请参考之前文章。注意开启istio的自动注入功能。本实验的服务间调用关系如下： 本实验采用时下流行的前后端分离模式 前端项目基于vue/react实现 前端调用python实现的API接口 python服务调用后端node实现的服务和lua实现的服务 node服务调用go实现的服务 —-&gt;service-js —-&gt;service-python —-&gt;service-lua —-&gt;service-node —-&gt;service-go 本实验使用的语言技术栈： vue/react python2/3 node8/10 openresty1.11 /1.13 go1.10/1.9 架构图如下： 下载实验仓库1git clone https://github.com/mgxian/istio-test 部署服务1234567891011cd istio-testkubectl apply -f service/go/v1/go-v1.ymlkubectl apply -f service/go/v2/go-v2.ymlkubectl apply -f service/python/v1/python-v1.ymlkubectl apply -f service/python/v2/python-v2.ymlkubectl apply -f service/js/v1/js-v1.ymlkubectl apply -f service/js/v2/js-v2.ymlkubectl apply -f service/node/v1/node-v1.ymlkubectl apply -f service/node/v2/node-v2.ymlkubectl apply -f service/lua/v1/lua-v1.ymlkubectl apply -f service/lua/v2/lua-v2.yml 暴露服务1234567# 使用istio提供的ingress功能# 暴露js和python服务让k8s集群外部访问kubectl apply -f istio/ingress-python.ymlkubectl apply -f istio/ingress-js.yml# 查看kubectl get ingress 测试访问12345678910# 配置hosts解析# 11.11.11.112为其中一个node的ip11.11.11.112 istio-test.will# 使用curlcurl -I istio-test.willcurl -s istio-test.will | egrep "vue|React"# 此时如果作用浏览器，可能会出会页面显示不正常的情况。# 因为此时请求会轮流分发到后端js服务的v1/v2版本，因此css/js并不能正常加载 流量管理根据请求的信息，把流量路由到服务的不同版本。实验过程如果没有达到预期效果，很有可能是因为存在路由规则冲突，而且没有设置优先级，可以先删除之前设置的路由规则或者把优先级设置高一点。 把所有流量导向v1版本1234567891011121314151617# 创建路由规则istioctl create -f istio/route-rule-all-v1.yml# 查看路由规则istioctl get routerule# 访问浏览器测试http://istio-test.will/# 此时你会看到react app的界面# 点击发射按钮，会发送ajax请求到python服务# 由于把所有流量都导向了v1版本# 多次点击发射按钮会得到一样的内容# react-----&gt;Python2.7.15-----&gt;Gogo1.9.6# 清除路由规则istioctl delete -f istio/route-rule-all-v1.yml 根据请求把流量导向不同版本（A/B测试）123456789101112131415# 创建路由规则# 根据浏览器的不同返回不同内容istioctl create -f istio/route-rule-js-by-agent.yml# 使用访问浏览器# 如果你用chrome浏览器你会看到react app的界面# 如果你用firefox浏览器你会看到vue app的界面# 多次点击发射按钮，会获取到不同的内容# 根据前端app不同使用不同版本的python服务istioctl create -f istio/route-rule-python-by-header.yml# 此步骤创建的第一个路由规则保留不删除，为下面做实验提供方便istioctl delete -f istio/route-rule-python-by-header.yml 根据源服务把流量导向不同版本1234567891011# 创建路由规则istioctl create -f istio/route-rule-go-by-source.yml# 此时规则如下# 所有chrome浏览器都走v1版本服务# 所有firefox浏览器都走v2版本服务# react-----&gt;Python2.7.15-----&gt;Gogo1.9.6# vue-----&gt;Python3.6.5-----&gt;Gogo1.10.2# 清除路由规则istioctl delete -f istio/route-rule-go-by-source.yml 指定权重进行流量分隔123456789# 指定权重把流量分隔# 25%流量路由到v1版本# 75%流量路由到v2版本# 创建路由规则istioctl create -f istio/route-rule-go-v1-v2.yaml# 清除路由规则istioctl delete -f istio/route-rule-go-v1-v2.yaml 集群内访问公开服务1234567891011121314151617181920212223242526272829# 默认情况下，启用了istio的服务是无法访问外部url的# 如果需要访问外部url，需要使用egress进行配置# egress同样支持设置路由规则# httpistioctl create -f istio/egress-rule-http-bin.yml# tcpistioctl create -f istio/egress-rule-tcp-wikipedia.yml# 查看istioctl get egressrule# 测试# 使用exec进入作为测试源使用的podkubectl apply -f istio/sleep.yamlkubectl get podsexport SOURCE_POD=$(kubectl get pod -l app=sleep -o jsonpath=&#123;.items..metadata.name&#125;)kubectl exec -it $SOURCE_POD -c sleep bash# http测试curl http://httpbin.org/headers# tcp测试curl -o /dev/null -s -w "%&#123;http_code&#125;\n" https://www.wikipedia.orgcurl -s https://en.wikipedia.org/wiki/Main_Page | grep articlecount | grep 'Special:Statistics'# 清理istioctl delete -f istio/egress-rule-http-bin.ymlistioctl delete -f istio/egress-rule-tcp-wikipedia.ymlkubectl delete -f istio/sleep.yaml 故障管理 调用超时设置和重试设置 故障注入，模拟服务故障 设置超时时间与模拟服务超时故障12345678910111213# 设置python服务超时时间istioctl create -f istio/route-rule-python-timeout.yml# 模拟go服务超时故障istioctl create -f istio/route-rule-go-delay.yml# 使用浏览器访问并打开调试面板查看网络标签（按F12键）# 多次点击发射按钮观察响应时间# 会看到平均50%的请求会返回504超时# 清除路由规则istioctl delete -f istio/route-rule-python-timeout.ymlistioctl delete -f istio/route-rule-go-delay.yml 设置重试与模拟服务500故障12345678910111213# 设置python服务超时时间istioctl create -f istio/route-rule-python-retry.yml# 模拟go服务超时故障istioctl create -f istio/route-rule-go-abort.yml# 使用浏览器访问并打开调试面板查看网络标签（按F12键）# 多次点击发射按钮观察响应时间# 会看到部分请求会返回500错误# 清除路由规则istioctl delete -f istio/route-rule-python-retry.ymlistioctl delete -f istio/route-rule-go-abort.yml 超时和服务故障模拟配合使用1234567891011# 所有请求延迟5秒钟，然后失败其中的10％... route: - labels: version: v1 httpFault: delay: fixedDelay: 5s abort: percent: 10 httpStatus: 400 熔断器1234567891011121314151617181920212223242526272829303132333435# 熔断器规则需要应用到路由规则上# 需要先配置至少一个路由规则# 设置路由规则istioctl create -f istio/route-rule-go-default.yml# 设置熔断规则istioctl create -f istio/route-rule-go-cb.yml# 查看规则istioctl get destinationpolicy# 创建测试用的fortiokubectl apply -f &lt;(istioctl kube-inject --debug -f istio/fortio-deploy.yaml)# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-go/env# 测试熔断 2并发kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -c 2 -qps 0 -n 20 -loglevel Warning http://service-go/env# 测试熔断 3并发kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -c 3 -qps 0 -n 20 -loglevel Warning http://service-go/env# 增加并发会看到失败的请求占比增高# 查看状态# upstream_rq_pending_overflow 表示被熔断的请求数kubectl exec -it $FORTIO_POD -c istio-proxy -- sh -c 'curl localhost:15000/stats' | grep service-go | grep pending# 清理kubectl delete -f istio/fortio-deploy.yamlistioctl delete -f istio/route-rule-go-default.ymlistioctl delete -f istio/route-rule-go-cb.yml 限流动态设置服务qps https://github.com/istio/istio/blob/master/samples/bookinfo/kube/mixer-rule-ratings-ratelimit.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 创建service-python默认路由# 经测试，一定要配置路由规则，否则无法完成限流# 所以极有可能限流是配置在路由规则上的# 在路由时进行限流统计istioctl create -f istio/route-rule-python-default.yml# 配置一个速率限制的memquota适配器# 默认设置500qpsistioctl create -f istio/ratelimit-handler.yaml# 配置速率限制实例和规则istioctl create -f istio/ratelimit-rule-service-go.yaml# 查看kubectl get memquota -n istio-systemkubectl get quota -n istio-systemkubectl get rule -n istio-systemkubectl get quotaspec -n istio-systemkubectl get quotaspecbinding -n istio-system# 创建测试用的fortiokubectl apply -f &lt;(istioctl kube-inject -f istio/fortio-deploy.yaml)# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-python/env# 测试# 会出现部分请求不正常# python 返回 code 500# go 返回 code 429kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 20 -n 100 -loglevel Warning http://service-python/envkubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 50 -n 100 -loglevel Warning http://service-go/env# 清理istioctl delete -f istio/route-rule-python-default.ymlistioctl delete -f istio/ratelimit-handler.yamlistioctl delete -f istio/ratelimit-rule-service-go.yamlkubectl delete -f istio/fortio-deploy.yaml# 带条件的速率限制apiVersion: config.istio.io/v1alpha2kind: rulemetadata: name: quota namespace: istio-systemspec: match: source.namespace != destination.namespace actions: - handler: handler.memquota instances: - requestcount.quota 流量镜像复制服务的流量到别一个镜像服务，一般用于线上新上服务的测试。 12345678910111213141516171819202122232425262728# 创建默认策略# 默认所有流量路由到v1istioctl create -f istio/route-rule-go-default.yml# 创建测试用的fortiokubectl apply -f &lt;(istioctl kube-inject -f istio/fortio-deploy.yaml)# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-go/env# 查看v1的日志kubectl logs -f $(kubectl get pods | grep service-go-v1 | awk '&#123;print $1&#125;'| head -n 1) -c service-go# 查看v2的日志# 再开一个终端查看日志kubectl logs -f $(kubectl get pods | grep service-go-v2 | awk '&#123;print $1&#125;'| head -n 1) -c service-go# 创建镜像规则istioctl create -f istio/route-rule-go-mirror.yml# 测试多次访问kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -c 10 -qps 0 -t 10s -loglevel Warning http://service-go/env# 清理kubectl delete -f istio/fortio-deploy.yamlistioctl delete -f istio/route-rule-go-default.ymlistioctl delete -f istio/route-rule-go-mirror.yml 清理12345678910111213141516# 删除相关deploy和svckubectl delete -f service/go/v1/go-v1.ymlkubectl delete -f service/go/v2/go-v2.ymlkubectl delete -f service/python/v1/python-v1.ymlkubectl delete -f service/python/v2/python-v2.ymlkubectl delete -f service/js/v1/js-v1.ymlkubectl delete -f service/js/v2/js-v2.ymlkubectl delete -f service/node/v1/node-v1.ymlkubectl delete -f service/node/v2/node-v2.ymlkubectl delete -f service/lua/v1/lua-v1.ymlkubectl delete -f service/lua/v2/lua-v2.yml# 清除路由规则kubectl delete -f istio/ingress-python.ymlkubectl delete -f istio/ingress-js.ymlistioctl delete routerule $(istioctl get routerule | grep RouteRule | awk '&#123;print $1&#125;') 参考文档 http://istio.doczh.cn https://istio.io/docs https://istio.io/docs/reference/config/istio.networking.v1alpha3.html https://istio.io/docs/reference/config/istio.routing.v1alpha1.html]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>service mesh</tag>
        <tag>istio</tag>
        <tag>k8s</tag>
        <tag>microservice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[istio安装测试]]></title>
    <url>%2Fposts%2F24%2F</url>
    <content type="text"><![CDATA[简介istio是一个service mesh开源实现，由Google/IBM/Lyft共同开发。与之类似的还有conduit，但是功能不如istio丰富稳定。架构图如下： 安装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# 去下面的地址下载压缩包# https://github.com/istio/istio/releaseswget https://github.com/istio/istio/releases/download/0.7.1/istio-0.7.1-linux.tar.gztar xf istio-0.7.1-linux.tar.gz# 使用官方的安装脚本安装curl -L https://git.io/getLatestIstio | sh -# 安装配置环境变量mv istio-0.7.1 /usr/local/ln -sv /usr/local/istio-0.7.1 /usr/local/istioecho 'export PATH=/usr/local/istio/bin:$PATH' &gt; /etc/profile.d/istio.shsource /etc/profile.d/istio.shistioctl version# 如果环境不是云环境，不支持LoadBalancer# 作如下修改，使得 ingress 监听在80和443端口# 修改 Istio ingress 使用 NodePort# 修改使用主机端口映射# 使用此修改版本之后，每台机器只能运行单个实例# 大概在1548-1590行左右cd /usr/local/istiocp install/kubernetes/istio.yaml install/kubernetes/istio.yaml.orivim install/kubernetes/istio.yaml...################################# Istio ingress################################apiVersion: v1kind: Servicemetadata: name: istio-ingress namespace: istio-system labels: istio: ingressspec: #type: LoadBalancer # 使用NodePort方式 type: NodePort ports: - port: 80# nodePort: 32000 name: http - port: 443 name: https selector: istio: ingress---apiVersion: extensions/v1beta1#kind: Deployment# 使用DaemonSet部署方式kind: DaemonSetmetadata: name: istio-ingress namespace: istio-systemspec: #DaemonSet不支持replicas #replicas: 1 template:... imagePullPolicy: IfNotPresent ports: - containerPort: 80 #主机80端口映射 hostPort: 80 - containerPort: 443 #主机443端口映射 hostPort: 443...# 以下两种选择一种安装方式# 安装不使用认证（不使用tls）kubectl apply -f install/kubernetes/istio.yaml# 安装使用认证（使用tls）kubectl apply -f install/kubernetes/istio-auth.yaml# 查看状态kubectl get svc -n istio-systemkubectl get pods -n istio-system 启用自动注入 sidecar 不开启自动注入部署应用需要使用如下方式的命令 kubectl apply -f &lt;(istioctl kube-inject -f samples/bookinfo/kube/bookinfo.yaml) 开启自动注入后，使用正常命令即可部署应用 kubectl apply -f samples/bookinfo/kube/bookinfo.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# k8s 1.9 版本以后才能使用# 查看是否支持kubectl api-versions | grep admissionregistration# 除了要满足以上条件外还需要检查kube-apiserver启动的参数# k8s 1.9 版本要确保 --admission-control 里有 MutatingAdmissionWebhook,ValidatingAdmissionWebhook# k8s 1.9 之后的版本要确保 --enable-admission-plugins 里有MutatingAdmissionWebhook,ValidatingAdmissionWebhook# 生成所需要的证书./install/kubernetes/webhook-create-signed-cert.sh \ --service istio-sidecar-injector \ --namespace istio-system \ --secret sidecar-injector-certs # 创建配置configmapkubectl apply -f install/kubernetes/istio-sidecar-injector-configmap-release.yaml# 生成相关yamlcat install/kubernetes/istio-sidecar-injector.yaml | \ ./install/kubernetes/webhook-patch-ca-bundle.sh &gt; \ install/kubernetes/istio-sidecar-injector-with-ca-bundle.yaml # 安装webhookkubectl apply -f install/kubernetes/istio-sidecar-injector-with-ca-bundle.yaml# 查看kubectl -n istio-system get deployment -listio=sidecar-injectorkubectl get namespace -L istio-injection# 测试自动注入# 创建kubectl apply -f samples/sleep/sleep.yaml kubectl get deployment -o widekubectl get pod# 设置 default namespace 开启自动注入kubectl label namespace default istio-injection=enabledkubectl get namespace -L istio-injection# 删除创建的pod，等待重建kubectl delete pod $(kubectl get pod | grep sleep | cut -d ' ' -f 1)# 查看重建后的pod# 查看是否有istio-proxy容器kubectl get podkubectl describe pod $(kubectl get pod | grep sleep | cut -d ' ' -f 1)# 清理kubectl delete -f samples/sleep/sleep.yaml # 关闭自动注入kubectl label namespace default istio-injection-# 关闭部分pod的自动注入功能... template: metadata: annotations: sidecar.istio.io/inject: "false"... 部署官方测试用例12345678910# 启动（未开启自动注入）kubectl apply -f &lt;(istioctl kube-inject -f samples/bookinfo/kube/bookinfo.yaml)# 启动（已开启自动注入）kubectl apply -f samples/bookinfo/kube/bookinfo.yaml# 查看状态kubectl get serviceskubectl get podskubectl get ingress -o wide 访问测试12345678910111213# 命令行访问测试GATEWAY_URL=$(kubectl get po -l istio=ingress -n istio-system -o 'jsonpath=&#123;.items[0].status.hostIP&#125;'):$(kubectl get svc istio-ingress -n istio-system -o 'jsonpath=&#123;.spec.ports[0].nodePort&#125;')curl -o /dev/null -s -w "%&#123;http_code&#125;\n" http://$&#123;GATEWAY_URL&#125;/productpage# 浏览器访问测试NODE_PORT=$(kubectl get svc istio-ingress -n istio-system -o jsonpath='&#123;.spec.ports[0].nodePort&#125;')NODE_IP='11.11.11.112'echo http://$&#123;NODE_IP&#125;:$&#123;NODE_PORT&#125;/productpage# 使用daemonset方式部署可以使用如下方式访问# 11.11.11.112为其中一个node节点的ipcurl http://11.11.11.112/productpage 清理123456# 清理官方用例samples/bookinfo/kube/cleanup.sh# 清理istiokubectl delete -f install/kubernetes/istio.yaml# kubectl delete -f install/kubernetes/istio-auth.yaml 参考文档 https://istio.io/docs/setup/kubernetes/quick-start.html https://istio.io/docs/guides/bookinfo.html https://istio.io/docs/setup/kubernetes/sidecar-injection.html#automatic-sidecar-injection]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>service mesh</tag>
        <tag>istio</tag>
        <tag>k8s</tag>
        <tag>microservice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s安装traefik作为ingress]]></title>
    <url>%2Fposts%2F23%2F</url>
    <content type="text"><![CDATA[简介traefik 是一个前端负载均衡器，对于微服务架构尤其是 kubernetes 等编排工具具有良好的支持；同 nginx 等相比，traefik 能够自动感知后端容器变化，从而实现自动服务发现。 traefik部署在k8s上分为daemonset和deployment两种方式各有优缺点： daemonset 能确定有哪些node在运行traefik，所以可以确定的知道后端ip，但是不能方便的伸缩 deployment 可以更方便的伸缩，但是不能确定有哪些node在运行traefik所以不能确定的知道后端ip 一般部署两种不同类型的traefik: 面向内部(internal)服务的traefik，建议可以使用deployment的方式 面向外部(external)服务的traefik，建议可以使用daemonset的方式 建议使用traffic-type标签 traffic-type: external traffic-type: internal traefik相应地使用labelSelector traffic-type=internal traffic-type=external 安装12345678910111213141516171819202122232425262728293031mkdir traefik &amp;&amp; cd traefikwget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-rbac.yaml# 配置rbackubectl apply -f traefik-rbac.yaml# 以下两种方式选择一个# 80 提供正常服务，8080 是其自带的 UI 界面# 以daemonset方式启动traefik# 会在所有node节点启动一个traefik并监听在80端口# master节点不会启动traefikwget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-ds.yamlkubectl apply -f traefik-ds.yaml# 以deployment方式启动traefikwget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-deployment.yamlkubectl apply -f traefik-deployment.yaml# 查看状态kubectl get pods -n kube-system# 访问测试，如果有响应说明安装正确# 应该返回404# 如果以daemonset方式启动traefik使用如下方式验证# 11.11.11.112为任何一个node节点的ipcurl 11.11.11.112# 如果以deployment方式启动traefik# 访问node:nodeport或者集群ip验证 部署Træfik Web UI12345678910wget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/ui.yamlkubectl apply -f ui.yaml# 访问webui# 需要先配置host# 11.11.11.112为任何一个node节点的ip11.11.11.112 traefik-ui.minikube# 浏览器访问如下地址http://traefik-ui.minikube/ 使用basic验证1234567891011121314151617181920212223242526272829303132# 生成加密密码，如果没有安装htpasswd可以在线生成# https://tool.lu/htpasswd/htpasswd -c ./auth myusernamecat authmyusername:$apr1$78Jyn/1K$ERHKVRPPlzAX8eBtLuvRZ0# 从密码文件创建secret# monitoring必须和ingress rule处于同一个namespace kubectl create secret generic mysecret --from-file auth --namespace=monitoring# 创建ingresscat &gt;prometheus-ingress.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Ingressmetadata: name: prometheus-dashboard namespace: monitoring annotations: kubernetes.io/ingress.class: traefik ingress.kubernetes.io/auth-type: "basic" ingress.kubernetes.io/auth-secret: "mysecret"spec: rules: - host: dashboard.prometheus.example.com http: paths: - backend: serviceName: prometheus servicePort: 9090EOFkubectl create -f prometheus-ingress.yaml -n monitoring 官方实例1. 根据域名(host)路由12345678910111213141516171819202122232425# deploymentwget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/cheese-deployments.yamlkubectl apply -f cheese-deployments.yaml# servicewget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/cheese-services.yamlkubectl apply -f cheese-services.yaml# ingresswget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/cheese-ingress.yamlkubectl apply -f cheese-ingress.yaml# 查看状态kubectl get podskubectl get svckubectl get ingress# 测试# 配置hosts11.11.11.112 stilton.minikube cheddar.minikube wensleydale.minikube# 浏览器访问测试http://stilton.minikube/http://cheddar.minikube/http://wensleydale.minikube/ 2. 根据路径(path)路由123456789101112# 使用新的ingresswget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/cheeses-ingress.yamlkubectl apply -f cheeses-ingress.yaml# 测试# 配置hosts11.11.11.112 cheeses.minikube# 浏览器访问测试http://cheeses.minikube/stilton/http://cheeses.minikube/cheddar/http://cheeses.minikube/wensleydale/ 3. 指定路由优先级123456789101112131415161718192021222324252627282930apiVersion: extensions/v1beta1kind: Ingressmetadata: name: wildcard-cheeses annotations: traefik.frontend.priority: "1"spec: rules: - host: *.minikube http: paths: - path: / backend: serviceName: stilton servicePort: httpkind: Ingressmetadata: name: specific-cheeses annotations: traefik.frontend.priority: "2"spec: rules: - host: specific.minikube http: paths: - path: / backend: serviceName: stilton servicePort: http 参考文档 https://docs.traefik.io/user-guide/kubernetes/ https://mritd.me/2016/12/06/try-traefik-on-kubernetes/]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>ingress</tag>
        <tag>traefik</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10安装配置minikube]]></title>
    <url>%2Fposts%2F22%2F</url>
    <content type="text"><![CDATA[简介minukube是一个可以让开发人员在本地环境运行k8s的软件，便于开发人员在本地测试运行k8s 相关下载链接 链接：https://pan.baidu.com/s/10dJLJiUnXsZcA5c6HwWVqQ 密码：qh6k 安装1. 安装minikube 12345678# 到 minikube release 页面下载 minikube 安装文件https://github.com/kubernetes/minikube/releases/download/v0.26.1/minikube-installer.exe# 直接双击安装# 设置环境变量# 如果不设置，默认会在C盘中安装相关文件MINIKUBE_HOME=D:\minikube 2. 安装kubectl 12345678# 下载 如果不能正常下载 可能需要翻墙curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/windows/amd64/kubectl.exe# 把kubectl所在目录放在系统path中# 也可以放在minikube的安装目录中# 查看版本kubectl version 3.安装virtualbox 官方下载virtualbox对应平台的软件包安装 启动12345678910111213141516171819# 获取k8s可用版本minikube get-k8s-versions# 启动之前因为需要下载minikube-iso和localkube，所以需要使用代理# 有时不需要使用代理也能下载，但是启动之后 由于需要拉取gcr.io上面的镜像# 所以仍然需要配置docker的代理# 使用minikube ssh连接到minikube主机里# 下载好相关镜像，再重新打tag为gcr.io也是一种方法# 下载存储在 MINIKUBE_HOME/.minikube/cache 目录下# MINIKUBE_HOME 如果没有设置 默认为用户的家目录 https_proxy=http://127.0.0.1:1080 minikube start \--vm-driver virtualbox \--memory 2048 --disable-driver-mounts \--registry-mirror https://tfhzn46h.mirror.aliyuncs.com \--docker-env http_proxy=http://172.16.0.10:1080 \--docker-env https_proxy=http://172.16.0.10:1080 \--docker-env no_proxy='192.168.99.0/24,.docker.io,.aliyuncs.com'# 然后根据提示操作 基本测试使用12345678910111213141516171819202122232425262728293031323334353637# 获取minikube的ipminikube ip# ssh连接到minikube主机里# 使用git-bash可能会无法正常连接minikube ssh# 官方示例kubectl run hello-minikube --image=k8s.gcr.io/echoserver:1.8 --port=8080kubectl expose deployment hello-minikube --type=NodePort# 启动nginx并创建服务kubectl run nginx --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort# 查看访问kubectl get podskubectl get svccurl $(minikube service hello-minikube --url)curl $(minikube service nginx --url)# 清理kubectl delete deployment nginx hello-minikubekubectl delete svc nginx hello-minikube# 停止删除minikube集群minikube stopminikube delete# 查看其他组件minikube addons list# 启用组件minikube addons enable heapster# 在浏览器中打开组件minikube addons open heapster 参考文档 https://kubernetes.io/docs/getting-started-guides/minikube/ https://github.com/kubernetes/minikube/blob/v0.24.1/README.md]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>minikube</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用vagrant基于官方的box制作自己的基础box]]></title>
    <url>%2Fposts%2F21%2F</url>
    <content type="text"><![CDATA[使用vagrant启动虚拟机 123456789101112131415161718mkdir base &amp;&amp; cd basecat &gt;Vagrantfile&lt;&lt;EOF# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure("2") do |config| # centos-7.4-docker-17为你想使用的基础box config.vm.box = "centos-7.4-docker-17" config.ssh.insert_key = false config.vm.provider "virtualbox" do |v| v.customize ["modifyvm", :id, "--name", "will"] end config.vm.synced_folder ".", "/vagrant", disabled: trueendEOF# 启动vagrant up 登录配置安装相关软件123456789101112131415161718192021222324# 使用xshell等ssh工具登录# 配置源# 安装配置所需要的软件# 安装完成关机前做如下清理操作# 删除网卡mac信息rm -f /etc/udev/rules.d/70-persistent-net.rules# 删除临时文件yum clean allapt-get cleanrm -rf /tmp/*rm -f /var/log/wtmp /var/log/btmp# 清除命令历史记录history -c&gt; .bash_historysudo su - vagranthistory -c&gt; .bash_history# 关机vagrant halt 制作基础box123456789101112cat &gt;Vagrantfile.base&lt;&lt;EOF# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure(2) do |config| # Disable synced folders config.vm.synced_folder ".", "/vagrant", disabled: trueendEOF# will为之前启动时设置的vm名vagrant package --base will --vagrantfile Vagrantfile.base --output will-base.box 测试12345678910111213# 添加制作完成的boxvagrant box add base will-base.box# 初始化vagrant init base# 启动vagrant up# 登录# 如果能ssh连接成功，表示基础box配置正确vagrant ssh-configvagrant ssh 参考文档 https://thornelabs.net/2013/11/11/create-a-centos-6-vagrant-base-box-from-scratch-using-virtualbox.html https://www.dravetech.com/blog/2016/01/14/vagrant_box_ios_xr.html http://blog.pangyanhan.com/posts/2015-11-10-creating-a-vagrant-base-box.html]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>vagrant</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用vagrant快速搭建linux实验环境]]></title>
    <url>%2Fposts%2F20%2F</url>
    <content type="text"><![CDATA[简介本文主要介绍如何使用vagrant配合virtualbox快速搭建实验环境。virtualbox是一个开源跨平台虚拟机管理软件，功能类似收费的vmwarevagrant是一个开源的虚拟机配置编排软件，可以在命令行快速启动管理虚拟机。 相关资源的百度云下载链接链接：https://pan.baidu.com/s/1nt_b96SEOIIWl2gIrabPpg 密码：6c3d 安装1.安装virtualbox 官方下载virtualbox对应平台的软件包安装 2.安装vagrant 官方下载vagrant对应平台的软件包安装，由于官方网站在国外，可能下载比较慢。 3.设置virtualbox虚拟机存方目录 1234# 默认情况下 virtualbox 启动虚拟机会存放在用户的家目录里# windows 下C盘可能过小，需要设置特定目录存储虚拟机VBoxManage setproperty machinefolder D:\virtualboxVBoxManage list systemproperties | grep machine 搭建实验环境1.下载导入相关box 1234# 由于需要从国外拉取box，可能会很慢。推荐使用我存储在百度云的box# 导入boxvagrant box add centos-7.4-base centos-7.4-base.boxvagrant box list 2.启动单机 1234567891011121314151617181920212223242526272829303132mkdir single &amp;&amp; cd singlecat &gt;Vagrantfile&lt;&lt;EOF# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| config.vm.box = "centos-7.4-base" config.vm.hostname = "will" config.ssh.insert_key = false # 指定CPU和内存大小 config.vm.provider "virtualbox" do |v| v.memory = 1024 v.cpus = 2 v.customize ["modifyvm", :id, "--name", "will"] end # 配置网络 config.vm.network "private_network", ip: "11.11.11.111" # config.vm.network "private_network", ip: "192.168.22.10" # 配置启动后的操作 config.vm.provision "shell", inline: &lt;&lt;-SHELL hostname SHELLendEOF# 启动vagrant up 3.启动多主机 123456789101112131415161718192021222324252627282930mkdir double &amp;&amp; cd doublecat &gt;Vagrantfile&lt;&lt;EOF# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| config.vm.define "web" do |web| web.vm.provider "virtualbox" do |v| v.customize ["modifyvm", :id, "--name", "web", "--memory", "512"] end web.vm.box = "centos-6.9" web.vm.hostname = "web" web.vm.network "private_network", ip: "11.11.11.11" end config.vm.define "db" do |db| db.vm.provider "virtualbox" do |v| v.customize ["modifyvm", :id, "--name", "db", "--memory", "512"] end db.vm.box = "centos-6.9" db.vm.hostname = "db" db.vm.network "private_network", ip: "11.11.11.22" endendEOF# 启动vagrant up 3.启动集群 12345678910111213141516171819202122232425262728mkdir cluster &amp;&amp; cd clustercat &gt;Vagrantfile&lt;&lt;EOF# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..6).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.12.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 4 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "2048"] end end endendEOF# 启动vagrant up 4.连接虚拟机 1234# 一般情况下虚拟机ssh连接用户名为 vagrant# 一般情况下都不支持 密码登录， 可以登录之后自行配置支持密码登录# 连接的sshkey存储在用户家目录下 .vagrant.d 目录下# C:\Users\will\.vagrant.d\insecure_private_key 常用命令 以下命令后面都可以接虚拟机名，只对指定虚拟机作操作 启动虚拟机 vagrant up 暂停虚拟机 vagrant suspend 关闭虚拟机 vagrant halt 删除虚拟机 vagrant destroy 存储快照 vagrant snapshot save lab1 init 恢复快照 vagrant snapshot restore lab1 init]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>vagrant</tag>
        <tag>virtualbox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7配置k8s集群使用coredns]]></title>
    <url>%2Fposts%2F19%2F</url>
    <content type="text"><![CDATA[简介CoreDNS是一个Go语言实现的链式插件DNS服务端，是CNCF成员，是一个高性能、易扩展的DNS服务端。可以很方便的部署在k8s集群中，用来代替kube-dns。 使用kubeadm初始化时指定 安装方法与《centos7使用kubeadm安装k8s集群》基本一致只需要简单修改kubeadm-master.config配置文件 123456789101112apiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationkubernetesVersion: v1.9.0imageRepository: registry.cn-shanghai.aliyuncs.com/gcr-k8setcd: image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/etcd-amd64:3.1.10api: advertiseAddress: 11.11.11.111networking: podSubnet: 10.244.0.0/16featureGates: CoreDNS: true 单独部署coredns 不依赖kubeadm的方式，适用于不是使用kubeadm创建的k8s集群，或者kubeadm初始化集群之后，删除了dns相关部署。 123456789101112# 在calico网络中也配置一个coredns# 10.96.0.10 为k8s官方指定的kube-dns地址mkdir coredns &amp;&amp; cd corednswget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sedwget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/deploy.shchmod +x deploy.sh./deploy.sh -i 10.96.0.10 &gt; coredns.ymlkubectl apply -f coredns.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s与各网络插件集成]]></title>
    <url>%2Fposts%2F18%2F</url>
    <content type="text"><![CDATA[通用说明 如果多次换不同网络插件实验，每次实验前先把/etc/cni/net.d/目录下文件清空 1rm -rf /etc/cni/net.d/* flannel1234567891011121314151617181920212223242526272829303132333435363738# 创建flannel目录下载相关文件mkdir flannel &amp;&amp; cd flannelwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 修改配置# 此处的ip配置要与kubeadm的pod-network参数配置的一致 net-conf.json: | &#123; "Network": "192.168.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1# 启动kubectl apply -f kube-flannel.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system calico1.配置启动etcd集群 12# 本次实验使用与k8s一个etcd集群# 生境环境建议使用单独的一套集群 2.配置启动calico 1234567891011121314151617181920# 创建calico目录下载相关文件mkdir calico &amp;&amp; cd calicowget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/rbac.yamlwget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/calico.yaml# 如果启用了RBAC（默认k8s集群启用），配置RBACkubectl apply -f rbac.yaml# 修改calico.yaml文件中名为calico-config的ConfigMap中的etcd_endpoints参数为自己的etcd集群etcd_endpoints: "http://11.11.11.111:2379,http://11.11.11.112:2379,http://11.11.11.113:2379"# 修改镜像为国内镜像sed -i 's@image: quay.io/calico/@image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/calico-@g' calico.yaml# 启动kubectl apply -f calico.yaml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system 3.参考文档 https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/calico#installing-with-the-etcd-datastore canal123456789101112131415161718192021222324252627282930313233343536373839# 创建flannel目录下载相关文件mkdir canal &amp;&amp; cd canalwget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/canal/rbac.yamlwget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/canal/canal.yaml# 修改配置# 此处的ip配置要与kubeadm的pod-network参数配置的一致 net-conf.json: | &#123; "Network": "192.168.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改calico镜像sed -i 's@image: quay.io/calico/@image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/calico-@g' canal.yaml# 修改flannel镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: [ "/opt/bin/flanneld", "--ip-masq", "--kube-subnet-mgr", "--iface=eth1" ]# 启动kubectl apply -f rbac.yamlkubectl apply -f canal.yaml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system kube-router123456789101112131415161718192021222324# 本次实验重新创建了集群，使用之前测试其他网络插件的集群环境没有成功# 可能是由于环境干扰，实验时需要注意# 创建kube-router目录下载相关文件mkdir kube-router &amp;&amp; cd kube-routerwget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yamlwget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter-all-features.yaml# 以下两种部署方式任选其一# 1. 只启用 pod网络通信，网络隔离策略 功能kubectl apply -f kubeadm-kuberouter.yaml# 2. 启用 pod网络通信，网络隔离策略，服务代理 所有功能# 删除kube-proxy和其之前配置的服务代理kubectl apply -f kubeadm-kuberouter-all-features.yamlkubectl -n kube-system delete ds kube-proxy# 在每个节点上执行docker run --privileged --net=host registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.10.2 kube-proxy --cleanup# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system romana1234567891011121314# 创建flannel目录下载相关文件mkdir romana &amp;&amp; cd romanawget https://raw.githubusercontent.com/romana/romana/master/containerize/specs/romana-kubeadm.yml# 修改镜像sed -i 's@gcr.io/@registry.cn-hangzhou.aliyuncs.com/@g' romana-kubeadm.ymlsed -i 's@quay.io/romana/@registry.cn-shanghai.aliyuncs.com/gcr-k8s/romana-@g' romana-kubeadm.yml# 启动kubectl apply -f romana-kubeadm.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system CNI-Genie12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# CNI-Genie是华为开源的网络组件，可以使k8s同时部署多个网络插件# 在k8s集群中安装calico组件# 在k8s集群中安装flannel组件# 在k8s集群中安装Genie组件mkdir CNI-Genie &amp;&amp; cd CNI-Geniewget https://raw.githubusercontent.com/Huawei-PaaS/CNI-Genie/master/conf/1.8/genie.yamlsed -i 's@image: quay.io/cnigenie/v1.5:latest@image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/cnigenie-v1.5:latest@g' genie.yamlkubectl apply -f genie.yaml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system# 测试cat &gt;nginx-calico.yml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-calico labels: app: web annotations: cni: "calico"spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80EOFcat &gt;nginx-flannel.yml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-flannel labels: app: web annotations: cni: "flannel"spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80EOFkubectl apply -f nginx-calico.ymlkubectl apply -f nginx-flannel.yml# 查看kubectl get pods -o wide# 测试网络通信kubectl exec nginx-calico -i -t -- ping -c4 1.1.1.1kubectl exec nginx-flannel -i -t -- ping -c4 1.1.1.1# 由于先启动的flannel，然后k8s创建了coredns，所以使用flannel cni的能正常使用dns# 使用calico cni无法使用正常dns# 测试dnskubectl exec nginx-calico -i -t -- ping -c4 www.baidu.comkubectl exec nginx-flannel -i -t -- ping -c4 www.baidu.com 总结 kube-router性能损失最小，时延最小，其他网络插件性能差距不大。除了flannel没有网络隔离策略，其他均支持网络隔离策略。CNI-Genie是一个可以让k8s使用多个cni网络插件的组件，暂时不支持隔离策略。 理论结果： kube-router &gt; calico &gt; canal = flannel = romana]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7使用kubeadm配置高可用k8s集群]]></title>
    <url>%2Fposts%2F17%2F</url>
    <content type="text"><![CDATA[简介使用kubeadm配置多master节点，实现高可用。 安装实验环境说明实验架构图12345678lab1: etcd master haproxy keepalived 11.11.11.111lab2: etcd master haproxy keepalived 11.11.11.112lab3: etcd master haproxy keepalived 11.11.11.113lab4: node 11.11.11.114lab5: node 11.11.11.115lab6: node 11.11.11.116vip(loadblancer ip): 11.11.11.110 实验使用的Vagrantfile123456789101112131415161718192021# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..6).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "2048"] end end endend 在所有机器上安装kubeadm参考之前的文章《centos7安装kubeadm》 配置所有节点的kubelet12345678910# 配置kubelet使用国内可用镜像# 修改/etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 添加如下配置 Environment="KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-shanghai.aliyuncs.com/gcr-k8s/pause-amd64:3.0"# 使用命令sed -i '/ExecStart=$/i Environment="KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-shanghai.aliyuncs.com/gcr-k8s/pause-amd64:3.0"' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 重新载入配置systemctl daemon-reload 配置所有节点的hosts12345678cat &gt;&gt;/etc/hosts&lt;&lt;EOF11.11.11.111 lab111.11.11.112 lab211.11.11.113 lab311.11.11.114 lab411.11.11.115 lab511.11.11.116 lab6EOF 启动etcd集群在lab1,lab2,lab3节点上启动etcd集群 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# lab1docker stop etcd &amp;&amp; docker rm etcdrm -rf /data/etcdmkdir -p /data/etcddocker run -d \--restart always \-v /etc/etcd/ssl/certs:/etc/ssl/certs \-v /data/etcd:/var/lib/etcd \-p 2380:2380 \-p 2379:2379 \--name etcd \registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.1.12 \etcd --name=etcd0 \--advertise-client-urls=http://11.11.11.111:2379 \--listen-client-urls=http://0.0.0.0:2379 \--initial-advertise-peer-urls=http://11.11.11.111:2380 \--listen-peer-urls=http://0.0.0.0:2380 \--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \--initial-cluster=etcd0=http://11.11.11.111:2380,etcd1=http://11.11.11.112:2380,etcd2=http://11.11.11.113:2380 \--initial-cluster-state=new \--auto-tls \--peer-auto-tls \--data-dir=/var/lib/etcd# lab2docker stop etcd &amp;&amp; docker rm etcdrm -rf /data/etcdmkdir -p /data/etcddocker run -d \--restart always \-v /etc/etcd/ssl/certs:/etc/ssl/certs \-v /data/etcd:/var/lib/etcd \-p 2380:2380 \-p 2379:2379 \--name etcd \registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.1.12 \etcd --name=etcd1 \--advertise-client-urls=http://11.11.11.112:2379 \--listen-client-urls=http://0.0.0.0:2379 \--initial-advertise-peer-urls=http://11.11.11.112:2380 \--listen-peer-urls=http://0.0.0.0:2380 \--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \--initial-cluster=etcd0=http://11.11.11.111:2380,etcd1=http://11.11.11.112:2380,etcd2=http://11.11.11.113:2380 \--initial-cluster-state=new \--auto-tls \--peer-auto-tls \--data-dir=/var/lib/etcd# lab3docker stop etcd &amp;&amp; docker rm etcdrm -rf /data/etcdmkdir -p /data/etcddocker run -d \--restart always \-v /etc/etcd/ssl/certs:/etc/ssl/certs \-v /data/etcd:/var/lib/etcd \-p 2380:2380 \-p 2379:2379 \--name etcd \registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.1.12 \etcd --name=etcd2 \--advertise-client-urls=http://11.11.11.113:2379 \--listen-client-urls=http://0.0.0.0:2379 \--initial-advertise-peer-urls=http://11.11.11.113:2380 \--listen-peer-urls=http://0.0.0.0:2380 \--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \--initial-cluster=etcd0=http://11.11.11.111:2380,etcd1=http://11.11.11.112:2380,etcd2=http://11.11.11.113:2380 \--initial-cluster-state=new \--auto-tls \--peer-auto-tls \--data-dir=/var/lib/etcd# 验证查看集群docker exec -ti etcd ashetcdctl member listetcdctl cluster-healthexit 在第一台master节点初始化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106# 生成token# 保留token后面还要使用token=$(kubeadm token generate)echo $token# 生成配置文件cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationkubernetesVersion: v1.10.1#imageRepository: registry.cn-shanghai.aliyuncs.com/gcr-k8simageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapi: advertiseAddress: 11.11.11.111apiServerExtraArgs: endpoint-reconciler-type: leasecontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 192.168.0.0/16etcd: endpoints: - "http://11.11.11.111:2379" - "http://11.11.11.112:2379" - "http://11.11.11.113:2379"apiServerCertSANs:- "lab1"- "lab2"- "lab3"- "11.11.11.111"- "11.11.11.112"- "11.11.11.113"- "11.11.11.110"- "127.0.0.1"token: $tokentokenTTL: "0"featureGates: CoreDNS: trueEOF# 初始化kubeadm init --config kubeadm-master.configsystemctl enable kubelet# 保存初始化完成之后的join命令# 如果丢失可以使用命令"kubeadm token list"获取# kubeadm join 11.11.11.111:6443 --token nevmjk.iuh214fc8i0k3iue --discovery-token-ca-cert-hash sha256:0e4f738348be836ff810bce754e059054845f44f01619a37b817eba83282d80f# 配置kubectl使用mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 安装网络插件# 下载配置mkdir flannel &amp;&amp; cd flannelwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 修改配置# 此处的ip配置要与上面kubeadm的pod-network一致 net-conf.json: | &#123; "Network": "192.168.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 启动kubectl apply -f kube-flannel.yml# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system# 设置master允许部署应用pod，参与工作负载，现在可以部署其他系统组件# 如 dashboard, heapster, efk等kubectl taint nodes --all node-role.kubernetes.io/master- 启动其他master节点1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# 打包第一台master初始化之后的/etc/kubernetes/pki目录cd /etc/kubernetes &amp;&amp; tar czvf /root/pki.tgz pki/ &amp;&amp; cd ~# 上传到其他master的/etc/kubernetes目录下tar xf pki.tgz -C /etc/kubernetes/# 删除pki目录下的apiserver.crt 和 apiserver.key文件rm -rf /etc/kubernetes/pki/&#123;apiserver.crt,apiserver.key&#125;# 生成配置文件# 使用和之前master一样的配置文件# token保持一致cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationkubernetesVersion: v1.10.1#imageRepository: registry.cn-shanghai.aliyuncs.com/gcr-k8simageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers# 注意修改IPapi: advertiseAddress: 11.11.11.112apiServerExtraArgs: endpoint-reconciler-type: leasecontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 192.168.0.0/16etcd: endpoints: - "http://11.11.11.111:2379" - "http://11.11.11.112:2379" - "http://11.11.11.113:2379"apiServerCertSANs:- lab1- lab2- lab3- "11.11.11.111"- "11.11.11.112"- "11.11.11.113"- "11.11.11.110"- "127.0.0.1"token: nevmjk.iuh214fc8i0k3iuetokenTTL: "0"featureGates: CoreDNS: trueEOF# 初始化kubeadm init --config kubeadm-master.configsystemctl enable kubelet# 查看状态kubectl get pod --all-namespaces -o wide | grep lab1kubectl get pod --all-namespaces -o wide | grep lab2kubectl get pod --all-namespaces -o wide | grep lab3kubectl get nodes -o wide 配置haproxy代理和keepalived在lab1,lab2,lab3节点上启动haproxy和keepalived 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697# 拉取haproxy镜像docker pull haproxy:1.7.8-alpinemkdir /etc/haproxycat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOFglobal log 127.0.0.1 local0 err maxconn 50000 uid 99 gid 99 #daemon nbproc 1 pidfile haproxy.piddefaults mode http log 127.0.0.1 local0 err maxconn 50000 retries 3 timeout connect 5s timeout client 30s timeout server 30s timeout check 2slisten admin_stats mode http bind 0.0.0.0:1080 log 127.0.0.1 local0 err stats refresh 30s stats uri /haproxy-status stats realm Haproxy\ Statistics stats auth will:will stats hide-version stats admin if TRUEfrontend k8s-https bind 0.0.0.0:8443 mode tcp #maxconn 50000 default_backend k8s-httpsbackend k8s-https mode tcp balance roundrobin server lab1 11.11.11.111:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab2 11.11.11.112:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab3 11.11.11.113:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3EOF# 启动haproxydocker run -d --name my-haproxy \-v /etc/haproxy:/usr/local/etc/haproxy:ro \-p 8443:8443 \-p 1080:1080 \--restart always \haproxy:1.7.8-alpine# 查看日志docker logs my-haproxy# 浏览器查看状态http://11.11.11.111:1080/haproxy-statushttp://11.11.11.112:1080/haproxy-status# 拉取keepalived镜像docker pull osixia/keepalived:1.4.4# 启动# 载入内核相关模块lsmod | grep ip_vsmodprobe ip_vs# 启动keepalived# eth1为本次实验11.11.11.0/24网段的所在网卡docker run --net=host --cap-add=NET_ADMIN \-e KEEPALIVED_INTERFACE=eth1 \-e KEEPALIVED_VIRTUAL_IPS="#PYTHON2BASH:['11.11.11.110']" \-e KEEPALIVED_UNICAST_PEERS="#PYTHON2BASH:['11.11.11.111','11.11.11.112','11.11.11.113']" \-e KEEPALIVED_PASSWORD=hello \--name k8s-keepalived \--restart always \-d osixia/keepalived:1.4.4# 查看日志# 会看到两个成为backup 一个成为masterdocker logs k8s-keepalived# 此时会配置 11.11.11.110 到其中一台机器# ping测试ping -c4 11.11.11.110# 如果失败后清理后，重新实验docker rm -f k8s-keepalivedip a del 11.11.11.110/32 dev eth1# 修改~/.kube/config文件里ip和端口，然后使用kubectl测试rm -rf .kube/cache .kube/http-cachekubectl get pods -n kube-system -o wide 修改master节点相关组件配置指向vip123456789# lab1 lab2 lab3sed -i 's@server: https://11.11.11.*:6443@server: https://11.11.11.110:8443@g' /etc/kubernetes/&#123;admin.conf,kubelet.conf,scheduler.conf,controller-manager.conf&#125;# 重启kubeletsystemctl daemon-reloadsystemctl restart kubelet docker# 查看所有节点状态kubectl get nodes -o wide 修改kube-proxy的配置12345678910111213# 修改kube-proxy的配置指定vip# 执行命令之后修改为 server: https://11.11.11.110:8443kubectl edit -n kube-system configmap/kube-proxy# 查看设置kubectl get -n kube-system configmap/kube-proxy -o yaml# 删除重建kube-proxykubectl get pods --all-namespaces -o wide | grep proxyall_proxy_pods=$(kubectl get pods --all-namespaces -o wide | grep proxy | awk '&#123;print $2&#125;' | xargs)echo $all_proxy_podskubectl delete pods $all_proxy_pods -n kube-systemkubectl get pods --all-namespaces -o wide | grep proxy 启动node节点1234# 加入master节点# 这个命令是之前初始化master完成时，输出的命令kubeadm join 11.11.11.110:8443 --token nevmjk.iuh214fc8i0k3iue --discovery-token-ca-cert-hash sha256:0e4f738348be836ff810bce754e059054845f44f01619a37b817eba83282d80fsystemctl enable kubelet 修改node节点kubelet配置并重启123456789# 修改配置sed -i 's@server: https://11.11.11.*:6443@server: https://11.11.11.110:8443@g' /etc/kubernetes/kubelet.conf# 重启kubeletsystemctl daemon-reloadsystemctl restart kubelet docker# 查看所有节点状态kubectl get nodes -o wide 禁止master节点发布应用 设置master不接受负载123456789# 查看状态kubectl get nodes# 设置# kubectl patch node lab1 -p '&#123;"spec":&#123;"unschedulable":true&#125;&#125;'kubectl taint nodes lab1 lab2 lab3 node-role.kubernetes.io/master=true:NoSchedule# 查看状态kubectl get nodes 测试重建多个coredns副本12345678910111213# 删除coredns的podskubectl get pods -n kube-system -o wide | grep corednsall_coredns_pods=$(kubectl get pods -n kube-system -o wide | grep coredns | awk '&#123;print $1&#125;' | xargs)echo $all_coredns_podskubectl delete pods $all_coredns_pods -n kube-system# 修改副本数# replicas: 3# 可以修改为node节点的个数kubectl edit deploy coredns -n kube-system# 查看状态kubectl get pods -n kube-system -o wide | grep coredns 基础测试1. 启动12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# 直接使用命令测试kubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service# 使用配置文件测试cat &gt;example-nginx.yml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginxspec: replicas: 2 template: metadata: labels: app: nginx spec: restartPolicy: Always containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 livenessProbe: httpGet: path: / port: 80 initialDelaySeconds: 10 periodSeconds: 3 readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 10 periodSeconds: 3---kind: ServiceapiVersion: v1metadata: name: example-servicespec: selector: app: nginx ports: - name: http port: 80 targetPort: 80---kind: ServiceapiVersion: v1metadata: name: example-service-nodeportspec: selector: app: nginx type: NodePort ports: - name: http-nodeport port: 80 nodePort: 32223EOFkubectl apply -f example-nginx.yml 2. 查看状态1234kubectl get deploykubectl get podskubectl get svckubectl describe svc example-service 3. DNS解析12345678kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service# 如果时间过长会返回错误，可以使用如下方式再进入测试curlPod=$(kubectl get pod | grep curl | awk '&#123;print $1&#125;')kubectl exec -ti $curlPod -- sh 4. 访问测试123456# 10.96.59.56 为查看svc时获取到的clusteripcurl "10.96.59.56:80"# 32223 为查看svc时获取到的 nodeporthttp://11.11.11.114:32223/http://11.11.11.115:32223/ 3. 清理删除12kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 高可用测试关闭master节点测试集群是能否正常执行上一步的基础测试，查看相关信息，不能同时关闭lab1和lab2，因为上面有haproxy和keepalived服务 123456789kubectl get pod --all-namespaces -o widekubectl get pod --all-namespaces -o wide | grep lab1kubectl get pod --all-namespaces -o wide | grep lab2kubectl get pod --all-namespaces -o wide | grep lab3kubectl get nodes -o widekubectl get deploykubectl get podskubectl get svckubectl describe svc example-service 注意事项 当直接把node节点关闭时，只有过了5分钟之后，上面的pod才会被检测到有问题，并迁移到其他节点 如果想快速迁移可以执行 kubectl delete node 也可以修改controller-manager的的pod-eviction-timeout参数，默认5m node-monitor-grace-period参数，默认40s 参考文档 https://kubernetes.io/docs/admin/high-availability/ https://www.kubernetes.org.cn/3536.html https://github.com/indiketa/kubeadm-ha https://zhuanlan.zhihu.com/p/34740013 https://github.com/cookeem/kubeadm-ha/blob/master/README_CN.md https://blog.frognew.com/2017/04/install-etcd-cluster.html https://blog.frognew.com/2017/04/install-ha-kubernetes-1.6-cluster.html https://medium.com/@bambash/ha-kubernetes-cluster-via-kubeadm-b2133360b198 https://github.com/kubernetes/kubeadm/issues/546 https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7使用kubeadm安装k8s集群]]></title>
    <url>%2Fposts%2F16%2F</url>
    <content type="text"><![CDATA[实验环境说明实验架构123lab1: master 11.11.11.111lab2: node 11.11.11.112lab3: node 11.11.11.113 实验使用的Vagrantfile123456789101112131415161718192021# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..3).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "2048"] end end endend 安装要求 需要在每个节点上先安装好kubeadm 有每个节点配置好hosts解析 12345cat &gt;&gt;/etc/hosts&lt;&lt;EOF11.11.11.111 lab111.11.11.112 lab211.11.11.113 lab3EOF 安装配置master节点说明 由于kubeadm默认会去拉取gcr.io上的镜像来启动master相关的组件，由于在国内无法访问gcr.io所以会导致无法成功启动。有如下几种解决办法： 在能翻墙的机器上拉取镜像，再打包导入到master机器上（docker save/load） 直接拉国内别人的镜像，然后打tag为gcr.io的镜像 启动配置集群的时候指定镜像相关配置，使用阿里镜像（本次实验采用） 初始化12345678910111213141516171819202122232425262728293031cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationkubernetesVersion: v1.10.7imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersetcd: image: registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.1.12api: advertiseAddress: 11.11.11.111controllerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16EOF# 配置kubelet使用国内可用pause镜像# 修改/etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 添加如下配置sed -i '/ExecStart=$/i Environment="KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-shanghai.aliyuncs.com/gcr-k8s/pause-amd64:3.0"' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 重新载入配置systemctl daemon-reload# 设置kubelet开机启动systemctl enable kubelet# 使用配置文件方式kubeadm init --config kubeadm-master.config 没有经过测试的另一种初始化方法12345# 使用命令行方式KUBE_REPO_PREFIX='registry.cn-hangzhou.aliyuncs.com/google_containers' kubeadm init \--kubernetes-version=v1.10.3 \--pod-network-cidr=10.244.0.0/16 \--apiserver-advertise-address=11.11.11.111 配置kubectl使用123456789101112rm -rf $HOME/.kubemkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 查看node节点kubectl get nodes# 只有网络插件也安装配置完成之后，才能会显示为ready状态# 设置master允许部署应用pod，参与工作负载，现在可以部署其他系统组件# 如 dashboard, heapster, efk等kubectl taint nodes --all node-role.kubernetes.io/master- 配置使用网络插件配置使用flannel 1234567891011121314151617181920212223242526272829303132333435363738# 下载配置mkdir flannel &amp;&amp; cd flannelwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 修改配置# 此处的ip配置要与上面kubeadm的pod-network一致 net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1# 启动kubectl apply -f kube-flannel.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system 安装配置node节点配置12345678910# 配置kubelet使用国内可用pause镜像# 修改/etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 添加如下配置sed -i '/ExecStart=$/i Environment="KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-shanghai.aliyuncs.com/gcr-k8s/pause-amd64:3.0"' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 重新载入配置systemctl daemon-reload# 设置kubelet开机启动systemctl enable kubelet 加入集群12# 此命令为启动master成功后返回的结果kubeadm join --token 55a6f8.1091208463fe1252 11.11.11.111:6443 --discovery-token-ca-cert-hash sha256:790c6b38b087b167c1f52c04526d8729115192a305eb91c01c0fd8dc7facbbcd 测试容器间的通信和DNS 配置好calico网络之后，kubeadm会自动部署kube-dns 启动123kubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service 查看状态1234kubectl get deploykubectl get podskubectl get svckubectl describe svc example-service DNS解析1234kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service 访问测试123456# 10.96.59.56 为查看svc时获取到的clusteripcurl "10.96.59.56:80"# 32223 为查看svc时获取到的 nodeporthttp://11.11.11.112:32223/http://11.11.11.113:32223/ 清理删除12kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 挖坑记1. 如果删除kube-dns后怎么修复123456# 第一种方法kubeadm upgrade apply v1.10.3 --config kubeadm-master.config# 第二种方法kubeadm config upload from-file --config kubeadm-master.configkubeadm upgrade apply v1.10.3 2. 忘记初始master节点时的node节点加入集群命令怎么办 123456# 简单方法kubeadm token create --print-join-command# 第二种方法token=$(kubeadm token generate)kubeadm token create $token --print-join-command --ttl=0 参考文档 https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/ https://blog.frognew.com/2017/12/kubeadm-install-kubernetes-1.9.html]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装kubeadm]]></title>
    <url>%2Fposts%2F15%2F</url>
    <content type="text"><![CDATA[关闭防火墙12systemctl stop firewalldsystemctl disable firewalld 安装配置docker v1.9.0版本推荐使用docker v1.12,v1.11, v1.13, 17.03也可以使用，再高版本的docker可能无法正常使用。测试发现17.09无法正常使用，不能使用资源限制(内存CPU) 安装docker12345# 卸载安装指定版本docker-ceyum remove -y docker-ce docker-ce-selinux container-selinuxyum install -y --setopt=obsoletes=0 \docker-ce-17.03.1.ce-1.el7.centos \docker-ce-selinux-17.03.1.ce-1.el7.centos 修改docker配置使用systemd 在/etc/docker/daemon.json文件添加如下配置centos7安装的docker12不能添加此参数，否则会无法启动123456789&#123; "exec-opts": ["native.cgroupdriver=systemd"]&#125;# 如果使用了加速器配置格式如下&#123; "registry-mirrors": ["https://tfhzn46h.mirror.aliyuncs.com"], "exec-opts": ["native.cgroupdriver=systemd"]&#125; 启动docker1systemctl enable docker &amp;&amp; systemctl restart docker 安装 kubeadm, kubelet 和 kubectl如下的安装方法，选择其中一个即可。 翻墙安装 需要翻墙才能正常安装，如果不能翻墙，可以使用阿里云或其他云提供的容器海外构建功能，下载好包。启动容器，之后从容器中把文件获取出来安装即可。 12345678910111213141516171819202122232425262728# 安装依赖yum install -y ebtables socat# 在能翻墙的机器上下载rpm包cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOFyum install -y yum-utilsk8s_version=$(yum info kubelet | grep Version | awk -F ':' '&#123;print $2&#125;' | tr -d ' ')mkdir k8s-$k8s_version &amp;&amp; cd k8s-$k8s_version# 下载最新版本yumdownloader kubelet kubeadm kubectl kubernetes-cni# 下载指定版本yumdownloader kubelet-1.10.7 kubeadm-1.10.7 kubectl-1.10.7 kubernetes-cni# 打包下载到要安装的机器上cd .. &amp;&amp; tar cvzf k8s-$&#123;k8s_version&#125;.tgz k8s-$k8s_version/# 安装tar xf k8s-*.tgz &amp;&amp; cd k8s-* &amp;&amp; yum localinstall -y *.rpm 使用阿里镜像安装12345678910111213141516# 配置源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 安装最新版本yum install -y kubelet kubeadm kubectl# 安装指定版本yum install -y kubelet-1.10.7 kubeadm-1.10.7 kubectl-1.10.7 二进制安装方法（不推荐）此需要自己安装kubernetes-cni 1234567# 下载安装version=$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)wget https://storage.googleapis.com/kubernetes-release/release/$version/bin/linux/amd64/kubectlwget https://storage.googleapis.com/kubernetes-release/release/$version/bin/linux/amd64/kubeadmwget https://storage.googleapis.com/kubernetes-release/release/$version/bin/linux/amd64/kubeletchmod +x kubectl kubeadm kubeletmv kubectl kubeadm kubelet /usr/local/bin 实验所用kubeadm相关文件已经上传到了百度网盘链接：https://pan.baidu.com/s/1pl7YYUYZsPd98J0DhNLWEQ 密码：40na 配置系统相关参数12345678910111213141516171819202122232425# 临时禁用selinux# 永久关闭 修改/etc/sysconfig/selinux文件设置sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinuxsetenforce 0# 临时关闭swap# 永久关闭 注释/etc/fstab文件里swap相关的行swapoff -a# 开启forward# Docker从1.13版本开始调整了默认的防火墙规则# 禁用了iptables filter表中FOWARD链# 这样会引起Kubernetes集群中跨Node的Pod无法通信iptables -P FORWARD ACCEPT# 配置转发相关参数，否则可能会出错cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1vm.swappiness=0EOFsysctl --system 参考文档 https://kubernetes.io/docs/setup/independent/install-kubeadm/ https://blog.frognew.com/2017/12/kubeadm-install-kubernetes-1.9.html]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装终端记录回放器asciinema]]></title>
    <url>%2Fposts%2F14%2F</url>
    <content type="text"><![CDATA[简介终端记录器，可以记录你在终端的任务操作，然后回放。官方示例网站https://asciinema.org/ 安装1yum install -y asciinema 简单使用1.简单记录并传到asciinema.org1234567891011121314151617# 简单asciinema rec# 指定标题asciinema rec -t "My git tutorial"# 记录保存到本地 asciinema rec demo.json# 指定最多等待间隔2.5sasciinema rec -w 2.5 demo.json# 重放操作asciinema play demo.json# 重放asciinema.org上的操作asciinema play https://asciinema.org/a/difqlgx86ym6emrmd8u62yqu8 简单配置123vim ~/.config/asciinema/config[api]url = http://asciinema.example.com 安装配置自己的asciinema-server 安装之前先安装docker-ce， 测试发现docker-12版本不能正常使用 1. 安装配置123456789101112131415git clone --recursive https://github.com/asciinema/asciinema-server.gitcd asciinema-servergit checkout mastercp .env.production.sample .env.productionvim .env.production# 设置url相关URL_SCHEME=httpURL_HOST=asciinema.example.comURL_PORT=80# 设置密钥secret=$(docker-compose run --rm web mix phx.gen.secret)SECRET_KEY_BASE=$secret 2. 初始化12docker-compose up -d postgresdocker-compose run --rm web setup 3. 启动12docker-compose up -ddocker ps -f 'name=asciinema_' 4. 浏览器访问测试12345# 配置hosts11.11.11.111 asciinema.example.com# 浏览器访问http://asciinema.example.com/ 5. 配置客户端12345678910111213# 配置hostecho '11.11.11.111 asciinema.example.com' &gt;&gt; /etc/hosts# ~/.config/asciinema/config[api]url = http://asciinema.example.com# 也可以使用环境变量ASCIINEMA_API_URL=http://asciinema.example.com asciinema rec# 注意# 如果没有token配置项会无法上传，可以先直接执行asciinema rec生成配置文件后# 再添加url的配置项 6. 测试使用1asciinema rec 参考文档 https://github.com/asciinema/asciinema/blob/master/README.md https://github.com/asciinema/asciinema-server/blob/master/docs/INSTALL.md]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>terminal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7配置使用代理]]></title>
    <url>%2Fposts%2F13%2F</url>
    <content type="text"><![CDATA[简介 本文档主要介绍如何在centos7上使用代理，访问墙外的网站。 shadowsocks实现sock5代理，privoxy把sock5代理转换为http代理。 安装shadowsocks123456789101112131415161718192021222324252627282930313233343536373839# 安装sudo su - rootyum -y install python-pippip install shadowsocks# 配置mkdir /etc/shadowsockscat &gt;/etc/shadowsocks/shadowsocks.json&lt;&lt;EOF&#123; "server":"47.52.xx.xx", "server_port":52113, "local_address": "127.0.0.1", "local_port":1080, "password":"xxxxxx", "timeout":300, "method":"aes-256-cfb", "fast_open": false, "workers": 1&#125;EOF# 启动脚本cat &gt;/etc/systemd/system/shadowsocks.service&lt;&lt;EOF[Unit]Description=Shadowsocks[Service]TimeoutStartSec=0ExecStart=/usr/bin/sslocal -c /etc/shadowsocks/shadowsocks.json[Install]WantedBy=multi-user.targetEOF# 启动并加入开机启动systemctl enable shadowsocks.servicesystemctl start shadowsocks.servicesystemctl status shadowsocks.service# 测试curl --socks5 127.0.0.1:1080 ip.cn 安装privoxy 另一种选择polipo 安装1yum -y install privoxy 以下两种模式选择一种 简单全局模式配置1echo 'forward-socks5t / 127.0.0.1:1080 . #转发到本地端口' &gt;&gt; /etc/privoxy/config PAC模式配置（推荐）12345678# 下载生成privoxy-action配置的脚本curl -skL https://raw.github.com/zfl9/gfwlist2privoxy/master/gfwlist2privoxy -o gfwlist2privoxy# 生成配置启动# '127.0.0.1:1080' 为你的sock5代理地址bash gfwlist2privoxy '127.0.0.1:1080'cp -af gfwlist.action /etc/privoxy/echo 'actionsfile gfwlist.action' &gt;&gt; /etc/privoxy/config 启动测试12345678910111213141516171819202122232425# 启动并加入开机启动systemctl enable privoxy.servicesystemctl start privoxy.servicesystemctl status privoxy.service# export http_proxy=http://127.0.0.1:8118# export https_proxy=http://127.0.0.1:8118# export all_proxy=http://127.0.0.1:8118# 加入环境变量中cat &gt;/etc/profile.d/proxy.sh&lt;&lt;EOFalias proxy='export all_proxy=http://127.0.0.1:8118'alias unproxy='unset all_proxy'EOF# 测试. /etc/profile.d/proxy.sh# 使用代理proxycurl www.google.comcurl ip.cn# 不使用代理unproxycurl ip.cn]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>proxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码整洁之道]]></title>
    <url>%2Fposts%2F12%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>开发</tag>
        <tag>代码规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装kubernetes-v1.7 Volumes和Persistent Volumes]]></title>
    <url>%2Fposts%2F11%2F</url>
    <content type="text"><![CDATA[简介Volumes用来存储要持久保留的数据，不想重启容器后就消失。POD中容器之间共享数据。PersistentVolume（PV）和 PersistentVolumeClaim（PVC）是kubernetes提供的 两种API资源，用于抽象存储细节。管理员关注于如何通过pv提供存储功能而无需关注用户如何使用，同样的用户只需要挂载PVC到容器中而不需要关注存储卷采用何种技术实现。PVC和PV的关系跟pod和node关系类似，前者消耗后者的资源。PVC可以向PV申请指定大小的存储资源并设置访问模式。 基本使用1234567891011121314151617181920212223242526272829303132333435363738apiVersion: v1kind: Podmetadata: name: counterspec: containers: - name: count image: busybox args: - /bin/sh - -c - &gt; i=0; while true; do echo "$i: $(date)" &gt;&gt; /var/log/1.log; echo "$(date) INFO $i" &gt;&gt; /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log - name: count-log-1 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log'] volumeMounts: - name: varlog mountPath: /var/log - name: count-log-2 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log'] volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog emptyDir: &#123;&#125; 安装配置glusterfs1. 安装glusterfs1234567891011121314# 先安装 gluster 源yum install centos-release-gluster -y# 安装 glusterfs 组件yum install -y glusterfs glusterfs-server glusterfs-fuse \glusterfs-rdma glusterfs-geo-replication glusterfs-devel# 启动 glusterfssystemctl start glusterd.servicesystemctl enable glusterd.service# 查看状态systemctl status glusterd.service 2. 配置集群123456789101112131415# 配置 hosts192.168.12.211 lab1192.168.12.212 lab2192.168.12.213 lab3# 创建存储目录mkdir -pv /data/gfs# 添加节点到 集群# 执行操作的本机不需要probe 本机gluster peer probe lab2gluster peer probe lab3# 查看集群状态gluster peer status 3. 配置 volume1234567891011# 创建分布卷gluster volume create k8s-volume transport tcp \lab1:/data/gfs \lab2:/data/gfs \lab3:/data/gfs force# 查看volume状态gluster volume info# 启动 分布卷gluster volume start k8s-volume Kubernetes中配置glusterfs1. kubernetes安装客户端123# 在所有 node 节点安装yum install centos-release-gluster -yyum install -y glusterfs glusterfs-fuse 2. 配置 hosts123192.168.12.211 lab1192.168.12.212 lab2192.168.12.213 lab3 3. 配置 endpoints12345678910# 下载curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-endpoints.json# 配置 glusters 集群节点ip每一个 addresses 为一个 ip 组# 创建kubectl apply -f glusterfs-endpoints.json# 查看kubectl get ep 4. 配置 service1234567curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-service.json# 修改 port 的刚刚上一步中修改的 port# 创建kubectl apply -f glusterfs-service.jsonkubectl get svc 测试1. 创建测试 pod1234567891011121314curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-pod.json# 修改 volumes 下的 path 为上面创建的 k8s-volume"path": "k8s-volume"# 创建kubectl apply -f glusterfs-pod.json# 查看kubectl get podskubectl describe pods/glusterfs#登陆 node 物理机，使用 df 可查看挂载目录df -h PV和PVC1. 配置PV12345678910111213141516171819202122# 创建pvcat &gt;glusterfs-pv.yaml&lt;&lt;EOFapiVersion: v1kind: PersistentVolumemetadata: name: gluster-dev-volumespec: capacity: storage: 8Gi accessModes: - ReadWriteMany glusterfs: endpoints: "glusterfs-cluster" path: "k8s-volume" readOnly: falseEOF# 创建kubectl apply -f glusterfs-pv.yaml# 查看 pvkubectl get pv 2. 配置PVC12345678910111213141516171819cat &gt;glusterfs-pvc.yaml&lt;&lt;EOFkind: PersistentVolumeClaimapiVersion: v1metadata: name: glusterfs-nginxspec: accessModes: - ReadWriteMany resources: requests: storage: 8GiEOF# 创建 pvckubectl apply -f glusterfs-pvc.yaml# 查看 pvckubectl get pvkubectl get pvc 3. 创建 nginx deployment 挂载 volume123456789101112131415161718192021222324252627282930313233cat &gt;nginx-deployment.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-dmspec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: gluster-dev-volume mountPath: "/usr/share/nginx/html" volumes: - name: gluster-dev-volume persistentVolumeClaim: claimName: glusterfs-nginxEOF# 创建kubectl apply -f nginx-deployment.yamlkubectl expose deployment nginx-dm --name=nginx-dm-svc# 查看 deploymentkubectl get pods |grep nginx-dm 4. 测试12345678# 查看挂载kubectl exec -it nginx-dm-2194008866-szfl2 -- df -h | grep k8s-volume# 创建文件 测试kubectl exec -it nginx-dm-2194008866-szfl2 -- touch /usr/share/nginx/html/index.html# 查看文件kubectl exec -it nginx-dm-2194008866-szfl2 -- ls -lt /usr/share/nginx/html/index.html StorageClasshttps://kubernetes.io/docs/concepts/storage/persistent-volumes/#storageclasses 参考文档 https://kubernetes.io/docs/concepts/storage/volumes/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装kubernetes-v1.7插件]]></title>
    <url>%2Fposts%2F10%2F</url>
    <content type="text"><![CDATA[本教程紧接 centos7安装kubernetes-v1.7 请参考。 安装和配置 kubedns 插件1. 拉取镜像1234567891011# 在所有node节点上操作# pulldocker pull jicki/k8s-dns-sidecar-amd64:1.14.4docker pull jicki/k8s-dns-kube-dns-amd64:1.14.4docker pull jicki/k8s-dns-dnsmasq-nanny-amd64:1.14.4# tagdocker tag jicki/k8s-dns-sidecar-amd64:1.14.4 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.4docker tag jicki/k8s-dns-kube-dns-amd64:1.14.4 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.4docker tag jicki/k8s-dns-dnsmasq-nanny-amd64:1.14.4 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.4 2. 修改配置ymal123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212# 创建目录mkdir /server/software/k8s/dnscd /server/software/k8s/dns# config mapcat &gt;kubedns-cm.yaml&lt;&lt;EOFapiVersion: v1kind: ConfigMapmetadata: name: kube-dns namespace: kube-system labels: addonmanager.kubernetes.io/mode: EnsureExistsdata: upstreamNameservers: | ["114.114.114.114", "8.8.8.8"]EOF# service accountcat &gt;kubedns-sa.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: kube-dns namespace: kube-system labels: kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: ReconcileEOF# controllercat &gt;kubedns-controller.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: # replicas: not specified here: # 1. In order to make Addon Manager do not reconcile this replicas parameter. # 2. Default is 1. # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on. strategy: rollingUpdate: maxSurge: 10% maxUnavailable: 0 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: tolerations: - key: "CriticalAddonsOnly" operator: "Exists" volumes: - name: kube-dns-config configMap: name: kube-dns optional: true containers: - name: kubedns image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.4 resources: # TODO: Set memory limits when we've profiled the container for large # clusters, then set request = limit to keep this container in # guaranteed class. Currently, this container falls into the # "burstable" category so the kubelet doesn't backoff from restarting it. limits: memory: 170Mi requests: cpu: 100m memory: 70Mi livenessProbe: httpGet: path: /healthcheck/kubedns port: 10054 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /readiness port: 8081 scheme: HTTP # we poll on pod startup for the Kubernetes master service and # only setup the /readiness HTTP server once that's available. initialDelaySeconds: 3 timeoutSeconds: 5 args: - --domain=cluster.local. - --dns-port=10053 - --config-dir=/kube-dns-config - --v=2 #__PILLAR__FEDERATIONS__DOMAIN__MAP__ env: - name: PROMETHEUS_PORT value: "10055" ports: - containerPort: 10053 name: dns-local protocol: UDP - containerPort: 10053 name: dns-tcp-local protocol: TCP - containerPort: 10055 name: metrics protocol: TCP volumeMounts: - name: kube-dns-config mountPath: /kube-dns-config - name: dnsmasq image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.4 livenessProbe: httpGet: path: /healthcheck/dnsmasq port: 10054 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 args: - -v=2 - -logtostderr - -configDir=/etc/k8s/dns/dnsmasq-nanny - -restartDnsmasq=true - -- - -k - --cache-size=1000 - --log-facility=- - --server=/cluster.local./127.0.0.1#10053 - --server=/in-addr.arpa/127.0.0.1#10053 - --server=/ip6.arpa/127.0.0.1#10053 ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP # see: https://github.com/kubernetes/kubernetes/issues/29055 for details resources: requests: cpu: 150m memory: 20Mi volumeMounts: - name: kube-dns-config mountPath: /etc/k8s/dns/dnsmasq-nanny - name: sidecar image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.4 livenessProbe: httpGet: path: /metrics port: 10054 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 args: - --v=2 - --logtostderr - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local.,5,A - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local.,5,A ports: - containerPort: 10054 name: metrics protocol: TCP resources: requests: memory: 20Mi cpu: 10m dnsPolicy: Default # Don't use cluster DNS. serviceAccountName: kube-dnsEOF# servicecat &gt;kubedns-svc.yaml&lt;&lt;EOFapiVersion: v1kind: Servicemetadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "KubeDNS"spec: selector: k8s-app: kube-dns clusterIP: 10.254.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCPEOF 3. 系统预定义的 RoleBinding预定义的 RoleBinding system:kube-dns 将 kube-system 命名空间的 kubednsServiceAccount 与 system:kube-dns Role 绑定， 该 Role 具有访问 kubeapiserverDNS 相关 API 的权限； 1kubectl get clusterrolebindings system:kube-dns -o yaml 4. 创建 kube-dns12345cd /server/software/k8s/dnskubectl create -f .kubectl get pods -n kube-systemkubectl get deploy -n kube-systemkubectl get svc -n kube-system 5. 测试DNS1234567891011121314151617181920212223242526272829303132333435# 创建deploycat &gt;my-nginx.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: my-nginxspec: replicas: 2 template: metadata: labels: run: my-nginx spec: containers: - name: my-nginx image: nginx:1.9 ports: - containerPort: 80EOFkubectl create -f my-nginx.yamlkubectl get podskubectl get deploykubectl expose deploy my-nginxkubectl get services --all-namespaces |grep my-nginx# 测试kubectl run dns-test --rm -ti --image busybox /bin/shnslookup kubernetescat /etc/resolv.confping my-nginxping kuberneteswget -q my-nginx -O -# 删除测试相关的podkubectl delete -f my-nginx.yaml 可以自行指定特殊的域名使用指定的dns服务器 ，也可以自行指定上游服务器。参考https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/ 安装dashboard插件1. 拉取镜像12docker pull jicki/kubernetes-dashboard-amd64:v1.6.1docker tag jicki/kubernetes-dashboard-amd64:v1.6.1 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.1 2. 配置yaml文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192mkdir -pv /server/software/k8s/dashboardcd /server/software/k8s/dashboard# Controllercat &gt;dashboard-controller.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: serviceAccountName: dashboard containers: - name: kubernetes-dashboard image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.1 resources: limits: cpu: 100m memory: 50Mi requests: cpu: 100m memory: 50Mi ports: - containerPort: 9090 livenessProbe: httpGet: path: / port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 tolerations: - key: "CriticalAddonsOnly" operator: "Exists"EOF# servicecat &gt;dashboard-service.yaml&lt;&lt;EOFapiVersion: v1kind: Servicemetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: type: NodePort selector: k8s-app: kubernetes-dashboard ports: - port: 80 targetPort: 9090EOF# rbaccat &gt;dashboard-rbac.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: dashboard namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: dashboardsubjects: - kind: ServiceAccount name: dashboard namespace: kube-systemroleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.ioEOF 增加了一个dashboard-rbac.yaml文件，定义一个名为 dashboard 的 ServiceAccount，然后将它和 Cluster Role view 绑定。 指定端口类型为 NodePort，这样外界可以通过地址 nodeIP:nodePort 访问 dashboard； 3. 创建dashboard123456kubectl create -f .kubectl get pods -n kube-systemkubectl get deploy -n kube-system# 查看nodePortkubectl get services kubernetes-dashboard -n kube-system 4. 测试访问 暴露了 NodePort，可以使用 http://NodeIP:nodePort 地址访问 dashboard；http://192.168.12.212:31680 通过 kube-apiserver 访问 dashboard； 123456789101112131415# 获取地址信息kubectl cluster-info# 访问https://192.168.12.211:6443/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy# 可能会遇到的问题https://github.com/opsnull/follow-me-install-kubernetes-cluster/issues/5# 导出证书openssl pkcs12 -export -in admin.pem -out admin.p12 -inkey admin-key.pem# 导入证书# 将生成的admin.p12证书导入的你的电脑，导出的时候记住你设置的密码，导入的时候还要用到。# 密码可以设置为空，直接回车即可。 通过 kubectl proxy 访问 dashboard 12345# 创建kubectl proxy --address='192.168.12.211' --port=8086 --accept-hosts='^*$'# 访问http://192.168.12.211:8086/ui 配置和安装 Heapster1. 拉取镜像12345678docker pull daocloud.io/will835559313/k8s:master-f7ff86edocker tag daocloud.io/will835559313/k8s:master-f7ff86e gcr.io/google_containers/heapster-amd64:v1.3.0docker pull daocloud.io/will835559313/k8s:master-545d95ddocker tag daocloud.io/will835559313/k8s:master-545d95d gcr.io/google_containers/heapster-grafana-amd64:v4.0.2docker pull daocloud.io/will835559313/k8s:master-f694b85docker tag daocloud.io/will835559313/k8s:master-f694b85 gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1 2. 配置yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315mkdir -pv /server/software/k8s/heapstercd /server/software/k8s/heapster# influxdb# ConfigMap的配置主要为了开启 admin UI 插件# 可以根据需求来删除此配置。cat &gt;influxdb.yaml&lt;&lt;EOF---apiVersion: v1kind: ConfigMapmetadata: name: influxdb-config namespace: kube-systemdata: config.toml: | reporting-disabled = true bind-address = ":8088" [meta] dir = "/data/meta" retention-autocreate = true logging-enabled = true [data] dir = "/data/data" wal-dir = "/data/wal" query-log-enabled = true cache-max-memory-size = 1073741824 cache-snapshot-memory-size = 26214400 cache-snapshot-write-cold-duration = "10m0s" compact-full-write-cold-duration = "4h0m0s" max-series-per-database = 1000000 max-values-per-tag = 100000 trace-logging-enabled = false [coordinator] write-timeout = "10s" max-concurrent-queries = 0 query-timeout = "0s" log-queries-after = "0s" max-select-point = 0 max-select-series = 0 max-select-buckets = 0 [retention] enabled = true check-interval = "30m0s" [admin] enabled = true bind-address = ":8083" https-enabled = false https-certificate = "/etc/ssl/influxdb.pem" [shard-precreation] enabled = true check-interval = "10m0s" advance-period = "30m0s" [monitor] store-enabled = true store-database = "_internal" store-interval = "10s" [subscriber] enabled = true http-timeout = "30s" insecure-skip-verify = false ca-certs = "" write-concurrency = 40 write-buffer-size = 1000 [http] enabled = true bind-address = ":8086" auth-enabled = false log-enabled = true write-tracing = false pprof-enabled = false https-enabled = false https-certificate = "/etc/ssl/influxdb.pem" https-private-key = "" max-row-limit = 10000 max-connection-limit = 0 shared-secret = "" realm = "InfluxDB" unix-socket-enabled = false bind-socket = "/var/run/influxdb.sock" [[graphite]] enabled = false bind-address = ":2003" database = "graphite" retention-policy = "" protocol = "tcp" batch-size = 5000 batch-pending = 10 batch-timeout = "1s" consistency-level = "one" separator = "." udp-read-buffer = 0 [[collectd]] enabled = false bind-address = ":25826" database = "collectd" retention-policy = "" batch-size = 5000 batch-pending = 10 batch-timeout = "10s" read-buffer = 0 typesdb = "/usr/share/collectd/types.db" [[opentsdb]] enabled = false bind-address = ":4242" database = "opentsdb" retention-policy = "" consistency-level = "one" tls-enabled = false certificate = "/etc/ssl/influxdb.pem" batch-size = 1000 batch-pending = 5 batch-timeout = "1s" log-point-errors = true [[udp]] enabled = false bind-address = ":8089" database = "udp" retention-policy = "" batch-size = 5000 batch-pending = 10 read-buffer = 0 batch-timeout = "1s" precision = "" [continuous_queries] log-enabled = true enabled = true run-interval = "1s"---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: monitoring-influxdb namespace: kube-systemspec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: influxdb spec: containers: - name: influxdb image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1 volumeMounts: - mountPath: /data name: influxdb-storage - mountPath: /etc/ name: influxdb-config volumes: - name: influxdb-storage emptyDir: &#123;&#125; - name: influxdb-config configMap: name: influxdb-config---apiVersion: v1kind: Servicemetadata: labels: task: monitoring # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: 'true' kubernetes.io/name: monitoring-influxdb name: monitoring-influxdb namespace: kube-systemspec: type: NodePort ports: - port: 8086 targetPort: 8086 name: http - port: 8083 targetPort: 8083 name: admin selector: k8s-app: influxdbEOF# grafanacat &gt;grafana.yaml&lt;&lt;EOF---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: monitoring-grafana namespace: kube-systemspec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: grafana spec: containers: - name: grafana image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 ports: - containerPort: 3000 protocol: TCP volumeMounts: - mountPath: /var name: grafana-storage env: - name: INFLUXDB_HOST value: monitoring-influxdb - name: GRAFANA_PORT value: "3000" # The following env variables are required to make Grafana accessible via # the kubernetes api-server proxy. On production clusters, we recommend # removing these env variables, setup auth for grafana, and expose the grafana # service using a LoadBalancer or a public IP. - name: GF_AUTH_BASIC_ENABLED value: "false" - name: GF_AUTH_ANONYMOUS_ENABLED value: "true" - name: GF_AUTH_ANONYMOUS_ORG_ROLE value: Admin - name: GF_SERVER_ROOT_URL # If you're only using the API Server proxy, set this value instead: value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/ #value: / volumes: - name: grafana-storage emptyDir: &#123;&#125;---apiVersion: v1kind: Servicemetadata: labels: # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: 'true' kubernetes.io/name: monitoring-grafana name: monitoring-grafana namespace: kube-systemspec: # In a production setup, we recommend accessing Grafana through an external Loadbalancer # or through a public IP. # type: LoadBalancer # You could also use NodePort to expose the service at a randomly-generated port ports: - port : 80 targetPort: 3000 selector: k8s-app: grafanaEOF# heapstercat &gt;heapster.yaml&lt;&lt;EOF---apiVersion: v1kind: ServiceAccountmetadata: name: heapster namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: heapstersubjects: - kind: ServiceAccount name: heapster namespace: kube-systemroleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: heapster namespace: kube-systemspec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: heapster spec: serviceAccountName: heapster containers: - name: heapster image: gcr.io/google_containers/heapster-amd64:v1.3.0 imagePullPolicy: IfNotPresent command: - /heapster - --source=kubernetes:https://kubernetes.default - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086---apiVersion: v1kind: Servicemetadata: labels: task: monitoring # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: 'true' kubernetes.io/name: Heapster name: heapster namespace: kube-systemspec: ports: - port: 80 targetPort: 8082 selector: k8s-app: heapsterEOF 3. 创建1234567kubectl create -f .kubectl get deploy -n kube-systemkubectl get pods -n kube-systemkubectl get svc -n kube-system# 访问dashboard查有无监控图表 4. 访问 grafana 通过 kube-apiserver 访问 12kubectl cluster-info# https://192.168.12.211:6443/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy 通过 kubectl proxy 访问 12kubectl proxy --address='192.168.12.211' --port=8086 --accept-hosts='^*$'# http://192.168.12.211:8086/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana 5. 访问 influxdb admin UI 通过NodePort 12kubectl get svc -n kube-system|grep influxdb# http://192.168.12.212:31083/ 通过api-server 1# http://192.168.12.211:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb:8083/ 如果后续使用 kube-apiserver 或者 kubectl proxy 访问 grafana dashboard，则必须将 GF_SERVER_ROOT_URL 设置为/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/，否则后续访问grafana时访问时提示找不到http://192.168.12.211:8086/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/api/dashboards/home 页面；heapster1.4版本会有问题，造成grafana无法成功启动，建议使用1.3版本 配置和安装 EFK1. 拉取镜像12345678docker pull daocloud.io/will835559313/k8s:master-6ccfe3ddocker tag daocloud.io/will835559313/k8s:master-6ccfe3d gcr.io/google_containers/kibana:v4.6.1-1docker pull daocloud.io/will835559313/k8s:master-425d70bdocker tag daocloud.io/will835559313/k8s:master-425d70b gcr.io/google_containers/fluentd-elasticsearch:1.22docker pull daocloud.io/will835559313/k8s:master-1e3ccdfdocker tag daocloud.io/will835559313/k8s:master-1e3ccdf gcr.io/google_containers/elasticsearch:v2.4.1-2 2. 修改yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219mkdir -pv /server/software/k8s/efkcd /server/software/k8s/efk# rbaccat &gt;efk-rbac.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: efk namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: efksubjects: - kind: ServiceAccount name: efk namespace: kube-systemroleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.ioEOF# escat &gt;es.yaml&lt;&lt;EOFapiVersion: v1kind: ReplicationControllermetadata: name: elasticsearch-logging-v1 namespace: kube-system labels: k8s-app: elasticsearch-logging version: v1 kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: replicas: 2 selector: k8s-app: elasticsearch-logging version: v1 template: metadata: labels: k8s-app: elasticsearch-logging version: v1 kubernetes.io/cluster-service: "true" spec: serviceAccountName: efk containers: - image: gcr.io/google_containers/elasticsearch:v2.4.1-2 name: elasticsearch-logging resources: # need more cpu upon initialization, therefore burstable class limits: cpu: 1000m requests: cpu: 100m ports: - containerPort: 9200 name: db protocol: TCP - containerPort: 9300 name: transport protocol: TCP volumeMounts: - name: es-persistent-storage mountPath: /data env: - name: "NAMESPACE" valueFrom: fieldRef: fieldPath: metadata.namespace volumes: - name: es-persistent-storage emptyDir: &#123;&#125;---apiVersion: v1kind: Servicemetadata: name: elasticsearch-logging namespace: kube-system labels: k8s-app: elasticsearch-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "Elasticsearch"spec: ports: - port: 9200 protocol: TCP targetPort: db selector: k8s-app: elasticsearch-loggingEOF# kibanacat &gt;kibana.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: kibana-logging namespace: kube-system labels: k8s-app: kibana-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: replicas: 1 selector: matchLabels: k8s-app: kibana-logging template: metadata: labels: k8s-app: kibana-logging spec: serviceAccountName: efk containers: - name: kibana-logging image: gcr.io/google_containers/kibana:v4.6.1-1 resources: # keep request = limit to keep this container in guaranteed class limits: cpu: 100m requests: cpu: 100m env: - name: "ELASTICSEARCH_URL" value: "http://elasticsearch-logging:9200" - name: "KIBANA_BASE_URL" value: "/api/v1/proxy/namespaces/kube-system/services/kibana-logging" ports: - containerPort: 5601 name: ui protocol: TCP---apiVersion: v1kind: Servicemetadata: name: kibana-logging namespace: kube-system labels: k8s-app: kibana-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "Kibana"spec: ports: - port: 5601 protocol: TCP targetPort: ui selector: k8s-app: kibana-loggingEOF# flentdcat &gt;fluentd-es-ds.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: fluentd-es-v1.22 namespace: kube-system labels: k8s-app: fluentd-es kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile version: v1.22spec: template: metadata: labels: k8s-app: fluentd-es kubernetes.io/cluster-service: "true" version: v1.22 # This annotation ensures that fluentd does not get evicted if the node # supports critical pod annotation based priority scheme. # Note that this does not guarantee admission on the nodes (#40573). annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: serviceAccountName: efk containers: - name: fluentd-es image: gcr.io/google_containers/fluentd-elasticsearch:1.22 command: - '/bin/sh' - '-c' - '/usr/sbin/td-agent 2&gt;&amp;1 &gt;&gt; /var/log/fluentd.log' resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true nodeSelector: beta.kubernetes.io/fluentd-ds-ready: "true" tolerations: - key : "node.alpha.kubernetes.io/ismaster" effect: "NoSchedule" terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containersEOF 3. 给 Node 设置标签 定义 DaemonSet fluentd-es-v1.22 时设置了 nodeSelector beta.kubernetes.io/fluentd-ds-ready=true ，所以需要在期望运行 fluentd 的 Node 上设置该标签； 12kubectl get nodeskubectl label nodes 192.168.12.212 beta.kubernetes.io/fluentd-ds-ready=true 4. 创建12345678kubectl create -f .kubectl get deploy -n kube-systemkubectl get pods -n kube-systemkubectl get svc -n kube-system# kibana Pod 第一次启动时会用**较长时间(10-20分钟)**来优化和 Cache 状态页面，# 可以 tailf 该 Pod 的日志观察进度：kubectl logs -f kibana-logging-269483651-c2tl0 -n kube-system 5. 访问 通过 kube-apiserver 访问 12kubectl cluster-info# https://192.168.12.211:6443/api/v1/namespaces/kube-system/services/kibana-logging/proxy 通过 kubectl proxy 访问 123kubectl proxy --address='192.168.12.211' --port=8086 --accept-hosts='^*$'# http://192.168.12.211:8086/api/v1/proxy/namespaces/kube-system/services/kibana-logging 可能遇到的问题如果你在这里发现Create按钮是灰色的无法点击，且Time-filed name中没有选项，fluentd要读取/var/log/containers/目录下的log日志，这些日志是从/var/lib/docker/containers/${CONTAINER_ID}/${CONTAINER_ID}-json.log链接过来的，查看你的docker配置，—log-dirver需要设置为json-file格式，默认的可能是journald，参考docker logging。 安装配置 traefik ingress1. 简介 理解Ingress 简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。Ingress相当于nginx、apache等负载均衡方向代理服务器，其中还包括规则定义，即URL的路由信息，路由信息得的刷新由Ingress controller来提供。 理解Ingress Controller Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用。 2. 配置yaml文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140mkdir /server/software/k8s/traefikcd /server/software/k8s/traefik# rbaccat &gt;ingress-rbac.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: ingress namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: ingresssubjects: - kind: ServiceAccount name: ingress namespace: kube-systemroleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.ioEOF# ingresscat &gt;ingress.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Ingressmetadata: name: traefik-ingressspec: rules: - host: traefik.nginx.io http: paths: - path: / backend: serviceName: my-nginx servicePort: 80 - host: traefik.frontend.io http: paths: - path: / backend: serviceName: frontend servicePort: 80 - host: rolling-update-test.traefik.io http: paths: - path: / backend: serviceName: rolling-update-test servicePort: 9090 - host: k8s-app-monitor-agent.jimmysong.io http: paths: - path: / backend: serviceName: k8s-app-monitor-agent servicePort: 8080EOF# traefikcat &gt;traefik.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: traefik-ingress-lb namespace: kube-system labels: k8s-app: traefik-ingress-lbspec: template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: terminationGracePeriodSeconds: 60 hostNetwork: true restartPolicy: Always serviceAccountName: ingress containers: - image: traefik name: traefik-ingress-lb resources: limits: cpu: 200m memory: 30Mi requests: cpu: 100m memory: 20Mi ports: - name: http containerPort: 80 hostPort: 80 - name: admin containerPort: 8580 hostPort: 8580 args: - --web - --web.address=:8580 - --kubernetes nodeSelector: edgenode: "true"EOF# uicat &gt;ui.yaml&lt;&lt;EOFapiVersion: v1kind: Servicemetadata: name: traefik-web-ui namespace: kube-systemspec: selector: k8s-app: traefik-ingress-lb ports: - name: web port: 80 targetPort: 8580---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: traefik-web-ui namespace: kube-systemspec: rules: - host: traefik-ui.local http: paths: - path: / backend: serviceName: traefik-web-ui servicePort: webEOF 3. 创建12345678# 设置 node 的 labelkubectl get nodeskubectl label nodes 192.168.12.212 edgenode=truekubectl create -f .kubectl get pods -n kube-systemkubectl get ds -n kube-systemkubectl get svc -n kube-system 4. 访问1234567# 管理页面http://192.168.12.212:8580/# 测试 nginx# 配置hosts# 192.168.12.212 traefik.nginx.iocurl traefik.nginx.io 参考文档 https://www.kubernetes.org.cn/1870.html https://github.com/rootsongjc/kubernetes-handbook https://github.com/opsnull/follow-me-install-kubernetes-cluster http://www.cnblogs.com/ericnie/p/6965091.html https://docs.traefik.io/user-guide/kubernetes/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装kubernetes-v1.7安装配置calico网络组件]]></title>
    <url>%2Fposts%2F9%2F</url>
    <content type="text"><![CDATA[安装条件 kube-apiserver 必须开启参数 --runtime-config=extensions/v1beta1/networkpolicies=true v1.6以及以前的版本需要在apiserver开启 extensions/v1beta1/networkpolicies v1.7+版本Network Policy已经GA，API版本为 networking.k8s.io/v1 kubelet 配置使用cni网络插件 --network-plugin=cni kube-proxy 必须使用iptables模式 --proxy-mode=iptables，默认就是这个参数 kube-proxy 不能设置 --masquerade-all kubernetes &gt; v1.3.0 安装1. 配置rbac1234mkdir /server/software/k8s/calicocd /server/software/k8s/calicowget http://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/rbac.yamlkubectl apply -f rbac.yaml 2. 拉取镜像1234567docker pull calico/node:v2.4.0docker pull calico/cni:v1.10.0docker pull calico/kube-policy-controller:v0.7.0docker tag calico/node:v2.4.0 quay.io/calico/node:v2.4.0docker tag calico/cni:v1.10.0 quay.io/calico/cni:v1.10.0docker tag calico/kube-policy-controller:v0.7.0 quay.io/calico/kube-policy-controller:v0.7.0 3. 启动1234567891011121314151617181920212223242526272829303132333435363738# 下载配置文件wget http://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/hosted/calico.yaml# 修改etcd相关的配置etcd_endpoints: "https://192.168.12.211:2379,https://192.168.12.212:2379,https://192.168.12.213:2379"# 由于使用了tls需要配置如下位置etcd_ca: "/calico-secrets/etcd-ca"etcd_cert: "/calico-secrets/etcd-cert"etcd_key: "/calico-secrets/etcd-key"...etcd-key: xxxxxxxxxxetcd-cert: xxxxxxxxxxxetcd-ca: xxxxxxxxxxxx...# 上面的内容使用如下方式获取base64 /etc/kubernetes/ssl/kubernetes-key.pem | tr -d '\n'base64 /etc/kubernetes/ssl/kubernetes.pem | tr -d '\n'base64 /etc/kubernetes/ssl/ca.pem | tr -d '\n'# 如果pod不能正常上网，还可以指定网卡。# 在配置node的containers段的env配置如下环境变量- name: IP_AUTODETECTION_METHOD value: "IP_AUTODETECTION_METHOD=can-reach=www.baidu.com"# 或者配置如下形式- name: IP_AUTODETECTION_METHOD value: "IP_AUTODETECTION_METHOD=interface=eth0"# 启动kubectl apply -f calico.yaml# 查看kubectl get pods -n kube-system 4. 测试1234# 启动多个容器查看，看是否能ping通kubectl run calico-test-1 --rm -ti --image busybox /bin/shkubectl run calico-test-2 --rm -ti --image busybox /bin/shkubectl run calico-test-3 --rm -ti --image busybox /bin/sh Network policy 测试默认网络是全部连通的，POD之间可以随意访问。 1. 创建 nginx deploy123kubectl run nginx --image=nginx --replicas=2kubectl expose deployment nginx --port=80kubectl get svc,pod 2. 测试测试应该通过123# 10.254.132.113 为上面获取到的 nginx 的集群 IPkubectl run busybox --rm -ti --image=busybox /bin/shwget --spider --timeout=1 10.254.132.113 3. 限制连接限制连接，只有设置了label access: true才能访问到 nginx 服务1234567891011121314151617cat &gt;nginx-policy.yaml&lt;&lt;EOFkind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata: name: access-nginxspec: podSelector: matchLabels: run: nginx ingress: - from: - podSelector: matchLabels: access: "true"EOFkubectl create -f nginx-policy.yaml 4. 测试测试应该不能通过12kubectl run busybox --rm -ti --image=busybox /bin/shwget --spider --timeout=1 10.254.132.113 5. 设置label测试测试应该通过12kubectl run busybox --rm -ti --labels="access=true" --image=busybox /bin/shwget --spider --timeout=1 10.254.132.113 6. 清理12kubectl delete deploy nginxkubectl delete svc nginx 配置参考文档http://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/hosted/index#configuration-options 参考文档 http://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/ http://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/hosted/hosted http://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/hosted/index#configuration-options http://docs.projectcalico.org/v2.4/reference/node/configuration https://zhuanlan.zhihu.com/p/27699958 https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>calico</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装kubernetes-v1.7 master 高可用配置]]></title>
    <url>%2Fposts%2F8%2F</url>
    <content type="text"><![CDATA[本教程紧接 centos7安装kubernetes-v1.7 请参考。所有master节点均安装 kube-apiserver,kube-scheduler,kube-controller-manager组件。为了方便，本教程使用docker相关组件。 配置 haproxy1. 拉取镜像1docker pull haproxy:1.7.8-alpine 2. 配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758mkdir /etc/haproxycat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOFglobal log 127.0.0.1 local0 info maxconn 50000 uid 99 gid 99 #daemon nbproc 1 pidfile haproxy.piddefaults mode http log 127.0.0.1 local0 info maxconn 50000 retries 3 timeout connect 5s timeout client 30s timeout server 30s timeout check 2slisten admin_stats mode http bind 0.0.0.0:1080 log 127.0.0.1 local0 err stats refresh 30s stats uri /haproxy-status stats realm Haproxy\ Statistics stats auth will:will stats hide-version stats admin if TRUEfrontend k8s-https bind 0.0.0.0:6443 mode tcp #maxconn 50000 default_backend k8s-httpsfrontend k8s-http bind 0.0.0.0:8080 mode tcp #maxconn 50000 default_backend k8s-httpbackend k8s-https mode tcp balance roundrobin server lab1 192.168.12.211:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab2 192.168.12.212:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab3 192.168.12.213:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3backend k8s-http mode tcp balance roundrobin server lab1 192.168.12.211:8080 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab2 192.168.12.212:8080 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab3 192.168.12.213:8080 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3EOF 3. 启动1234567docker run -d --name my-haproxy \-v /etc/haproxy:/usr/local/etc/haproxy:ro \-p 6445:6443 \-p 8090:8080 \-p 1080:1080 \--restart always \haproxy:1.7.8-alpine 4. 查看日志1docker logs my-haproxy 配置 keepalived1. 拉取镜像12#docker pull osixia/keepalived:1.3.5-1docker pull oberthur/docker-keepalived 2. 配置123# 本次使用的镜像不需要配置cat &gt;/etc/keepalived/keepalived.conf&lt;&lt;EOFEOF 3. 启动123456789101112131415161718lsmod | grep ip_vsmodprobe ip_vs# masterdocker run --net=host --cap-add=NET_ADMIN -e VIP=192.168.12.215 \-e VROUTERID=112 -e STATE=BACKUP -e INTERFACE=eth1 -e PRIORITY=100 \-e AUTHPASS=blah \--name keepalived0 \--restart always \-d oberthur/docker-keepalived# backupdocker run --net=host --cap-add=NET_ADMIN -e VIP=192.168.12.215 \-e VROUTERID=112 -e STATE=BACKUP -e INTERFACE=eth1 -e PRIORITY=100 \-e AUTHPASS=blah \--name keepalived0 \--restart always \-d oberthur/docker-keepalived 此时会配置 192.168.12.215 到其中一台机器 配置k8s的注意事项 把api的地址指向负载均衡的地址 192.168.12.215 在创建kubernetes.pem的时候要把这个 VIP 加入其中 测试关闭相应的组件查看日志。12journalctl -f -u kube-controller-managerjournalctl -f -u kube-scheduler]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装配置kubernetes-v1.7]]></title>
    <url>%2Fposts%2F7%2F</url>
    <content type="text"><![CDATA[环境介绍安装kubernetes-v1.7并使用TLS认证，请提前安装好docker环境。本教程使用docker-12.3安装时需要下载的文件都放在/server/software/k8s目录下 安装创建 kubernetes 各组件 TLS 加密通信的证书和秘钥1. 安装 CFSSL12345678910111213mkdir -pv /server/software/k8scd /server/software/k8swget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfomv cfssl_linux-amd64 /usr/local/bin/cfsslmv cfssljson_linux-amd64 /usr/local/bin/cfssljsonchmod +x /usr/local/bin/cfssl* 2. 创建 CA 生成配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243mkdir /root/sslcd /root/ssl#cfssl print-defaults config &gt; config.json#cfssl print-defaults csr &gt; csr.jsoncat &gt;ca-config.json&lt;&lt;EOF&#123; "signing": &#123; "default": &#123; "expiry": "8760h" &#125;, "profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "8760h" &#125; &#125; &#125;&#125;EOFcat &gt;ca-csr.json&lt;&lt;EOF&#123; "CN": "kubernetes", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOF “CN”：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法； “O”：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)； 生成 CA 证书和私钥12cfssl gencert -initca ca-csr.json | cfssljson -bare cals ca* 3. 创建 kubernetes 证书 123456789101112131415161718192021222324252627282930313233343536# 配置cat &gt;kubernetes-csr.json&lt;&lt;EOF&#123; "CN": "kubernetes", "hosts": [ "127.0.0.1", "192.168.12.211", "192.168.12.212", "192.168.12.213", "10.254.0.1", "kubernetes", "kubernetes.default", "kubernetes.default.svc", "kubernetes.default.svc.cluster", "kubernetes.default.svc.cluster.local" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOF# 生成cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetesls kubernetes* 如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，由于该证书后续被 etcd集群和 kubernetes master集群使用，所以上面分别指定了 etcd 集群、kubernetes master集群的主机 IP 和 kubernetes 服务的服务 IP（一般是kue-apiserver 指定的service-cluster-ip-range 网段的第一个IP，如 10.254.0.1。如果配置高可用，注意把负载均衡的VIP加入其中。 4. 创建 admin 证书12345678910111213141516171819202122232425# 配置cat &gt;admin-csr.json&lt;&lt;EOF&#123; "CN": "admin", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "system:masters", "OU": "System" &#125; ]&#125;EOF# 生成cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes admin-csr.json | cfssljson -bare adminls admin* 后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权； kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将Group system:masters 与 Rolecluster-admin 绑定，该 Role 授予了调用kube-apiserver的所有 API的权限； OU 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限； 5. 创建 kube-proxy 证书12345678910111213141516171819202122232425# 配置cat &gt;kube-proxy-csr.json&lt;&lt;EOF&#123; "CN": "system:kube-proxy", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOF# 生成cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxyls kube-proxy* CN 指定该证书的 User 为 system:kube-proxy； kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy相关 API 的权限； 6. 校验证书1cfssl-certinfo -cert kubernetes.pem 7. 分发证书1234567# 复制证书mkdir -p /etc/kubernetes/sslcp *.pem /etc/kubernetes/ssl# copy到其他机器ssh lab2 'mkdir -pv /etc/kubernetes'scp -r /etc/kubernetes/ssl lab2:/etc/kubernetes/ssl 创建 kubeconfig 文件 这一步骤的操作只需要在master节点上操作，然后把配置文件分发到其他node节点即可。 1. 安装 kubectl12345cd /server/software/k8swget https://storage.googleapis.com/kubernetes-release/release/v1.7.2/bin/linux/amd64/kubectlchmod +x kubectlmv kubectl /usr/local/binkubectl version 2. 创建 TLS Bootstrapping Token kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权；kubernetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet生成证书。 12345678export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')cat &gt; token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,"system:kubelet-bootstrap"EOF#将token.csv发到所有机器Master的 /etc/kubernetes/ 目录cp token.csv /etc/kubernetesscp token.csv lab2:/etc/kubernetes 3. 创建 kubelet bootstrapping kubeconfig 文件1234567891011121314151617181920212223cd /etc/kubernetesexport KUBE_APISERVER="https://192.168.12.211:6443"# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \ --token=$&#123;BOOTSTRAP_TOKEN&#125; \ --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig 4. 创建 kube-proxy kubeconfig 文件123456789101112131415161718192021222324export KUBE_APISERVER="https://192.168.12.211:6443"# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-proxy.kubeconfig# 设置客户端认证参数kubectl config set-credentials kube-proxy \ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=kube-proxy \ --kubeconfig=kube-proxy.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 5. 分发 kubeconfig 文件12cp bootstrap.kubeconfig kube-proxy.kubeconfig /etc/kubernetes/scp bootstrap.kubeconfig kube-proxy.kubeconfig lab2:/etc/kubernetes/ 创建高可用 etcd 集群 本次实验使用lab1,lab2,lab3组成etcd集成，lab1为k8s-master节点，lab2,lab3为k8s-node节点。 在集群启动的时候尽量一起启动。因为ca里配置是使用的ip，所有在这里也全要使用ip地址。否则会无法识别ca，报错，无法成功创建集群。 1. 安装12345cd /server/software/k8swget https://github.com/coreos/etcd/releases/download/v3.2.4/etcd-v3.2.4-linux-amd64.tar.gztar xf etcd-v3.2.4-linux-amd64.tar.gzcd etcd-v3.2.4-linux-amd64 &amp;&amp; cp etcd etcdctl /usr/local/binetcdctl -v 2. 创建 etcd 的 systemd unit 文件1234567891011121314151617181920212223242526272829303132333435363738export ETCD_NAME=lab1export INTERNAL_IP=$(hostname -i)mkdir -pv /data/etcdcat &gt; etcd.service &lt;&lt;EOF[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/data/etcdEnvironmentFile=-/etc/etcd/etcd.confExecStart=/usr/local/bin/etcd \\ --name $&#123;ETCD_NAME&#125; \\ --cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls https://$&#123;INTERNAL_IP&#125;:2380 \\ --listen-peer-urls https://$&#123;INTERNAL_IP&#125;:2380 \\ --listen-client-urls https://$&#123;INTERNAL_IP&#125;:2379,https://127.0.0.1:2379 \\ --advertise-client-urls https://$&#123;INTERNAL_IP&#125;:2379 \\ --initial-cluster-token my-etcd-token \\ --initial-cluster lab1=https://192.168.12.211:2380,lab2=https://192.168.12.212:2380,lab3=https://192.168.12.213:2380 \\ --initial-cluster-state new \\ --data-dir=/data/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF 3. 启动12345mv etcd.service /etc/systemd/system/systemctl daemon-reloadsystemctl enable etcdsystemctl start etcdsystemctl status etcd 4. 测试12345etcdctl --endpoints "https://127.0.0.1:2379" \ --ca-file=/etc/kubernetes/ssl/ca.pem \ --cert-file=/etc/kubernetes/ssl/kubernetes.pem \ --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \ cluster-health 配置 kubectl 命令行工具 只需要在需要连接api-server的client上配置。主要用来操作集群。 1234567891011121314151617export KUBE_APISERVER="https://192.168.12.211:6443"# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125;# 设置客户端认证参数kubectl config set-credentials admin \ --client-certificate=/etc/kubernetes/ssl/admin.pem \ --embed-certs=true \ --client-key=/etc/kubernetes/ssl/admin-key.pem# 设置上下文参数kubectl config set-context kubernetes \ --cluster=kubernetes \ --user=admin# 设置默认上下文kubectl config use-context kubernetes 生成的 kubeconfig 被保存到 ~/.kube/config 文件 部署高可用 kubernetes master 集群 kube-scheduler 、 kube-controller-manager 和 kube-apiserver 三者的功能紧密相关；要部署在同台服务器。 同时只能有一个 kube-scheduler 、 kube-controller-manager 进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader； 多个master节点可以直接部署，多个master使用同一个etcd集成即可。然后把kube-apiserver通过负载均衡与其他组件通信。这样就组成了高可用 master 集群。 具体可以参考centos7安装kubernetes-v1.7 master 高可用配置教程 1. 下载安装1234567891011cd /server/software/k8swget https://storage.googleapis.com/kubernetes-release/release/v1.7.2/bin/linux/amd64/kube-apiserverwget https://storage.googleapis.com/kubernetes-release/release/v1.7.2/bin/linux/amd64/kube-controller-managerwget https://storage.googleapis.com/kubernetes-release/release/v1.7.2/bin/linux/amd64/kube-schedulerwget https://storage.googleapis.com/kubernetes-release/release/v1.7.2/bin/linux/amd64/kubectlwget https://storage.googleapis.com/kubernetes-release/release/v1.7.2/bin/linux/amd64/kubeletwget https://storage.googleapis.com/kubernetes-release/release/v1.7.2/bin/linux/amd64/kube-proxychmod +x kube-apiserver kube-controller-manager kubectl kubelet kube-proxy kube-scheduler\cp kube-apiserver kube-controller-manager kubectl kubelet kube-proxy kube-scheduler /usr/local/binscp kube-apiserver kube-controller-manager kubectl kubelet kube-proxy kube-scheduler lab2:/usr/local/bin 2. 配置和启动 kube-apiserver1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192cat &gt;/usr/lib/systemd/system/kube-apiserver.service&lt;&lt;EOF[Unit]Description=Kubernetes API ServiceDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.targetAfter=etcd.service[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/apiserverExecStart=/usr/local/bin/kube-apiserver \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBE_ETCD_SERVERS \\ \$KUBE_API_ADDRESS \\ \$KUBE_API_PORT \\ \$KUBELET_PORT \\ \$KUBE_ALLOW_PRIV \\ \$KUBE_SERVICE_ADDRESSES \\ \$KUBE_ADMISSION_CONTROL \\ \$KUBE_API_ARGSRestart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF# 该配置文件同时被kube-apiserver、kube-controller-manager、# kube-scheduler、kubelet、kube-proxy使用。cat &gt;/etc/kubernetes/config&lt;&lt;EOF#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR="--logtostderr=true"# journal message level, 0 is debugKUBE_LOG_LEVEL="--v=0"# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV="--allow-privileged=true"# How the controller-manager, scheduler, and proxy find the apiserver#KUBE_MASTER="--master=http://192.168.12.211:8080"KUBE_MASTER="--master=http://192.168.12.211:8080"EOFcat &gt;/etc/kubernetes/apiserver&lt;&lt;EOF##### kubernetes system config#### The following values are used to configure the kube-apiserver##### The address on the local server to listen to.#KUBE_API_ADDRESS="--insecure-bind-address=192.168.12.211"KUBE_API_ADDRESS="--advertise-address=192.168.12.211 --bind-address=192.168.12.211 --insecure-bind-address=192.168.12.211"### The port on the local server to listen on.#KUBE_API_PORT="--port=8080"### Port minions listen on#KUBELET_PORT="--kubelet-port=10250"### Comma separated list of nodes in the etcd clusterKUBE_ETCD_SERVERS="--etcd-servers=https://192.168.12.211:2379,192.168.12.212:2379,192.168.12.213:2379"### Address range to use for servicesKUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.254.0.0/16"### default admission control policiesKUBE_ADMISSION_CONTROL="--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota"### Add your own!KUBE_API_ARGS="--authorization-mode=RBAC --runtime-config=rbac.authorization.k8s.io/v1beta1 --kubelet-https=true --experimental-bootstrap-token-auth --token-auth-file=/etc/kubernetes/token.csv --service-node-port-range=30000-32767 --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem --enable-swagger-ui=true --apiserver-count=3 --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/var/lib/audit.log --event-ttl=1h"EOF# 启动systemctl daemon-reloadsystemctl enable kube-apiserversystemctl start kube-apiserversystemctl status kube-apiserver 2. 配置和启动 kube-controller-manager12345678910111213141516171819202122232425262728293031323334cat &gt;/usr/lib/systemd/system/kube-controller-manager.service&lt;&lt;EOFDescription=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/controller-managerExecStart=/usr/local/bin/kube-controller-manager \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBE_MASTER \\ \$KUBE_CONTROLLER_MANAGER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFcat &gt;/etc/kubernetes/controller-manager&lt;&lt;EOF#### The following values are used to configure the kubernetes controller-manager# defaults from config and apiserver should be adequate# Add your own!KUBE_CONTROLLER_MANAGER_ARGS="--address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem --leader-elect=true"EOF# 启动systemctl daemon-reloadsystemctl enable kube-controller-managersystemctl start kube-controller-managersystemctl status kube-controller-manager 3. 配置和启动 kube-scheduler1234567891011121314151617181920212223242526272829303132333435cat &gt;/usr/lib/systemd/system/kube-scheduler.service&lt;&lt;EOF[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/schedulerExecStart=/usr/local/bin/kube-scheduler \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBE_MASTER \\ \$KUBE_SCHEDULER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFcat &gt;/etc/kubernetes/scheduler&lt;&lt;EOF#### kubernetes scheduler config# default config should be adequate# Add your own!KUBE_SCHEDULER_ARGS="--leader-elect=true --address=127.0.0.1"EOF# 启动systemctl daemon-reloadsystemctl enable kube-schedulersystemctl start kube-schedulersystemctl status kube-scheduler 4. 验证节点1kubectl get componentstatuses 以上3个组件在3个master节点安装运行就可以组成高可用。 部署kubernetes node节点1. 安装配置Flanneld 本教程使用手动配置的方法，也可以使用官方的yml文件直接使用k8s部署。https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546cd /server/software/k8swget https://github.com/coreos/flannel/releases/download/v0.8.0/flannel-v0.8.0-linux-amd64.tar.gztar xf flannel-v0.8.0-linux-amd64.tar.gzcp flanneld mk-docker-opts.sh /usr/local/bin/scp flanneld mk-docker-opts.sh lab2:/usr/local/bin/# 配置cat &gt;/usr/lib/systemd/system/flanneld.service&lt;&lt;EOF[Unit]Description=Flanneld overlay address etcd agentAfter=network.targetAfter=network-online.targetWants=network-online.targetAfter=etcd.serviceBefore=docker.service[Service]Type=notifyEnvironmentFile=/etc/sysconfig/flanneldEnvironmentFile=-/etc/sysconfig/docker-networkExecStart=/usr/local/bin/flanneld \$FLANNELD_ARGSExecStartPost=/usr/local/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/dockerRestart=on-failure[Install]WantedBy=multi-user.targetRequiredBy=docker.serviceEOFcat &gt;/etc/sysconfig/flanneld&lt;&lt;EOFFLANNELD_ARGS='--etcd-endpoints=https://192.168.12.211:2379,https://192.168.12.212:2379,https://192.168.12.213:2379 --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem --etcd-prefix=/k8s/network --iface=eth1'EOF# 启动# 如果你要使用 host-gw 模式，可以直接将vxlan改成 host-gw 即可。etcdctl --endpoints "https://127.0.0.1:2379" \--ca-file=/etc/kubernetes/ssl/ca.pem \--cert-file=/etc/kubernetes/ssl/kubernetes.pem \--key-file=/etc/kubernetes/ssl/kubernetes-key.pem \set /k8s/network/config '&#123;"Network":"10.1.0.0/16", "Backend": &#123;"Type": "vxlan"&#125;&#125;'#set /k8s/network/config '&#123;"Network":"10.1.0.0/16", "Backend": &#123;"Type": "host-gw"&#125;&#125;'systemctl daemon-reloadsystemctl enable flanneldsystemctl start flanneldsystemctl status flanneld 3. 重启docker1234567891011121314151617181920212223242526272829303132333435363738394041#修改docker配置mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker# 修改docker启动文件cat &gt;/usr/lib/systemd/system/docker.service&lt;&lt;EOF[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network.target[Service]Type=notify# the default is not to use systemd for cgroups because the delegate issues still# exists and systemd currently does not support the cgroup feature set required# for containers run by dockerEnvironmentFile=-/run/flannel/dockerExecStart=/usr/bin/dockerd \$DOCKER_NETWORK_OPTIONS --registry-mirror=https://tfhzn46h.mirror.aliyuncs.comExecReload=/bin/kill -s HUP $MAINPID# Having non-zero Limit*s causes performance problems due to accounting overhead# in the kernel. We recommend using cgroups to do container-local accounting.LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity# Uncomment TasksMax if your systemd version supports it.# Only systemd 226 and above support this version.#TasksMax=infinityTimeoutStartSec=0# set delegate yes so that systemd does not reset the cgroups of docker containersDelegate=yes# kill only the docker process, not all processes in the cgroupKillMode=process[Install]WantedBy=multi-user.targetEOF# 重启systemctl daemon-reloadsystemctl restart dockersystemctl status dockerps -ef | grep docker 4. 安装配置kubelet kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色(role)， 然后 kubelet 才能有权限创建认证请求(certificate signing requests)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# --user=kubelet-bootstrap 是在 /etc/kubernetes/token.csv 文件中指定的用户名，同时也写入了/etc/kubernetes/bootstrap.kubeconfig 文件# 此步骤在master上操作cd /etc/kuberneteskubectl create clusterrolebinding kubelet-bootstrap \ --clusterrole=system:node-bootstrapper \ --user=kubelet-bootstrapcat &gt;/usr/lib/systemd/system/kubelet.service&lt;&lt;EOF[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/data/kubeletEnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/local/bin/kubelet \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBELET_API_SERVER \\ \$KUBELET_ADDRESS \\ \$KUBELET_PORT \\ \$KUBELET_HOSTNAME \\ \$KUBE_ALLOW_PRIV \\ \$KUBELET_POD_INFRA_CONTAINER \\ \$KUBELET_ARGSRestart=on-failure[Install]WantedBy=multi-user.targetEOFcat &gt;/etc/kubernetes/config&lt;&lt;EOF#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR="--logtostderr=true"# journal message level, 0 is debugKUBE_LOG_LEVEL="--v=0"# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV="--allow-privileged=true"# How the controller-manager, scheduler, and proxy find the apiserver#KUBE_MASTER="--master=http://192.168.12.211:8080"KUBE_MASTER="--master=http://192.168.12.211:8080"EOF# 注意修改相关ipcat &gt;/etc/kubernetes/kubelet&lt;&lt;EOF##### kubernetes kubelet (minion) config### The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)KUBELET_ADDRESS="--address=192.168.12.212"### The port for the info server to serve on#KUBELET_PORT="--port=10250"### You may leave this blank to use the actual hostnameKUBELET_HOSTNAME="--hostname-override=192.168.12.212"### location of the api-serverKUBELET_API_SERVER="--api-servers=http://192.168.12.211:8080"### pod infrastructure container#KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=sz-pg-oam-docker-hub-001.tendcloud.com/library/pod-infrastructure:rhel7"### Add your own!KUBELET_ARGS="--cgroup-driver=cgroupfs --cluster-dns=10.254.0.2 --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --require-kubeconfig --cert-dir=/etc/kubernetes/ssl --cluster-domain=cluster.local. --hairpin-mode promiscuous-bridge --serialize-image-pulls=false"EOF# 启动mkdir -pv /data/kubeletsystemctl daemon-reloadsystemctl enable kubeletsystemctl start kubeletsystemctl status kubelet 5. 通过 kublet 的 TLS 证书请求kubelet 首次启动时向 kube-apiserver 发送证书签名请求，必须通过后 kubernetes 系统才会将该 Node 加入到集群。123456789# 查看kubectl get csr# 通过kubectl certificate approve csr-2b308# 在node节点查看生成的文件ls -l /etc/kubernetes/kubelet.kubeconfigls -l /etc/kubernetes/ssl/kubelet* 6. 配置 kube-proxy123456789101112131415161718192021222324252627282930313233343536cat &gt;/usr/lib/systemd/system/kube-proxy.service&lt;&lt;EOF[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/proxyExecStart=/usr/local/bin/kube-proxy \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBE_MASTER \\ \$KUBE_PROXY_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFcat &gt;/etc/kubernetes/proxy&lt;&lt;EOF#### kubernetes proxy config# default config should be adequate# Add your own!KUBE_PROXY_ARGS="--bind-address=192.168.12.212 --hostname-override=192.168.12.212 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --cluster-cidr=10.254.0.0/16"EOF# 启动systemctl daemon-reloadsystemctl enable kube-proxysystemctl start kube-proxysystemctl status kube-proxy 7. 验证测试12345678910111213141516171819202122# 获取nodekubectl get nodes# 启动deploymentkubectl run nginx --replicas=2 --labels="run=load-balancer-example" --image=nginx:1.9 --port=80# 暴露服务kubectl expose deployment nginx --type=NodePort --name=example-service# 查看kubectl get podskubectl describe svc example-service# 访问curl "10.254.142.220:80"# 访问NodePorthttp://192.168.12.212:31075/# 删除kubectl delete svc example-servicekubectl delete deploy nginx 如果无法拉取镜像手动拉取docker pull pigletfly/pause-amd64:3.0docker tag pigletfly/pause-amd64:3.0 gcr.io/google_containers/pause-amd64:3.0 参考文档 https://www.kubernetes.org.cn/1870.html https://github.com/rootsongjc/kubernetes-handbook https://github.com/opsnull/follow-me-install-kubernetes-cluster]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang Web 框架 Echo 简单使用教程]]></title>
    <url>%2Fposts%2F6%2F</url>
    <content type="text"><![CDATA[简介Echo是一个高性能，灵活可扩展，极简的 go web 框架。支持多种格式的响应如：json、xml、html等。中间件众多，常用组件都能找到如：HTTPS、HTTP/2、WebSocket、JWT、Gzip、CORS、CSRF。天然支持RESTful API 开发，使用其做API开发异常的方便，同时支持使用模板，方便MVC模式的开发。 安装 Echo 如果没有翻墙，先配置如下的hosts 216.58.200.33 go.googlesource.com golang.org www.golang.org 12go get github.com/labstack/echogo get github.com/labstack/echo/middleware Hello World 测试写入如下内容到hello.go123456789101112131415package mainimport ( "net/http" "github.com/labstack/echo")func main() &#123; e := echo.New() e.GET("/", func(c echo.Context) error &#123; return c.String(http.StatusOK, "Hello, World!") &#125;) e.Logger.Fatal(e.Start(":1323"))&#125; 运行1go run hello.go 访问1curl -s http://127.0.0.1:1323/ JSON 格式输出响应内容写入如下内容到json.go123456789101112131415161718192021222324package mainimport ( "net/http" "github.com/labstack/echo")type User struct &#123; Name string `json:"name" xml:"name" form:"name" query:"name"` Email string `json:"email" xml:"email" form:"email" query:"email"`&#125;func main() &#123; e := echo.New() e.GET("/", func(c echo.Context) error &#123; u := new(User) u.Name = "will" u.Email = "will@will.com" return c.JSON(http.StatusOK, u) &#125;) e.Logger.Fatal(e.Start(":1323"))&#125; 运行1go run json.go 访问1curl -s http://127.0.0.1:1323/ 更多输出内容格式（xml、json、file、html、string、attachment、redirect等）参考如下官方文档https://echo.labstack.com/guide/response 路由12345678910111213// 路由e.POST("/users", saveUser)e.GET("/users/:id", getUser)e.PUT("/users/:id", updateUser)e.DELETE("/users/:id", deleteUser)// URI参数// e.GET("/users/:id", getUser)func getUser(c echo.Context) error &#123; // User ID from path `users/:id` id := c.Param("id") return c.String(http.StatusOK, id)&#125; URI查询参数/show?team=x-men&amp;member=wolverine1234567//e.GET("/show", show)func show(c echo.Context) error &#123; // Get team and member from the query string team := c.QueryParam("team") member := c.QueryParam("member") return c.String(http.StatusOK, "team:" + team + ", member:" + member)&#125; 从FORM接受参数POST /save12345678// e.POST("/save", save)func save(c echo.Context) error &#123; // Get name and email name := c.FormValue("name") email := c.FormValue("email") avatar, err := c.FormFile("avatar") return c.String(http.StatusOK, "name:" + name + ", email:" + email)&#125; REST API 示例写入如下内容到rest.go12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package mainimport ( "net/http" "github.com/labstack/echo")type User struct &#123; Id string `json:"id" form:"id"` Name string `json:"name" form:"name"` Email string `json:"email" form:"email"`&#125;func saveUser(c echo.Context) error &#123; u := new(User) u.Name = c.FormValue("name") u.Email = c.FormValue("email") u.Id = "1" return c.JSON(http.StatusCreated, u)&#125;func getUser(c echo.Context) error &#123; u := new(User) id := c.Param("id") u.Name = "will" u.Email = "will@will.com" u.Id = id return c.JSON(http.StatusOK, u)&#125;func updateUser(c echo.Context) error &#123; u := new(User) id := c.Param("id") u.Name = "will" u.Email = "will@will.com" u.Id = id return c.JSON(http.StatusOK, u)&#125;func deleteUser(c echo.Context) error &#123; u := new(User) id := c.Param("id") println(u) println(id) return c.NoContent(http.StatusNoContent)&#125;func main() &#123; e := echo.New() e.POST("/users", saveUser) e.GET("/users/:id", getUser) e.PUT("/users/:id", updateUser) e.DELETE("/users/:id", deleteUser) e.Logger.Fatal(e.Start(":1323"))&#125; 运行1go run rest.go 测试访问1234567891011# GETcurl -s -i http://127.0.0.1:1323/users/1# POSTcurl -s -i -F "name=Joe" -F "email=joe@labstack.com" http://127.0.0.1:1323/users# PUT curl -s -i -X PUT -F "name=Joe" -F "email=joe@labstack.com" http://127.0.0.1:1323/users/1# DELETE curl -s -i -X DELETE http://127.0.0.1:1323/users/1 官方参考文档https://echo.labstack.com/cookbook/crud 使用中间件1234567891011121314151617181920212223242526// 导入中间件模块import "github.com/labstack/echo/middleware"// 全局中间件e.Use(middleware.Logger())e.Use(middleware.Recover())// 组级中间件（只对以/admin开头的URI使用设置的中间件）g := e.Group("/admin")g.Use(middleware.BasicAuth(func(username, password string, c echo.Context) (error, bool) &#123; if username == "joe" &amp;&amp; password == "secret" &#123; return nil, true &#125; return nil, false&#125;))// 路由级中间件（只对单个URI使用设置的中间件）track := func(next echo.HandlerFunc) echo.HandlerFunc &#123; return func(c echo.Context) error &#123; println("request to /users") return next(c) &#125;&#125;e.GET("/users", func(c echo.Context) error &#123; return c.String(http.StatusOK, "/users")&#125;, track) 静态文件12345678910111213// 访问URI：/static/js/main.js 会寻找文件 assets/js/main.jse := echo.New()e.Static("/static", "assets")// 访问URI：/js/main.js 会寻找文件 assets/js/main.jse := echo.New()e.Static("/", "assets")// / ---&gt; public/index.htmle.File("/", "public/index.html")// /favicon.ico ---&gt; images/favicon.icoe.File("/favicon.ico", "images/favicon.ico") 使用模板创建模板文件夹1mkdir -pv public/views 添加模板文件public/views/hello.html1&#123;&#123;define &quot;hello&quot;&#125;&#125;Hello, &#123;&#123;.&#125;&#125;!&#123;&#123;end&#125;&#125; 写入如下内容到template.go123456789101112131415161718192021222324252627282930313233package mainimport ( "html/template" "io" "net/http" "github.com/labstack/echo")type Template struct &#123; templates *template.Template&#125;// 实现 echo.Renderer 接口func (t *Template) Render(w io.Writer, name string, data interface&#123;&#125;, c echo.Context) error &#123; return t.templates.ExecuteTemplate(w, name, data)&#125;func Hello(c echo.Context) error &#123; return c.Render(http.StatusOK, "hello", "Will")&#125;func main() &#123; t := &amp;Template&#123; templates: template.Must(template.ParseGlob("public/views/*.html")), &#125; e := echo.New() e.Renderer = t e.GET("/hello", Hello) e.Logger.Fatal(e.Start(":1323"))&#125; 使用测试组件写入如下内容到handler.go12345678910111213141516171819202122232425262728293031323334package handlerimport ( "net/http" "github.com/labstack/echo")type ( User struct &#123; Name string `json:"name" form:"name"` Email string `json:"email" form:"email"` &#125; handler struct &#123; db map[string]*User &#125;)func (h *handler) createUser(c echo.Context) error &#123; u := new(User) if err := c.Bind(u); err != nil &#123; return err &#125; return c.JSON(http.StatusCreated, u)&#125;func (h *handler) getUser(c echo.Context) error &#123; email := c.Param("email") user := h.db[email] if user == nil &#123; return echo.NewHTTPError(http.StatusNotFound, "user not found") &#125; return c.JSON(http.StatusOK, user)&#125; 写入如下内容到handler_test.go12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package handlerimport ( "net/http" "net/http/httptest" "strings" "testing" "github.com/labstack/echo" "github.com/stretchr/testify/assert")var ( mockDB = map[string]*User&#123; "jon@labstack.com": &amp;User&#123;"Jon Snow", "jon@labstack.com"&#125;, &#125; userJSON = `&#123;"name":"Jon Snow","email":"jon@labstack.com"&#125;`)func TestCreateUser(t *testing.T) &#123; // Setup e := echo.New() req := httptest.NewRequest(echo.POST, "/", strings.NewReader(userJSON)) req.Header.Set(echo.HeaderContentType, echo.MIMEApplicationJSON) rec := httptest.NewRecorder() c := e.NewContext(req, rec) h := &amp;handler&#123;mockDB&#125; // Assertions if assert.NoError(t, h.createUser(c)) &#123; assert.Equal(t, http.StatusCreated, rec.Code) assert.Equal(t, userJSON, rec.Body.String()) &#125;&#125;func TestGetUser(t *testing.T) &#123; // Setup e := echo.New() req := httptest.NewRequest(echo.GET, "/", nil) rec := httptest.NewRecorder() c := e.NewContext(req, rec) c.SetPath("/users/:email") c.SetParamNames("email") c.SetParamValues("jon@labstack.com") h := &amp;handler&#123;mockDB&#125; // Assertions if assert.NoError(t, h.getUser(c)) &#123; assert.Equal(t, http.StatusOK, rec.Code) assert.Equal(t, userJSON, rec.Body.String()) &#125;&#125; 运行测试12go get github.com/stretchr/testify/assertgo test 更多用法123456789101112131415// 使用 Formf := make(url.Values)f.Set("name", "Jon Snow")f.Set("email", "jon@labstack.com")req := httptest.NewRequest(echo.POST, "/", strings.NewReader(f.Encode()))req.Header.Set(echo.HandlerContentType, echo.MIMEApplicationForm)// 设置URI参数c.SetParamNames("id", "email")c.SetParamValues("1", "jon@labstack.com")// 设置查询参数q := make(url.Values)q.Set("email", "jon@labstack.com")req := http.NewRequest(echo.POST, "/?"+q.Encode(), nil) 使用cookie1234567891011121314151617181920212223242526272829// 创建cookiefunc writeCookie(c echo.Context) error &#123; cookie := new(http.Cookie) cookie.Name = "username" cookie.Value = "jon" cookie.Expires = time.Now().Add(24 * time.Hour) c.SetCookie(cookie) return c.String(http.StatusOK, "write a cookie")&#125;// 读取单个cookiefunc readCookie(c echo.Context) error &#123; cookie, err := c.Cookie("username") if err != nil &#123; return err &#125; fmt.Println(cookie.Name) fmt.Println(cookie.Value) return c.String(http.StatusOK, "read a cookie")&#125;// 读取所有cookiefunc readAllCookies(c echo.Context) error &#123; for _, cookie := range c.Cookies() &#123; fmt.Println(cookie.Name) fmt.Println(cookie.Value) &#125; return c.String(http.StatusOK, "read all cookie")&#125; 参考文档 https://echo.labstack.com/guide https://echo.labstack.com/cookbook https://echo.labstack.com/middleware]]></content>
      <categories>
        <category>Web开发</category>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>Web开发</tag>
        <tag>Echo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go常用Web框架简介]]></title>
    <url>%2Fposts%2F5%2F</url>
    <content type="text"><![CDATA[常用web框架 echo 高性能，可扩展，极简 go Web 框架。 中间件多，性能高，REST支持，HTTPS支持，HTTP/2支持，WebSocket支持。 123456789101112131415package mainimport ( "net/http" "github.com/labstack/echo")func main() &#123; e := echo.New() e.GET("/", func(c echo.Context) error &#123; return c.String(http.StatusOK, "Hello, World!") &#125;) e.Logger.Fatal(e.Start(":1323"))&#125; gin 类martini，高性能 go Web 框架。 性能高，REST支持，HTTPS支持。 12345678910111213package mainimport "gopkg.in/gin-gonic/gin.v1"func main() &#123; r := gin.Default() r.GET("/ping", func(c *gin.Context) &#123; c.JSON(200, gin.H&#123; "message": "pong", &#125;) &#125;) r.Run() // listen and serve on 0.0.0.0:8080&#125; iris 最高性能 go Web 框架。 中间件多，性能高，REST支持。 1234567891011121314package mainimport ( "github.com/kataras/iris" "github.com/kataras/iris/context")func main() &#123; app := iris.New() app.Handle("GET", "/", func(ctx context.Context) &#123; ctx.HTML("&lt;b&gt; Hello world! &lt;/b&gt;") &#125;) app.Run(iris.Addr(":8080"))&#125; revel 高生产力，全功能 go Web 框架。 功能完整，MVC构架。 12revel new myapprevel run myapp martini Martini是一个强大为了编写模块化Web应用而生的GO语言框架。 1234567891011package mainimport "github.com/go-martini/martini"func main() &#123; m := martini.Classic() m.Get("/", func() string &#123; return "Hello world!" &#125;) m.Run()&#125; go-json-rest 易于构建RESTful JSON APIs的go Web框架。 中间件多，REST支持，HTTPS支持，WebSocket支持。 12345678910111213141516package mainimport ( "github.com/ant0ine/go-json-rest/rest" "log" "net/http")func main() &#123; api := rest.NewApi() api.Use(rest.DefaultDevStack...) api.SetApp(rest.AppSimple(func(w rest.ResponseWriter, r *rest.Request) &#123; w.WriteJson(map[string]string&#123;"Body": "Hello World!"&#125;) &#125;)) log.Fatal(http.ListenAndServe(":8080", api.MakeHandler()))&#125; utron 轻量级MVC的 go Web 框架。 macaron 高生产力 go Web 框架。 1234567891011package mainimport "gopkg.in/macaron.v1"func main() &#123; m := macaron.Classic() m.Get("/", func() string &#123; return "Hello world!" &#125;) m.Run()&#125; buffalo 快速的 go Web 开发框架。 12buffalo new cokebuffalo dev go-tigertonic json web 服务开发框架。 1234567891011121314151617type MyRequest struct &#123; ID string `json:"id"` Stuff interface&#123;&#125; `json:"stuff"`&#125;type MyResponse struct &#123; ID string `json:"id"` Stuff interface&#123;&#125; `json:"stuff"`&#125;func myHandler(u *url.URL, h http.Header, *MyRequest) (int, http.Header, *MyResponse, error) &#123; return http.StatusOK, nil, &amp;MyResponse&#123;"ID", "STUFF"&#125;, nil&#125;mux := tigertonic.NewTrieServeMux()mux.Handle("POST", "/stuff", tigertonic.Timed(tigertonic.Marshaled(myHandler), "myHandler", nil))tigertonic.NewServer(":8000", tigertonic.Logged(mux, nil)).ListenAndServe() faygo 快速方便的高性能 go Web 开发框架，特别是开发API。 12345678910111213141516171819202122232425262728293031323334353637package mainimport ( // "mime/multipart" "time" "github.com/henrylee2cn/faygo")type Index struct &#123; Id int `param:"&lt;in:path&gt; &lt;required&gt; &lt;desc:ID&gt; &lt;range: 0:10&gt;"` Title string `param:"&lt;in:query&gt; &lt;nonzero&gt;"` Paragraph []string `param:"&lt;in:query&gt; &lt;name:p&gt; &lt;len: 1:10&gt; &lt;regexp: ^[\\w]*$&gt;"` Cookie string `param:"&lt;in:cookie&gt; &lt;name:faygoID&gt;"` // Picture *multipart.FileHeader `param:"&lt;in:formData&gt; &lt;name:pic&gt; &lt;maxmb:30&gt;"`&#125;func (i *Index) Serve(ctx *faygo.Context) error &#123; if ctx.CookieParam("faygoID") == "" &#123; ctx.SetCookie("faygoID", time.Now().String()) &#125; return ctx.JSON(200, i)&#125;func main() &#123; app := faygo.New("myapp", "0.1") // Register the route in a chain style app.GET("/index/:id", new(Index)) // Register the route in a tree style // app.Route( // app.NewGET("/index/:id", new(Index)), // ) // Start the service faygo.Run()&#125; tango 微内核，可插拔，高性能 go Web 开发框架。 12345678910111213141516171819202122232425package mainimport ( "errors" "github.com/lunny/tango")type Action struct &#123; tango.JSON&#125;func (Action) Get() interface&#123;&#125; &#123; if true &#123; return map[string]string&#123; "say": "Hello tango!", &#125; &#125; return errors.New("something error")&#125;func main() &#123; t := tango.Classic() t.Get("/", new(Action)) t.Run()&#125; traffic 受 Sinatra 启发的多正则匹配的 go Web 框架。 123456789101112131415161718192021222324252627package mainimport ( "net/http" "github.com/pilu/traffic" "fmt")func rootHandler(w traffic.ResponseWriter, r *traffic.Request) &#123; fmt.Fprint(w, "Hello World\n")&#125;func pageHandler(w traffic.ResponseWriter, r *traffic.Request) &#123; params := r.URL.Query() fmt.Fprintf(w, "Category ID: %s\n", params.Get("category_id")) fmt.Fprintf(w, "Page ID: %s\n", params.Get("id"))&#125;func main() &#123; router := traffic.New() // Routes router.Get("/", rootHandler) router.Get("/categories/:category_id/pages/:id", pageHandler) router.Run()&#125; rest-layer 让创建 REST API 更简单的 go Web 框架。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142package mainimport ( "log" "net/http" "github.com/rs/rest-layer-mem" "github.com/rs/rest-layer/resource" "github.com/rs/rest-layer/rest" "github.com/rs/rest-layer/schema")var ( // Define a user resource schema user = schema.Schema&#123; Description: `The user object`, Fields: schema.Fields&#123; "id": &#123; Required: true, // When a field is read-only, on default values or hooks can // set their value. The client can't change it. ReadOnly: true, // This is a field hook called when a new user is created. // The schema.NewID hook is a provided hook to generate a // unique id when no value is provided. OnInit: schema.NewID, // The Filterable and Sortable allows usage of filter and sort // on this field in requests. Filterable: true, Sortable: true, Validator: &amp;schema.String&#123; Regexp: "^[0-9a-v]&#123;20&#125;$", &#125;, &#125;, "created": &#123; Required: true, ReadOnly: true, Filterable: true, Sortable: true, OnInit: schema.Now, Validator: &amp;schema.Time&#123;&#125;, &#125;, "updated": &#123; Required: true, ReadOnly: true, Filterable: true, Sortable: true, OnInit: schema.Now, // The OnUpdate hook is called when the item is edited. Here we use // provided Now hook which just return the current time. OnUpdate: schema.Now, Validator: &amp;schema.Time&#123;&#125;, &#125;, // Define a name field as required with a string validator "name": &#123; Required: true, Filterable: true, Validator: &amp;schema.String&#123; MaxLen: 150, &#125;, &#125;, &#125;, &#125; // Define a post resource schema post = schema.Schema&#123; Description: `Represents a blog post`, Fields: schema.Fields&#123; // schema.*Field are shortcuts for common fields // (identical to users' same fields) "id": schema.IDField, "created": schema.CreatedField, "updated": schema.UpdatedField, // Define a user field which references the user owning the post. // See bellow, the content of this field is enforced by the fact // that posts is a sub-resource of users. "user": &#123; Required: true, Filterable: true, Validator: &amp;schema.Reference&#123; Path: "users", &#125;, &#125;, "published": &#123; Required: true, Filterable: true, Default: false, Validator: &amp;schema.Bool&#123;&#125;, &#125;, "title": &#123; Required: true, Validator: &amp;schema.String&#123; MaxLen: 150, &#125;, &#125;, "body": &#123; // Dependency defines that body field can't be changed if // the published field is not "false". Dependency: schema.Q(`&#123;"published": false&#125;`), Validator: &amp;schema.String&#123; MaxLen: 100000, &#125;, &#125;, &#125;, &#125;)func main() &#123; // Create a REST API resource index index := resource.NewIndex() // Add a resource on /users[/:user_id] users := index.Bind("users", user, mem.NewHandler(), resource.Conf&#123; // We allow all REST methods // (rest.ReadWrite is a shortcut for []resource.Mode&#123;resource.Create, // resource.Read, resource.Update, resource.Delete, resource,List&#125;) AllowedModes: resource.ReadWrite, &#125;) // Bind a sub resource on /users/:user_id/posts[/:post_id] // and reference the user on each post using the "user" field of the posts resource. users.Bind("posts", "user", post, mem.NewHandler(), resource.Conf&#123; // Posts can only be read, created and deleted, not updated AllowedModes: []resource.Mode&#123;resource.Read, resource.List, resource.Create, resource.Delete&#125;, &#125;) // Create API HTTP handler for the resource graph api, err := rest.NewHandler(index) if err != nil &#123; log.Fatalf("Invalid API configuration: %s", err) &#125; // Bind the API under /api/ path http.Handle("/api/", http.StripPrefix("/api/", api)) // Serve it log.Print("Serving API on http://localhost:8080") if err := http.ListenAndServe(":8080", nil); err != nil &#123; log.Fatal(err) &#125;&#125; gongular 让开发 API 简单的 go Web 框架。 123456789101112type WelcomeMessage struct &#123; Message string Date time.Time&#125;g := gongular.NewRouter()g.GET("/", func(c *gongular.Context) WelcomeMessage &#123; return WelcomeMessage&#123; Message: "Hello, you are coming from: " + c.Request().RemoteAddr, Date: time.Now(), &#125;&#125;) lessgo 简单、稳定、高效、灵活的 go Web 开发框架。 12345678910111213141516171819import ( "github.com/henrylee2cn/lessgo" "github.com/henrylee2cn/lessgoext/swagger" _ "github.com/henrylee2cn/lessgoext/dbservice/xorm" // _ "github.com/henrylee2cn/lessgoext/dbservice/gorm" _ "github.com/henrylee2cn/lessgo_demo/middleware" _ "github.com/henrylee2cn/lessgo_demo/router")func main() &#123; // 开启自动api文档，通过config/apidoc_allow.myconfig进行配置 swagger.Reg() // 指定根目录URL lessgo.SetHome("/home") // 开启网络服务 lessgo.Run()&#125; neo 极其简单，快速，微内核的 go Web 框架。 123456789101112131415package mainimport ( "github.com/ivpusic/neo")func main() &#123; app := neo.App() app.Get("/", func(ctx *neo.Ctx) (int, error) &#123; return 200, ctx.Res.Text("I am Neo Programmer") &#125;) app.Start()&#125; gondola 快速开发 go Web 框架。 12345678910111213141516package mainimport ( "gnd.la/app" "gnd.la/config")var ( App *app.App)func init() &#123; config.MustParse() App = app.New() App.HandleNamed("^/$", MainHandler, "main")&#125; golf 快速简单高性能轻量的 go Web 开发框架。 123456789101112131415161718package mainimport "github.com/dinever/golf"func mainHandler(ctx *golf.Context) &#123; ctx.Send("Hello World!")&#125;func pageHandler(ctx *golf.Context) &#123; ctx.Send("Page: " + ctx.Param("page"))&#125;func main() &#123; app := golf.New() app.Get("/", mainHandler) app.Get("/p/:page/", pageHandler) app.Run(":9000")&#125; go-relax 专为 RESTful API 开发的 go Web 框架。 123456789101112131415161718package mainimport ( "github.com/codehack/go-relax")type Hello stringfunc (h *Hello) Index(ctx *relax.Context) &#123; ctx.Respond(h)&#125;func main() &#123; h := Hello("hello world!") svc := relax.NewService("http://api.company.com/") svc.Resource(&amp;h) svc.Run()&#125; gem 高性能，易用，REST支持，HTTP/2支持的 go Web 框架。 123456789101112131415161718192021222324package mainimport ( "log" "github.com/go-gem/gem")func index(ctx *gem.Context) &#123; ctx.HTML(200, "hello world")&#125;func main() &#123; // Create server. srv := gem.New(":8080") // Create router. router := gem.NewRouter() // Register handler router.GET("/", index) // Start server. log.Println(srv.ListenAndServe(router.Handler()))&#125; goat 极简JSON API go Web 框架，REST支持。 123456789101112131415161718192021package mainimport ( "net/http" "github.com/bahlo/goat")func helloHandler(w http.ResponseWriter, r *http.Request, p goat.Params) &#123; goat.WriteJSON(w, map[string]string&#123; "hello": p["name"], &#125;)&#125;func main() &#123; r := goat.New() r.Get("/hello/:name", "hello_url", helloHandler) r.Run(":8080")&#125; rex 现在化 go Web 开发框架。 12345678910111213141516package mainimport ( "io" "net/http" "github.com/goanywhere/rex")func main() &#123; app := rex.New() app.Get("/", func(w http.ResponseWriter, r *http.Request) &#123; io.WriteString(w, "Hello World") &#125;) app.Run()&#125; air 一个理想的 RESTful API go Web 开发框架。 12345678910111213package mainimport "github.com/sheng/air"func main() &#123; a := air.New() a.GET("/", homeHandler) a.Serve()&#125;func homeHandler(c *air.Context) error &#123; return c.String("Hello, 世界")&#125; yarf 又一个 高性能 REST 开发框架。 1234567891011121314151617181920212223242526package mainimport ( "github.com/yarf-framework/yarf")// Define a simple resourcetype Hello struct &#123; yarf.Resource&#125;// Implement the GET methodfunc (h *Hello) Get(c *yarf.Context) error &#123; c.Render("Hello world!") return nil&#125;// Run app server on http://localhost:8080func main() &#123; y := yarf.New() y.Add("/", new(Hello)) y.Start(":8080")&#125; 适合开发 REST API 的框架 github收藏较多的框架 echo gin iris go-json-rest martini macaron 小众但可能好用的框架 tango neo go-relax gem goat air yarf faygo traffic rest-layer gongular lessgo go-tigertonic]]></content>
      <categories>
        <category>Web开发</category>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>Web开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown语法总结]]></title>
    <url>%2Fposts%2F4%2F</url>
    <content type="text"><![CDATA[简介Markdown 的目标是实现「易读易写」。Markdown 的语法全由一些符号所组成，这些符号经过精挑细选，其作用一目了然。比如：在文字两旁加上星号，看起来就像*强调*。Markdown的列表看起来，就是列表。Markdown 的区块引用看起来就真的像是引用一段文字。并且兼容 HTML 语法标签，可以在markdown文件里直接使用HTML标签如: &lt;div&gt;、&lt;table&gt;、&lt;pre&gt;、&lt;p&gt;、&lt;span&gt;、&lt;cite&gt;、&lt;del&gt;。并且现在有很多工具可以把markdown文件转换为pdf、html等格式，非常便于阅读，分享。 基本语法区块元素段落和换行一个 Markdown 段落是由一个或多个连续的文本行组成，它的前后要有一个以上的空行（空行的定义是显示上看起来像是空的，便会被视为空行。比方说，若某一行只包含空格和制表符，则该行也会被视为空行）。普通段落不该用空格或制表符来缩进。 如果你确实想要依赖 Markdown 来插入 &lt;br/&gt; 标签的话，在插入处先按入两个以上的空格然后回车。 标题在行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶，例如：12345# 这是 H1 ### 这是 H2 ##### 这是 H3 ###### 区块引用Markdown 文件中建立一个区块引用，那会看起来像是你自己先断好行，然后在每行的最前面加上 &gt; ：123&gt; 这是一个引用&gt; 这是上一个引用的接着部分&gt; 这是这个引用的最后部分 这是一个引用这是上一个引用的接着部分这是这个引用的最后部分 Markdown 也允许你偷懒只在整个段落的第一行最前面加上 &gt; ：123&gt; 这是一个引用这是上一个引用的接着部分这是这个引用的最后部分 这是一个引用这是上一个引用的接着部分这是这个引用的最后部分 区块引用可以嵌套（例如：引用内的引用），只要根据层次加上不同数量的 &gt; ：123&gt; 这是一个引用&gt; &gt; 这是一个引用里的引用&gt; 这是这个引用的最后部分 这是一个引用 这是一个引用里的引用这是这个引用的最后部分 引用的区块内也可以使用其他的 Markdown 语法，包括标题、列表、代码区块等：1234&gt; #### 这是一个标题&gt; - 这是一个引用&gt; - 这是上一个引用的接着部分&gt; - 这是这个引用的最后部分 这是一个标题 这是一个引用 这是上一个引用的接着部分 这是这个引用的最后部分 列表Markdown 支持有序列表和无序列表。无序列表使用星号、加号或是减号作为列表标记：123456789101112131415* Red* Green* Blue+ Red+ Green+ Blue- Red- Green- Blue1. Bird2. McHale3. Parish Red Green Blue Red Green Blue Red Green Blue Bird McHale Parish 内容用固定的缩进整理：12345* Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus.* Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse id sem consectetuer libero luctus adipiscing Lorem ipsum dolor sit amet, consectetuer adipiscing elit.Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi,viverra nec, fringilla in, laoreet vitae, risus. Donec sit amet nisl. Aliquam semper ipsum sit amet velit.Suspendisse id sem consectetuer libero luctus adipiscing 偷懒的做法12345* Lorem ipsum dolor sit amet, consectetuer adipiscing elit.Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi,viverra nec, fringilla in, laoreet vitae, risus.* Donec sit amet nisl. Aliquam semper ipsum sit amet velit.Suspendisse id sem consectetuer libero luctus adipiscing. Lorem ipsum dolor sit amet, consectetuer adipiscing elit.Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi,viverra nec, fringilla in, laoreet vitae, risus. Donec sit amet nisl. Aliquam semper ipsum sit amet velit.Suspendisse id sem consectetuer libero luctus adipiscing. 列表项目可以包含多个段落，每个项目下的段落都必须缩进 4 个空格或是 1 个制表符：123456789101112131415161718191. This is a list item with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. Donec sit amet nisl. Aliquam semper ipsum sit amet velit.2. Suspendisse id sem consectetuer libero luctus adipiscing.* This is a list item with two paragraphs. This is the second paragraph in the list item. You&apos;reonly required to indent the first line. Lorem ipsum dolorsit amet, consectetuer adipiscing elit.* Another item in the same list. This is a list item with two paragraphs. Lorem ipsum dolorsit amet, consectetuer adipiscing elit. Aliquam hendreritmi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreetvitae, risus. Donec sit amet nisl. Aliquam semper ipsumsit amet velit. Suspendisse id sem consectetuer libero luctus adipiscing. This is a list item with two paragraphs. This is the second paragraph in the list item. You’reonly required to indent the first line. Lorem ipsum dolorsit amet, consectetuer adipiscing elit. Another item in the same list. 如果要在列表项目内放进引用，那 &gt; 就需要缩进：1234* A list item with a blockquote: &gt; This is a blockquote &gt; inside a list item. A list item with a blockquote: This is a blockquoteinside a list item. 如果要放代码区块的话，该区块就需要缩进两次，也就是 8 个空格或是 2 个制表符：123* 一列表项包含一个列表区块： &lt;代码写在这&gt; 一列表项包含一个列表区块： &lt;代码写在这&gt; 代码区块要在 Markdown 中建立代码区块很简单，只要简单地缩进 4 个空格或是 1 个制表符就可以，例如，下面的输入：123这是一个普通段落： 这是一个代码区块。 这是一个普通段落： 这是一个代码区块。 代码也可以使用3个连续的反引号，后面还可以指明代码语言（部分编辑器支持，非官方语法）: 1echo hello 1echo hello 表格支持12345| 项目 | 价格 | 数量 || -------- | -----: | :----: || 计算机 | \$1600 | 5 || 手机 | \$12 | 12 || 管线 | \$1 | 234 | 项目 价格 数量 计算机 \$1600 5 手机 \$12 12 管线 \$1 234 分隔线你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线：123456789* * *********- - ---------------------------------------- 区段元素链接要建立一个行内式的链接，只要在方块括号后面紧接着圆括号并插入网址链接即可，如果你还想要加上链接的 title 文字（鼠标放在链接上时显示的文件），只要在网址后面，用双引号把 title文字包起来即可，例如：123This is [an example](http://example.com/ &quot;Title&quot;) inline link.[This link](http://example.net/) has no title attribute. This is an example inline link. This link has no title attribute. 一个参考式链接的范例：123456I get 10 times more traffic from [Google] [1] than from[Yahoo] [2] or [MSN] [3]. [1]: http://google.com/ &quot;Google&quot; [2]: http://search.yahoo.com/ &quot;Yahoo Search&quot; [3]: http://search.msn.com/ &quot;MSN Search&quot; I get 10 times more traffic from Google than fromYahoo or MSN. 改成用链接名称的方式写：123456I get 10 times more traffic from [Google][] than from[Yahoo][] or [MSN][]. [google]: http://google.com/ &quot;Google&quot; [yahoo]: http://search.yahoo.com/ &quot;Yahoo Search&quot; [msn]: http://search.msn.com/ &quot;MSN Search&quot; I get 10 times more traffic from Google than fromYahoo or MSN. 强调Markdown 使用星号（）和底线（_）作为标记强调字词的符号，被 或 _ 包围的字词会被转成用 &lt;em&gt; 斜体标签包围，用两个 * 或 _ 包起来的话，则会被转成 &lt;strong&gt;加粗，例如：1234567*single asterisks*_single underscores_**double asterisks**__double underscores__ single asterisks single underscores double asterisks double underscores 如果你的 * 和 _ 两边都有空白的话，它们就只会被当成普通的符号。 如果要在文字前后直接插入普通的星号或底线，你可以用反斜线：1\*this text is surrounded by literal asterisks\* *this text is surrounded by literal asterisks* 部分编辑器支持删除线 ~~这是一段错误的文本。~~ 这是一段错误的文本。 代码如果要标记一小段行内代码，你可以用反引号把它包起来（`），例如：1Use the `printf()` function. Use the printf() function. 如果要在代码区段内插入反引号，你可以用多个反引号来开启和结束代码区段：1``There is a literal backtick (`) here.`` 代码区段的起始和结束端都可以放入一个空白，起始端后面一个，结束端前面一个，这样你就可以在区段的一开始就插入反引号：123A single backtick in a code span: `` ` ``A backtick-delimited string in a code span: `` `foo` `` A single backtick in a code span: ` A backtick-delimited string in a code span: `foo` 图片Markdown 使用一种和链接很相似的语法来标记图片，同样也允许两种样式： 行内式和参考式。 行内式的图片语法看起来像是：123![Alt text](http://ojz1mcltu.bkt.clouddn.com/animals-august2015.jpg)![Alt text](http://ojz1mcltu.bkt.clouddn.com/animals-august2015.jpg &quot;docker stack&quot;) 参考式的图片语法则长得像这样：12![Alt text][id][id]: http://ojz1mcltu.bkt.clouddn.com/animals-august2015.jpg &quot;docker stack&quot; ![Alt text][id][id]: http://ojz1mcltu.bkt.clouddn.com/animals-august2015.jpg “docker stack” Markdown 还没有办法指定图片的宽高，如果你需要的话，你可以使用普通的 &lt;img&gt; 标签。 其它自动链接Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用方括号包起来，Markdown 就会自动把它转成链接。一般网址的链接文字就和链接地址一样，例如：12&lt;http://example.com/&gt;&lt;address@example.com&gt; http://example.com/&#97;&#100;&#x64;&#x72;&#101;&#x73;&#x73;&#64;&#x65;&#x78;&#97;&#109;&#x70;&#108;&#x65;&#46;&#99;&#111;&#109; 反斜杠Markdown 可以利用反斜杠来插入一些在语法中有其它意义的符号，例如：如果你想要用星号加在文字旁边的方式来做出强调效果（但不用 标签），你可以在星号的前面加上反斜杠：1\*literal asterisks\* Markdown 支持以下这些符号前面加上反斜杠来帮助插入普通的符号： \ 反斜线 ` 反引号 * 星号 _ 底线 {} 花括号 [] 方括号 () 括弧 # 井字号 + 加号 - 减号 . 英文句点 ! 惊叹号 待办事宜 Todo 列表使用带有[ ]或[x]（未完成或已完成）项的列表语法撰写一个待办事宜列表，并且支持子列表嵌套以及混用Markdown语法，此语法非标准语法，不是所有markdown编辑器都支持。 - [ ] **Cmd Markdown 开发** - [ ] 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率 - [ ] 支持以 PDF 格式导出文稿 - [x] 新增Todo列表功能 [语法参考](https://github.com/blog/1375-task-lists-in-gfm-issues-pulls-comments) - [x] 改进 LaTex 功能 - [x] 修复 LaTex 公式渲染问题 - [x] 新增 LaTex 公式编号功能 [ ] Cmd Markdown 开发 [ ] 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率 [ ] 支持以 PDF 格式导出文稿 [x] 新增Todo列表功能 语法参考 [x] 改进 LaTex 功能 [x] 修复 LaTex 公式渲染问题 [x] 新增 LaTex 公式编号功能 语法参考 参考文档 http://www.appinn.com/markdown/ https://www.zybuluo.com/mdeditor?url=https%3A%2F%2Fwww.zybuluo.com%2Fstatic%2Feditor%2Fmd-help.markdown]]></content>
      <categories>
        <category>文档相关</category>
      </categories>
      <tags>
        <tag>doc</tag>
        <tag>markdown</tag>
        <tag>pdf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[REST接口设计规范总结]]></title>
    <url>%2Fposts%2F3%2F</url>
    <content type="text"><![CDATA[简介Representational State Transfer 简称 REST 描述了一个架构样式的网络系统。REST 指的是一组架构约束条件和原则。满足这些约束条件和原则的应用程序或设计就是 RESTful。 概念: 资源（Resources） REST是”表现层状态转化”，其实它省略了主语。”表现层”其实指的是”资源”的”表现层”。那么什么是资源呢？就是我们平常上网访问的一张图片、一个文档、一个视频等。这些资源我们通过URI来定位，也就是一个URI表示一个资源。 表现层（Representation）资源是做一个具体的实体信息，他可以有多种的展现方式。而把实体展现出来就是表现层，例如一个txt文本信息，他可以输出成html、json、xml等格式，一个图片他可以jpg、png等方式展现，这个就是表现层的意思。URI确定一个资源，但是如何确定它的具体表现形式呢？应该在HTTP请求的头信息中用Accept和Content-Type字段指定，这两个字段才是对”表现层”的描述。 状态转化（State Transfer）访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，肯定涉及到数据和状态的变化。而HTTP协议是无状态的，那么这些状态肯定保存在服务器端，所以如果客户端想要通知服务器端改变数据和状态的变化，肯定要通过某种方式来通知它。 URI格式规范 URI中尽量使用连字符”-“代替下划线”_”的使用 URI中统一使用小写字母 URI中不要包含文件(脚本)的扩展名 资源的原型 文档(Document) 123456文档是资源的单一表现形式，可以理解为一个对象，或者数据库中的一条记录。在请求文档时，要么返回文档对应的数据，要么会返回一个指向另外一个资源(文档)的链接。以下是几个基于文档定义的URI例子：https://api.example.com/users/willhttps://api.example.com/posts/1https://api.example.com/posts/1/comments/1 集合(Collection) 1234集合可以理解为是资源的一个容器(目录)，我们可以向里面添加资源(文档)。例如：https://api.example.com/usershttps://api.example.com/postshttps://api.example.com/posts/1/comments 仓库(Store) 123456仓库是客户端来管理的一个资源库，客户端可以向仓库中新增资源或者删除资源。客户端也可以批量获取到某个仓库下的所有资源。仓库中的资源对外的访问不会提供单独URI的，客户端在创建资源时候的URI除外。例如：PUT /users/1234/favorites/posts/1 上面的例子我们可以理解为，我们向一个id是1234的用户的仓库(收藏夹)中，添加了一个id为1的post资源。通俗点儿说：就是用户收藏了一个自己喜爱的id为1的文章。 控制器(Controller) 12345678控制器资源模型，可以执行一个方法，支持参数输入，结果返回。 是为了除了标准操作:增删改查(CRUD)以外的一些逻辑操作。控制器(方法)一般定义子URI中末尾，并且不会有子资源(控制器)。例如：向用户重发ID为245743的消息POST /alerts/245743/resend 发布ID为1的文章POST /posts/1/publish 把动作转换成资源 123456789把动作转换成可以执行 CRUD 操作的资源， github 就是用了这种方法。比如“喜欢”一个 gist，就增加一个 /gists/:id/star 子资源，然后对其进行操作：“喜欢”使用 PUT /gists/:id/star，“取消喜欢”使用 DELETE /gists/:id/star或者使用 POST /gists/:id/unstar另外一个例子是 Fork，这也是一个动作，但是在 gist 下面增加 forks资源，就能把动作变成 CRUD 兼容的：POST /gists/:id/forks 可以执行用户 fork 的动作。 URI命名规范 文档(Document)类型的资源用名词(短语)单数命名 集合(Collection)类型的资源用名词(短语)复数命名 仓库(Store)类型的资源用名词(短语)复数命名 控制器(Controller)类型的资源用动词(短语)命名 URI中有些字段可以是变量，在实际使用中可以按需替换 123例如一个资源URI可以这样定义：https://api.example.com/posts/&#123;postId&#125;/comments/&#123;commentId&#125;postId,commentId 是变量(数字，字符串都类型都可以)。 CRUD的操作不要体现在URI中，HTTP协议中的操作符已经对CRUD做了映射。 123456789CRUD是创建，读取，更新，删除这四个经典操作的简称 例如删除的操作用REST规范执行的话，应该是这个样子：DELETE /users/1234以下是几个错误的示例：GET /deleteUser?id=1234 GET /deleteUser/1234 DELETE /deleteUser/1234 POST /users/1234/delete URI的query字段在REST中,query字段一般作为查询的参数补充，也可以帮助标示一个唯一的资源。但需要注意的是，作为一个提供查询功能的URI，无论是否有query条件，我们都应该保证结果的唯一性，一个URI对应的返回数据是不应该被改变的(在资源没有修改的情况下)。HTTP中的缓存也可能缓存查询结果。 Query参数可以作为Collection或Store类型资源的过滤条件来使用 例如： 123GET /users //返回所有用户列表 GET /users?role=admin //返回权限为admin的用户列表GET /search/users?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125; //根据多条件查询用户 Query参数可以作为Collection或Store资源列表分页标示使用 1234567891011121314如果是一个简单的列表操作，可以这样设计：GET /users?pageSize=25&amp;pageStartIndex=50 如果是一个复杂的列表或查询操作的话，我们可以为资源设计一个Collection，因为复杂查询可能会涉及比较多的参数，建议使用Post的方式传入，例如这样：POST /users/search相关的分页信息还可以存放到 Link 头部，这样客户端可以直接得到诸如下一页、最后一页、上一页等内容的 url 地址Status: 200 OKLink: &lt;https://api.github.com/resource?page=2&gt;; rel=&quot;previous&quot;, &lt;https://api.github.com/resource?page=2&gt;; rel=&quot;next&quot;, &lt;https://api.github.com/resource?page=5&gt;; rel=&quot;last&quot;X-RateLimit-Limit: 20X-RateLimit-Remaining: 19 HTTP请求方法的使用 GET方法用来获取资源 PUT方法可用来新增/更新Store类型的资源 PUT方法可用来更新一个资源的全部属性，使用时传递所有属性的值，即使有的值没有改变 PATCH方法更新资源的部分属性。因为 PATCH 比较新，而且规范比较复杂，所以真正实现的比较少，一般都是用 POST 替代 POST方法可用来创建一个资源 POST方法可用来触发执行一个Controller类型资源 DELETE方法用于删除资源 HTTP响应状态码的使用 200 (“OK”) 用于一般性的成功返回 200 (“OK”) 不可用于请求错误返回 201 (“Created”) 资源被创建 202 (“Accepted”) 用于Controller控制类资源异步处理的返回，仅表示请求已经收到。对于耗时比较久的处理，一般用异步处理来完成 204 (“No Content”) 此状态可能会出现在PUT、POST、DELETE的请求中，一般表示资源存在，但消息体中不会返回任何资源相关的状态或信息。 301 (“Moved Permanently”) 资源的URI被转移，需要使用新的URI访问 302 (“Found”) 不推荐使用，此代码在HTTP1.1协议中被303/307替代。我们目前对302的使用和最初HTTP1.0定义的语意是有出入的，应该只有在GET/HEAD方法下，客户端才能根据Location执行自动跳转，而我们目前的客户端基本上是不会判断原请求方法的，无条件的执行临时重定向 303 (“See Other”) 返回一个资源地址URI的引用，但不强制要求客户端获取该地址的状态(访问该地址) 304 (“Not Modified”) 有一些类似于204状态，服务器端的资源与客户端最近访问的资源版本一致，并无修改，不返回资源消息体。可以用来降低服务端的压力 307 (“Temporary Redirect”) 目前URI不能提供当前请求的服务，临时性重定向到另外一个URI。在HTTP1.1中307是用来替代早期HTTP1.0中使用不当的302 400 (“Bad Request”) 用于客户端一般性错误返回, 在其它4xx错误以外的错误，也可以使用400，具体错误信息可以放在body中 401 (“Unauthorized”) 在访问一个需要验证的资源时，验证错误 403 (“Forbidden”) 一般用于非验证性资源访问被禁止，例如对于某些客户端只开放部分API的访问权限，而另外一些API可能无法访问时，可以给予403状态 404 (“Not Found”) 找不到URI对应的资源 405 (“Method Not Allowed”) HTTP的方法不支持，例如某些只读资源，可能不支持POST/DELETE。但405的响应header中必须声明该URI所支持的方法 406 (“Not Acceptable”) 客户端所请求的资源数据格式类型不被支持，例如客户端请求数据格式为application/xml，但服务器端只支持application/json 409 (“Conflict”) 资源状态冲突，例如客户端尝试删除一个非空的Store资源 412 (“Precondition Failed”) 用于有条件的操作不被满足时 415 (“Unsupported Media Type”) 客户所支持的数据类型，服务端无法满足 429 (“Too Many Requests”) 客户端在规定的时间里发送了太多请求，在进行限流的时候会用到 500 (“Internal Server Error”) 服务器端的接口错误，此错误于客户端无关 HTTP Headers Content-Type 标示body的数据格式 Content-Length body 数据体的大小，客户端可以根据此标示检验读取到的数据是否完整，也可以通过Header判断是否需要下载可能较大的数据体 Last-Modified 用于服务器端的响应，是一个资源最后被修改的时间戳，客户端(缓存)可以根据此信息判断是否需要重新获取该资源 ETag 服务器端资源版本的标示，客户端(缓存)可以根据此信息判断是否需要重新获取该资源，需要注意的是，ETag如果通过服务器随机生成，可能会存在多个主机对同一个资源产生不同ETag的问题 Store类型的资源要支持有条件的PUT请求 1234567891011121314假设有两个客户端client#1/#2都向一个Store资源提交PUT请求，服务端是无法清楚的判断是要insert还是要update的，所以我们要在header中加入条件标示if-Match，If-Unmodified-Since来明确是本次调用API的意图。例如：client#1第一次向服务端发起一个请求 PUT /objects/2113 此时2113资源还不存在，那服务端会认为本次请求是一个insert操作，完成后，会返回 201 (“Created”)client#2再一次向服务端发起同一个请求 PUT /objects/2113 时，因2113资源已存在，服务端会返回 409 (“Conflict”)为了能让client#2的请求成功，或者说我们要清楚的表明本次操作是一次update操作，我们必须在header中加入一些条件标示，例如 if-Match。我们需要给出资源的ETag(if-Match:Etag)，来表明我们希望更新资源的版本，如果服务端版本一致，会返回200 (“OK”) 或者 204 (“No Content”)。如果服务端发现指定的版本与当前资源版本不一致，会返回 412 (“Precondition Failed”) Location 在响应header中使用，一般为客户端感兴趣的资源URI,例如在成功创建一个资源后，我们可以把新的资源URI放在Location中，如果是一个异步创建资源的请求，接口在响应202 (“Accepted”)的同时可以给予客户端一个异步状态查询的地址 Cache-Control, Expires, Date 通过缓存机制提升接口响应性能,同时根据实际需要也可以禁止客户端对接口请求做缓存。对于REST接口来说，如果某些接口实时性要求不高的情况下，我们可以使用max-age来指定一个小的缓存时间，这样对客户端和服务器端双方都是有利的。一般来说只对GET方法且返回200的情况下使用缓存，在某些情况下我们也可以对返回3xx或者4xx的情况下做缓存，可以防范错误访问带来的负载。 我们可以自定义一些头信息，作为客户端和服务器间的通信使用，但不能改变HTTP方法的性质。自定义头尽量简单明了，不要用body中的信息对其作补充说明。 API 地址和版本在 url 中指定 API 的版本是个很好地做法。如果 API 变化比较大，可以把 API 设计为子域名，比如 https://api.github.com/v3；也可以简单地把版本放在路径中，比如 https://example.com/api/v1。另一种做法是，将版本号放在HTTP头信息中。 限流 rate limit如果对访问的次数不加控制，很可能会造成 API 被滥用，甚至被 DDos 攻击。根据使用者不同的身份对其进行限流，可以防止这些情况，减少服务器的压力。 对用户的请求限流之后，要有方法告诉用户它的请求使用情况，Github API 使用的三个相关的头部： X-RateLimit-Limit: 用户每个小时允许发送请求的最大值 X-RateLimit-Remaining：当前时间窗口剩下的可用请求数目 X-RateLimit-Rest: 时间窗口重置的时候，到这个时间点可用的请求数量就会变成 X-RateLimit-Limit 的值 对于超过流量的请求，可以返回 429 Too many requests 状态码，并附带错误信息。 参考文档 http://cizixs.com/2016/12/12/restful-api-design-guide http://wangwei.info/about-rest-api/ http://www.ruanyifeng.com/blog/2011/09/restful.html http://www.ruanyifeng.com/blog/2014/05/restful_api.html https://zh.wikipedia.org/wiki/REST https://developer.github.com/v3 http://novoland.github.io/%E8%AE%BE%E8%AE%A1/2015/08/17/Restful%20API%20%E7%9A%84%E8%AE%BE%E8%AE%A1%E8%A7%84%E8%8C%83.html]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>REST</tag>
        <tag>API</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[codis3系列版本安装]]></title>
    <url>%2Fposts%2F2%2F</url>
    <content type="text"><![CDATA[简介Codis 是 Wandoujia Infrastructure Team 开发的一个分布式 Redis 服务,用户可以看成是一个无限内存的 Redis 服务, 有动态扩/缩容的能力. 对偏存储型的业务更实用,如果你需要 SUBPUB 之类的指令, Codis 是不支持的. 时刻记住 Codis 是一个分布式存储的项目.对于海量的 key, value不太大( &lt;= 1M ), 随着业务扩展缓存也要随之扩展的业务场景有特效. Codis 3.x 由以下组件组成： Codis Server：基于 redis-3.2.8 分支开发。增加了额外的数据结构，以支持 slot 有关的操作以及数据迁移指令。具体的修改可以参考文档 redis 的修改。 Codis Proxy：客户端连接的 Redis 代理服务, 实现了 Redis 协议。 除部分命令不支持以外(不支持的命令列表)，表现的和原生的 Redis 没有区别（就像 Twemproxy）。 对于同一个业务集群而言，可以同时部署多个 codis-proxy 实例； 不同 codis-proxy 之间由 codis-dashboard 保证状态同步。 Codis Dashboard：集群管理工具，支持 codis-proxy、codis-server 的添加、删除，以及据迁移等操作。在集群状态发生改变时，codis-dashboard 维护集群下所有codis-proxy 的状态的一致性。 对于同一个业务集群而言，同一个时刻 codis-dashboard 只能有 0个或者1个； 所有对集群的修改都必须通过 codis-dashboard 完成。 Codis Admin：集群管理的命令行工具。 可用于控制 codis-proxy、codis-dashboard 状态以及访问外部存储。 Codis FE：集群管理界面。 多个集群实例共享可以共享同一个前端展示页面； 通过配置文件管理后端 codis-dashboard 列表，配置文件可自动更新。 Codis HA：为集群提供高可用。 依赖 codis-dashboard 实例，自动抓取集群各个组件的状态； 会根据当前集群状态自动生成主从切换策略，并在需要时通过 codis-dashboard 完成主从切换。 Storage：为集群状态提供外部存储。 提供 Namespace 概念，不同集群的会按照不同 product name 进行组织； 目前仅提供了 Zookeeper、Etcd、Fs 三种实现，但是提供了抽象的 interface 可自行扩展。 安装配置 zookeeper(单机启动，生产环境需要集群)1.安装zookeeper1234567yum install -y java-1.8.0-openjdkcd /server/softwarewget http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.5.1-alpha/zookeeper-3.5.1-alpha.tar.gztar xf zookeeper-3.5.1-alpha.tar.gzmv zookeeper-3.5.1-alpha zookeepermv zookeeper /usr/local/chown root.root /usr/local/zookeeper -R 2.配置zookeeper123456789101112mkdir -pv /data/zookeeper/&#123;data,log&#125;cat &gt;/usr/local/zookeeper/conf/zoo.cfg&lt;&lt;EOFclientPort=2181maxClientCnxns=1024tickTime=2000initLimit=20syncLimit=10dataDir=/data/zookeeper/datadataLogDir=/data/zookeeper/logEOF 3.启动zookeeper12345#启动/usr/local/zookeeper/bin/zkServer.sh start#查看节点的状态/usr/local/zookeeper/bin/zkServer.sh status 安装 codis下载二进制包123456789cd /server/softwarewget https://github.com/CodisLabs/codis/releases/download/3.1.3/codis3.1.3-go1.7.4-linux.tar.gztar xf codis3.1.3-go1.7.4-linux.tar.gzcd codis3.1.3-go1.7.4-linuxmkdir bin etcmv codis-* redis-* bin/cd ..mv codis3.1.3-go1.7.4-linux /usr/local/codischown root.root /usr/local/codis -R 启动 Codis Dashboard 配置 123456cd /usr/local/codis# 生成默认的配置文件./bin/codis-dashboard --default-config &gt; etc/dashboard.toml# 修改配置参数 启动 12nohup ./bin/codis-dashboard --ncpu=4 --config=etc/dashboard.toml \ --log=dashboard.log --log-level=WARN &amp; 启动 Codis Proxy 配置 123456cd /usr/local/codis# 生成默认的配置文件./bin/codis-proxy --default-config &gt; etc/proxy.toml# 修改配置参数 启动 12nohup ./bin/codis-proxy --ncpu=4 --config=etc/proxy.toml \ --log=proxy.log --log-level=WARN &amp; 启动 Codis Server 配置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849mkdir -pv /data/redis/6379cat &gt;/usr/local/codis/etc/redis_6379.conf&lt;&lt;EOFdaemonize yesbind `ifconfig eth1 | grep "inet "| head -n 1 | awk -F'[: ]+' '&#123;print $3&#125;'`port 6379timeout 300loglevel noticelogfile "/data/redis/6379/redis.log"databases 16dbfilename dump.rdb#save 900 1#save 300 10#save 60 10000dir "/data/redis/6379"maxclients 10000#下面两项一般配置使用maxmemory 1024MB#内存不足时的清楚策略maxmemory-policy allkeys-lru#开启日志记录，相当于MySQL的binlog#appendonly yes #appendfilename "appendonly.aof"#appendfsync everysecEOFmkdir -pv /data/redis/6380cat &gt;/usr/local/codis/etc/redis_6380.conf&lt;&lt;EOFdaemonize yesbind `ifconfig eth1 | grep "inet "| head -n 1 | awk -F'[: ]+' '&#123;print $3&#125;'`port 6380timeout 300loglevel noticelogfile "/data/redis/6380/redis.log"databases 16dbfilename dump.rdb#save 900 1#save 300 10#save 60 10000dir "/data/redis/6380"maxclients 10000#下面两项一般配置使用maxmemory 1024MB#内存不足时的清楚策略maxmemory-policy allkeys-lru#开启日志记录，相当于MySQL的binlog#appendonly yes #appendfilename "appendonly.aof"#appendfsync everysecEOF 启动 1234567# 启动/usr/local/codis/bin/codis-server /usr/local/codis/etc/redis_6379.conf/usr/local/codis/bin/codis-server /usr/local/codis/etc/redis_6380.conf#测试ps -ef | grep codis-servernetstat -tunlp | grep 63 启动 Codis FE（可选组件） 配置 1234cd /usr/local/codis# 生成配置文件./bin/codis-admin --dashboard-list --zookeeper=lab1:2181 &gt; etc/codis.json 启动 1234cd /usr/local/codisnohup ./bin/codis-fe --ncpu=4 --log=fe.log --log-level=WARN \ --dashboard-list=etc/codis.json --assets-dir=/usr/local/codis/assets \ --listen=192.168.12.211:8090 &amp; 启动 Codis HA（可选组件） 启动 1nohup ./bin/codis-ha --log=ha.log --log-level=WARN --dashboard=192.168.12.211:18080 &amp; Codis Admin（命令行工具） codis-dashboard 异常退出的修复 1./bin/codis-admin --remove-lock --product=codis-famulei --zookeeper=lab1:2181 codis-proxy 异常退出的修复 12# 确认 codis-proxy 进程已经退出（很重要）./bin/codis-admin --dashboard=127.0.0.1:18080 --remove-proxy --addr=127.0.0.1:11080 --force 添加启动配置的 codis-server web页面方式添加 12# 访问如下页面http://192.168.12.211:8090/ 添加完成后还需要点击如下图标生成主从关系 命令行添加 123456789101112131415# 添加组./bin/codis-admin --dashboard=127.0.0.1:18080 --create-group --gid=2./bin/codis-admin --dashboard=127.0.0.1:18080 --create-group --gid=3# 把 codis-server 添加到指定组./bin/codis-admin --dashboard=127.0.0.1:18080 --group-add --gid=2 --addr=192.168.12.212:6379./bin/codis-admin --dashboard=127.0.0.1:18080 --group-add --gid=2 --addr=192.168.12.212:6380./bin/codis-admin --dashboard=127.0.0.1:18080 --group-add --gid=3 --addr=192.168.12.213:6379./bin/codis-admin --dashboard=127.0.0.1:18080 --group-add --gid=3 --addr=192.168.12.213:6380# 设置同步状态./bin/codis-admin --dashboard=127.0.0.1:18080 --sync-action --create \--addr=192.168.12.212:6379./bin/codis-admin --dashboard=127.0.0.1:18080 --sync-action --create \--addr=192.168.12.212:6380 分配 slots 到 group web页面方式添加 12# 访问如下页面http://192.168.12.211:8090/ 也可以直接使用如下按键快速分配 命令行添加 1./bin/codis-admin --dashboard=127.0.0.1:18080 --slot-action --create-range --beg=600 --end=1023 --gid=3 上线 proxyproxy启动之后需要上线才能使用 web页面方式添加 12# 访问如下页面http://192.168.12.211:8090/ 命令行添加 1./bin/codis-admin --dashboard=127.0.0.1:18080 --online-proxy --addr=192.168.88.211:11080 测试 基本测试 1./bin/redis-cli -h 192.168.12.211 -p 19000 性能测试 1./bin/redis-benchmark -h 192.168.12.211 -p 19000 参考文档 https://github.com/CodisLabs/codis/blob/release3.1/doc/tutorial_zh.md]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>codis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pika安装配置]]></title>
    <url>%2Fposts%2F1%2F</url>
    <content type="text"><![CDATA[简介Pika是一个可持久化的大容量redis存储服务，兼容string、hash、list、zset、set的绝大接口(兼容详情)，解决redis由于存储数据量巨大而导致内存不够用的容量瓶颈，并且可以像redis一样，通过slaveof命令进行主从备份，支持全同步和部分同步，pika还可以用在twemproxy或者codis中来实现静态数据分片（pika已经可以支持codis的动态迁移slot功能） 编译安装12345678910111213141516171819202122232425262728293031# 安装依赖yum install -y snappy-devel bz2 libzip-dev libsnappy-dev libprotobuf-dev \libevent-dev protobuf-compiler libgoogle-glog-dev protobuf-devel \libevent-devel bzip2-devel libbz2-dev zlib-devel gcc-c++# 查看gcc版本gcc -v# 如果不4.8需要先安装切换到4.8sudo rpm --import http://ftp.scientificlinux.org/linux/scientific/5x/x86_64/RPM-GPG-KEYs/RPM-GPG-KEY-cernsudo wget http://people.centos.org/tru/devtools-2/devtools-2.repo -O /etc/yum.repos.d/devtools-2.reposudo yum install -y devtoolset-2-gcc devtoolset-2-binutils devtoolset-2-gcc-c++scl enable devtoolset-2 bash# 下载源码git clone --recursive https://github.com/Qihoo360/pika.git &amp;&amp; cd pika# 编译make __REL=1 -j4# 若编译过程中，提示有依赖的库没有安装，则有提示安装后再重新编译# 安装cp -r output /usr/local/pika# 配置库echo '/usr/local/pika/lib' &gt; /etc/ld.so.conf.d/pika.confldconfig -v# 测试/usr/local/pika/bin/pika -v 配置运行12345678910111213141516171819202122232425262728293031323334353637383940414243# 配置mkdir -pv /data/pikamv /usr/local/pika/conf/pika.conf /usr/local/pika/conf/pika.conf.oricat &gt;/usr/local/pika/conf/pika.conf&lt;&lt;EOFport : 9221thread-num : 1sync-thread-num : 6sync-buffer-size : 10log-path : /data/pika/log/loglevel : infodb-path : /data/pika/db/write-buffer-size : 268435456timeout : 60requirepass :masterauth :userpass :userblacklist :dump-prefix :daemonize : yesdump-path : /data/pika/dump/pidfile : /data/pika/pika.pidmaxclients : 20000target-file-size-base : 20971520expire-logs-days : 7expire-logs-nums : 10root-connection-num : 2slowlog-log-slower-than : 10000slave-read-only : 0db-sync-path : /data/pika/dbsync/db-sync-speed : -1binlog-file-size : 104857600compression : snappymax-background-flushes : 1max-background-compactions : 2max-cache-files : 5000max-bytes-for-level-multiplier : 10EOF# 启动/usr/local/pika/bin/pika -c /usr/local/pika/conf/pika.conf# 检测端口netstat -tunlp | grep 9221 测试123456789# 基本测试yum install -y redisredis-cli -h 127.0.0.1 -p 9221set will mgxget will# 性能测试redis-benchmark -h 127.0.0.1 -p 9221 -n 1000000 -t set,get \-r 10000000000 -c 120 -d 200 参考文档 https://github.com/Qihoo360/pika/blob/master/README_CN.md https://github.com/Qihoo360/pika/wiki/pika-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
        <tag>pika</tag>
        <tag>redis</tag>
      </tags>
  </entry>
</search>
